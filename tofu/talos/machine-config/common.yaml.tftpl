

machine:
  install:
    # System extensions
    extensions:
      # Longhorn distributed storage requirements
      - image: ghcr.io/siderolabs/iscsi-tools:20240808-talos-v1.7.0
      - image: ghcr.io/siderolabs/util-linux-tools:2.40.1-talos-v1.7.0
  features:
    # Enable enhanced dashboard with detailed service monitoring
    stableHostname: true
    apidCheckExtKeyUsage: true
    diskQuotaSupport: true
    kubePrism:
      enabled: true
      port: 7445
    # Host DNS - CoreDNS forwards to Talos host DNS cache
    # Requires loopback interface fix below for Cilium compatibility
    # Ref: https://github.com/siderolabs/talos/issues/10002
    hostDNS:
      enabled: true
      forwardKubeDNSToHost: true
  kernel:
    modules:
      # Required for Longhorn distributed storage
      - name: iscsi_tcp
      - name: nvme_tcp
      - name: nvme_rdma
      - name: nbd        # Network Block Device - CRITICAL for Longhorn
      - name: configfs   # Configuration filesystem - CRITICAL for Longhorn
  kubelet:
    image: ghcr.io/siderolabs/kubelet:v1.33.2
    extraMounts:
      # Required for Longhorn distributed storage
      - destination: /var/lib/longhorn
        type: bind
        source: /var/lib/longhorn
        options:
          - bind
          - rshared
          - rw
    extraArgs:
      # Minimal stable configuration
      eviction-hard: "nodefs.available<5%,imagefs.available<5%,memory.available<100Mi"
      eviction-soft: "nodefs.available<10%,imagefs.available<10%,memory.available<200Mi"
      eviction-soft-grace-period: "nodefs.available=30s,imagefs.available=30s,memory.available=30s"
      # Very aggressive image GC for 20GB disks
      image-gc-high-threshold: "70"
      image-gc-low-threshold: "50"
      # Minimal logging to save disk space
      container-log-max-files: "2"
      container-log-max-size: "5Mi"
      # Reserve less resources for system
      system-reserved: "cpu=100m,memory=200Mi"
      kube-reserved: "cpu=100m,memory=200Mi"
  nodeLabels:
    topology.kubernetes.io/region: ${cluster_name}
    topology.kubernetes.io/zone: ${node_name}
  network:
    hostname: ${hostname}
    # Fix for Cilium + Host DNS: Add link-local address to loopback with global scope
    # Without this, Cilium can't route to 169.254.116.108 (Host DNS)
    # Ref: https://github.com/cilium/cilium/issues/35153
    interfaces:
      - interface: lo
        addresses:
          - 169.254.116.108/32
  logging:
    # Basic logging for stability
    destinations:
      - endpoint: tcp://127.0.0.1:5555
        format: json_lines
  time:
    # NTP servers for accurate timestamps
    servers:
      - time.cloudflare.com
      - pool.ntp.org
  sysctls:
    # CRITICAL: Fix inotify limits (main cause of storage issues!)
    fs.inotify.max_user_watches: 1048576
    fs.inotify.max_user_instances: 8192
    # CRITICAL: Elasticsearch requires higher vm.max_map_count
    vm.max_map_count: 262144
    # Optimized for small VMs with limited RAM
    vm.swappiness: 1
    vm.overcommit_memory: 1
    vm.panic_on_oom: 0
    # Reduce memory usage
    vm.nr_hugepages: 0  # No hugepages for small VMs
    # Network optimizations for homelab
    net.core.default_qdisc: fq
    net.core.rmem_max: 16777216  # Reduced from 64MB
    net.core.wmem_max: 16777216  # Reduced from 64MB
    net.ipv4.tcp_congestion_control: bbr
    net.ipv4.tcp_fastopen: 3
    net.ipv4.tcp_rmem: 4096 87380 16777216  # Reduced
    net.ipv4.tcp_wmem: 4096 65536 16777216  # Reduced
    # File system optimizations
    fs.file-max: 2097152

# ========================================================================
# ERKLÄRUNG: Was diese common.yaml.tftpl bewirkt
# ========================================================================
#
# 1. STORAGE OPTIMIERUNG:
#    - Eviction erst bei <5% statt <15% → mehr nutzbar!
#    - inotify auf 1048576 erhöht (128x mehr) → keine falschen Fehler
#    - Image GC bei 70% → räumt früher auf
#
# 2. DNS FIX FÜR CILIUM + TALOS HOST DNS:
#    - Problem: Cilium blockiert Link-Local 169.254.116.108 (Talos Host DNS)
#    - CoreDNS konnte nicht forwarden → DNS timeout cluster-weit
#    - Lösung: 169.254.116.108/32 auf loopback mit global scope
#    - Ref: https://github.com/cilium/cilium/issues/35153
#    - Ref: https://github.com/siderolabs/talos/issues/10002
#
# 3. HOST DNS CACHING:
#    - hostDNS.enabled: true → Talos cached DNS auf Host-Ebene
#    - forwardKubeDNSToHost: true → CoreDNS nutzt Talos DNS Cache
#    - Vorteil: Geteilter DNS Cache zwischen Host und Pods
#
# RESULTAT: Stabile DNS Resolution mit Cilium + Talos!
# ========================================================================
