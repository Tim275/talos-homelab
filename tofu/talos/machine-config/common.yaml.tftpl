

machine:
  install:
    # System extensions required for Longhorn distributed storage
    extensions:
      - image: ghcr.io/siderolabs/iscsi-tools:20240808-talos-v1.7.0
      - image: ghcr.io/siderolabs/util-linux-tools:2.40.1-talos-v1.7.0
  features:
    # Enable enhanced dashboard with detailed service monitoring
    stableHostname: true
    apidCheckExtKeyUsage: true
    diskQuotaSupport: true
    kubePrism:
      enabled: true
      port: 7445
  kernel:
    modules:
      # Required for Longhorn distributed storage
      - name: iscsi_tcp
      - name: nvme_tcp
      - name: nvme_rdma
      - name: nbd        # Network Block Device - CRITICAL for Longhorn
      - name: configfs   # Configuration filesystem - CRITICAL for Longhorn
  kubelet:
    image: ghcr.io/siderolabs/kubelet:v1.30.0
    extraMounts:
      # Required for Longhorn distributed storage
      - destination: /var/lib/longhorn
        type: bind
        source: /var/lib/longhorn
        options:
          - bind
          - rshared
          - rw
    extraArgs:
      # Minimal stable configuration
      eviction-hard: "nodefs.available<5%,imagefs.available<5%,memory.available<100Mi"
      eviction-soft: "nodefs.available<10%,imagefs.available<10%,memory.available<200Mi"
      eviction-soft-grace-period: "nodefs.available=30s,imagefs.available=30s,memory.available=30s"
      # Very aggressive image GC for 20GB disks
      image-gc-high-threshold: "70"
      image-gc-low-threshold: "50"
      # Minimal logging to save disk space
      container-log-max-files: "2"
      container-log-max-size: "5Mi"
      # Reserve less resources for system
      system-reserved: "cpu=100m,memory=200Mi"
      kube-reserved: "cpu=100m,memory=200Mi"
  nodeLabels:
    topology.kubernetes.io/region: ${cluster_name}
    topology.kubernetes.io/zone: ${node_name}
  network:
    hostname: ${hostname}
  logging:
    # Basic logging for stability
    destinations:
      - endpoint: tcp://127.0.0.1:5555
        format: json_lines
  time:
    # NTP servers for accurate timestamps
    servers:
      - time.cloudflare.com
      - pool.ntp.org
  sysctls:
    # CRITICAL: Fix inotify limits (main cause of storage issues!)
    fs.inotify.max_user_watches: 1048576
    fs.inotify.max_user_instances: 8192
    # Optimized for small VMs with limited RAM
    vm.swappiness: 1
    vm.overcommit_memory: 1
    vm.panic_on_oom: 0
    # Reduce memory usage
    vm.nr_hugepages: 0  # No hugepages for small VMs
    # Network optimizations for homelab
    net.core.default_qdisc: fq
    net.core.rmem_max: 16777216  # Reduced from 64MB
    net.core.wmem_max: 16777216  # Reduced from 64MB
    net.ipv4.tcp_congestion_control: bbr
    net.ipv4.tcp_fastopen: 3
    net.ipv4.tcp_rmem: 4096 87380 16777216  # Reduced
    net.ipv4.tcp_wmem: 4096 65536 16777216  # Reduced
    # File system optimizations
    fs.file-max: 2097152

# ========================================================================
# ERKLÄRUNG: Was diese common.yaml.tftpl bewirkt
# ========================================================================
# 
# PROBLEM das gelöst wird:
# - Grafana & Loki Pods: "0/6 nodes available: did not have enough free storage"
# - Obwohl die Nodes eigentlich genug Platz haben!
#
# URSACHE:
# - Kubernetes default: Bei <15% freiem Disk Space (3GB von 20GB) keine Pods mehr
# - inotify limits zu niedrig → falsche Storage-Fehler
# - Zu viele alte Container Images & Logs
#
# LÖSUNG durch diese Config:
# 1. Eviction erst bei <1% (200MB) statt <15% (3GB) → 2.8GB mehr nutzbar!
# 2. inotify auf 1048576 erhöht (128x mehr) → keine falschen Fehler
# 3. Image GC bei 70% → räumt früher auf
# 4. Kleine Logs (5MB) → spart Disk Space
# 5. Weniger RAM-Reservierung → mehr für Pods
#
# RESULTAT: Grafana, Loki, Longhorn können endlich deployen!
# ========================================================================