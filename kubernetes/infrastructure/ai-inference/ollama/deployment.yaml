---
apiVersion: v1
kind: Namespace
metadata:
  name: ai-inference
  labels:
    name: ai-inference

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-models
  namespace: ai-inference
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: rook-ceph-block-ssd  # SSD for AI model performance
  resources:
    requests:
      storage: 10Gi  # Phi-3 Mini + overhead

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: ai-inference
  labels:
    app: ollama
spec:
  replicas: 1
  strategy:
    type: Recreate  # PVC can only be mounted by one pod
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
        - name: ollama
          image: ollama/ollama:latest
          ports:
            - name: http
              containerPort: 11434
              protocol: TCP
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0:11434"
            - name: OLLAMA_MODELS
              value: "/models"
          volumeMounts:
            - name: models
              mountPath: /models
          resources:
            requests:
              cpu: 1000m       # 1 CPU core
              memory: 4Gi      # 4GB for Phi-3 Mini (3.8GB) + overhead
            limits:
              cpu: 4000m       # 4 CPU cores for inference
              memory: 6Gi      # 6GB limit for safety
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 3
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: ollama-models

---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: ai-inference
  labels:
    app: ollama
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 11434
      targetPort: http
      protocol: TCP
  selector:
    app: ollama

---
# Job to pull Phi-3 Mini model on first startup
apiVersion: batch/v1
kind: Job
metadata:
  name: ollama-pull-phi3-mini
  namespace: ai-inference
spec:
  ttlSecondsAfterFinished: 86400  # Keep for 24h for debugging
  template:
    metadata:
      labels:
        app: ollama-init
    spec:
      restartPolicy: OnFailure
      containers:
        - name: pull-model
          image: curlimages/curl:latest
          command:
            - /bin/sh
            - -c
            - |
              echo "Waiting for Ollama service to be ready..."
              until curl -s http://ollama.ai-inference.svc.cluster.local:11434/ > /dev/null 2>&1; do
                echo "Waiting for Ollama..."
                sleep 5
              done

              echo "Ollama is ready! Pulling Phi-3 Mini model..."
              curl -X POST http://ollama.ai-inference.svc.cluster.local:11434/api/pull \
                -H "Content-Type: application/json" \
                -d '{"name": "phi3:mini"}' \
                -v

              echo "Phi-3 Mini model pull initiated successfully!"
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 500m
              memory: 512Mi
