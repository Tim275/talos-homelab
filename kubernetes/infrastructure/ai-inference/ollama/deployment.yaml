---
apiVersion: v1
kind: Namespace
metadata:
  name: ai-inference
  labels:
    name: ai-inference

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-models
  namespace: ai-inference
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: rook-ceph-block-ssd  # SSD for AI model performance
  resources:
    requests:
      storage: 10Gi  # Phi-3 Mini + overhead

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: ai-inference
  labels:
    app: ollama
spec:
  replicas: 1
  strategy:
    type: Recreate  # PVC can only be mounted by one pod
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      # InitContainer pulls model before main container starts (Production Best Practice)
      initContainers:
        - name: pull-model
          image: ollama/ollama:latest
          command:
            - sh
            - -c
            - |
              echo "ðŸš€ Pulling Phi-3 Mini model..."
              ollama pull phi3:mini
              echo "âœ… Model ready!"
          env:
            - name: OLLAMA_MODELS
              value: "/models"
          volumeMounts:
            - name: models
              mountPath: /models
          resources:
            requests:
              cpu: 500m
              memory: 2Gi
            limits:
              cpu: 2000m
              memory: 4Gi
      containers:
        - name: ollama
          image: ollama/ollama:latest
          ports:
            - name: http
              containerPort: 11434
              protocol: TCP
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0:11434"
            - name: OLLAMA_MODELS
              value: "/models"
          volumeMounts:
            - name: models
              mountPath: /models
          resources:
            requests:
              cpu: 1000m       # 1 CPU core
              memory: 4Gi      # 4GB for Phi-3 Mini (3.8GB) + overhead
            limits:
              cpu: 4000m       # 4 CPU cores for inference
              memory: 6Gi      # 6GB limit for safety
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 3
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: ollama-models

---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: ai-inference
  labels:
    app: ollama
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 11434
      targetPort: http
      protocol: TCP
  selector:
    app: ollama
