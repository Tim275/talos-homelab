# Observability Stack Guide - The Three Pillars

**Cross-reference:** For Grafana setup and dashboard management, see [GRAFANA-SETUP-GUIDE.md](./GRAFANA-SETUP-GUIDE.md)

---

## ğŸ“– Table of Contents

1. [Executive Summary - The Three Pillars](#executive-summary---the-three-pillars)
2. [Was ist Prometheus?](#was-ist-prometheus)
3. [Prometheus Operator Magie](#prometheus-operator-magie)
4. [Was ist Loki?](#was-ist-loki)
5. [Was ist Tempo?](#was-ist-tempo)
6. [OpenTelemetry - Das Universal-SDK](#opentelemetry---das-universal-sdk)
7. [Was ist Thanos?](#was-ist-thanos)
8. [Complete Architecture](#complete-architecture)
9. [Tempo Trace Pipeline](#tempo-trace-pipeline)
10. [Production Setup](#production-setup)
11. [Quick Reference - All Tools](#quick-reference---all-tools)

---

## Executive Summary - The Three Pillars

### TL;DR - Was sind die drei SÃ¤ulen?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ THE THREE PILLARS OF OBSERVABILITY                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š METRICS (Prometheus)
â”œâ”€ Was: CPU 80%, Requests/sec 100, Latency 500ms
â”œâ”€ Frage: "WAS passiert?"
â””â”€ Beispiel: CPU ist hoch! Aber warum?

ğŸ“ LOGS (Loki)
â”œâ”€ Was: "Database connection timeout after 30s"
â”œâ”€ Frage: "WELCHER Error?"
â””â”€ Beispiel: Timeout Error! Aber wo im Request?

ğŸ” TRACES (Tempo)
â”œâ”€ Was: Request Flow von Frontend â†’ Backend â†’ DB
â”œâ”€ Frage: "WO ist das Problem?"
â””â”€ Beispiel: DB Query dauert 8.5s! Problem gefunden! ğŸ‰
```

**Zusammen:**
```
1. Metrics sagen: "Request Latency ist hoch!" (500ms avg)
2. Logs sagen: "Database timeout errors!"
3. Traces sagen: "Diese spezifische DB Query braucht 8.5s" â†’ FIX IT!
```

### Tech Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMPLETE OBSERVABILITY STACK - THE THREE PILLARS               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

App Pods (N8N, Kafka, PostgreSQL, etc.)
    â†“
    â”œâ”€ STDOUT/STDERR Logs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Promtail â”€â”€> Loki (Logs)
    â”œâ”€ /metrics Endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Prometheus (Metrics)
    â””â”€ OTLP Traces â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Tempo (Traces)
                                           â†“
                                       Thanos (S3 Storage)
                                           â†“
                                       Grafana (68 Dashboards)
                                           â”œâ”€ Explore Logs (Loki)
                                           â”œâ”€ Explore Metrics (Prometheus)
                                           â””â”€ Explore Traces (Tempo)
                                                   â†“
                                           Jaeger UI (Traces Frontend)
                                                   â†“
                                               Browser (User)
```

### Key Metrics

| Component | Purpose | Retention | Storage |
|-----------|---------|-----------|---------|
| **Prometheus** | Metrics (Time-Series) | 30 days | 100 GB (PVC) |
| **Loki** | Logs (Label-based) | 7 days | 50 GB (PVC) |
| **Tempo** | Traces (Distributed) | 30 days | Ceph S3 |
| **Thanos** | Long-term Metrics | Unlimited | Ceph S3 |
| **Alertmanager** | Alert Routing | N/A | ConfigMaps |
| **Jaeger** | Trace Frontend | N/A | Uses Tempo |

---

## Was ist Prometheus?

### Definition (IKEA-Style)

**Prometheus** = Dein **StromzÃ¤hler** fÃ¼r Kubernetes âš¡

- Sammelt **Metrics** (CPU, RAM, Requests)
- Speichert in **Time-Series Database** (TSDB)
- Macht **Alerts** (wenn CPU > 80%)
- Hat **PromQL** (Abfragesprache wie SQL)

### Use Cases

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WAS ÃœBERWACHT PROMETHEUS?                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. System Metrics    â†’ CPU, RAM, Disk (Node Exporter)          â”‚
â”‚ 2. Kubernetes Metricsâ†’ Pods, Deployments (kube-state-metrics)  â”‚
â”‚ 3. App Metrics       â†’ Requests, Latency (deine App)           â”‚
â”‚ 4. Custom Metrics    â†’ Business KPIs (z.B. Sales/hour)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Concepts

#### 1. **Metric** - Eine Zahl die sich Ã¤ndert

```
Metric: node_cpu_seconds_total
â”œâ”€ Type: Counter (immer steigend)
â”œâ”€ Value: 123456 (CPU seconds)
â””â”€ Labels: {cpu="0", mode="idle"}
```

**Metric Types:**
```
Counter   â†’ Immer steigend (Requests total: 1000, 1001, 1002...)
Gauge     â†’ Rauf und runter (CPU: 50%, 80%, 30%...)
Histogram â†’ Verteilung (Latency: 50ms, 100ms, 200ms...)
Summary   â†’ Quantile (p50, p95, p99)
```

#### 2. **Label** - Kategorien

Labels = Tags/Filter fÃ¼r Metrics

**Beispiel:**
```
http_requests_total{method="GET", status="200"} = 1000
http_requests_total{method="POST", status="500"} = 5

â†’ Mit Labels kannst du filtern:
  - Alle GET requests
  - Alle 500 errors
  - POST requests mit 200 OK
```

#### 3. **Scrape** - Daten sammeln

Prometheus **scraped** (sammelt) Metrics von Apps

```
Step 1: App hat /metrics endpoint
  curl http://my-app:8080/metrics
  # HELP http_requests_total Total HTTP requests
  # TYPE http_requests_total counter
  http_requests_total{method="GET"} 1000

Step 2: Prometheus scraped alle 15 Sekunden
  Prometheus â†’ http://my-app:8080/metrics â†’ Speichert in TSDB

Step 3: Du queried mit PromQL
  rate(http_requests_total[5m])
  â†’ Zeigt Requests/sec Ã¼ber letzte 5 Minuten
```

#### 4. **PromQL** - Query Language

PromQL = SQL fÃ¼r Time-Series

**Beispiele:**
```promql
# CPU Usage (%)
100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory Usage (GB)
node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes

# HTTP Error Rate (%)
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100

# Top 5 Pods by CPU
topk(5, rate(container_cpu_usage_seconds_total[5m]))
```

---

## Prometheus Operator Magie

### ğŸª„ Warum Prometheus Operator so geil ist

**Vanilla Prometheus (Old Way):**
```yaml
# prometheus.yml (Manual config)
scrape_configs:
  - job_name: 'my-app'
    static_configs:
      - targets: ['my-app:8080']
```

**Problem:** Du musst **JEDE App manuell** eintragen! ğŸ˜±

**Prometheus Operator (New Way):**
```yaml
# ServiceMonitor CRD (Auto-Discovery!)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app
spec:
  selector:
    matchLabels:
      app: my-app
  endpoints:
  - port: metrics
```

**Magie:** Prometheus **findet automatisch** alle Services mit Label `app: my-app`! ğŸ‰

### ServiceMonitor = Auto-Discovery

**IKEA-Analogie:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vanilla Prometheus = Du musst jede Schraube einzeln zÃ¤hlen    â”‚
â”‚ - Neue App? â†’ Manuell in prometheus.yml eintragen             â”‚
â”‚ - App deleted? â†’ Manuell aus prometheus.yml entfernen         â”‚
â”‚ - Neue Replica? â†’ Manuell alle IPs eintragen                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus Operator = IKEA zÃ¤hlt automatisch                  â”‚
â”‚ - Neue App? â†’ ServiceMonitor sagt "Scrape alles mit Label X"  â”‚
â”‚ - App deleted? â†’ Prometheus sieht das selbst                  â”‚
â”‚ - Neue Replica? â†’ Prometheus findet sie automatisch via K8s   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Konkrete Beispiele

#### Beispiel 1: N8N Metrics scrapen

**Ohne Operator:**
```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'n8n-prod'
    static_configs:
      - targets:
        - n8n-prod-0.n8n-prod.n8n-prod.svc.cluster.local:5678
        - n8n-prod-1.n8n-prod.n8n-prod.svc.cluster.local:5678
        # Oh nein, neue Replica? Manuell hinzufÃ¼gen... ğŸ˜°
```

**Mit Operator:**
```yaml
# servicemonitor-n8n.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: n8n-prod
  namespace: monitoring
spec:
  namespaceSelector:
    matchNames:
    - n8n-prod
  selector:
    matchLabels:
      app: n8n
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
```

**Fertig!** Prometheus scraped **ALLE** N8N Pods automatisch! ğŸš€

#### Beispiel 2: Kafka Metrics scrapen

**ServiceMonitor:**
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kafka
  namespace: monitoring
spec:
  namespaceSelector:
    matchNames:
    - kafka-demo-dev
  selector:
    matchLabels:
      strimzi.io/kind: Kafka
  endpoints:
  - port: tcp-prometheus
    interval: 30s
```

**Prometheus findet automatisch:**
- Alle Kafka Brokers
- Alle Kafka Producers
- Alle Kafka Consumers
- Alle Zookeeper Nodes (falls verwendet)

### ServiceMonitor CRD Struktur

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app                      # Name des ServiceMonitors
  namespace: monitoring              # Namespace (meist 'monitoring')
  labels:
    app: my-app                      # Optional: Labels fÃ¼r Organization
spec:
  # WELCHE Services scrapen?
  namespaceSelector:
    matchNames:
    - my-app-namespace               # Nur Services in diesem Namespace

  # WELCHE Labels haben die Services?
  selector:
    matchLabels:
      app: my-app                    # Service muss Label "app: my-app" haben

  # WO ist der /metrics endpoint?
  endpoints:
  - port: metrics                    # Port name (aus Service)
    path: /metrics                   # URL path
    interval: 30s                    # Scrape interval
    scheme: http                     # http oder https

    # Optional: Relabeling
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_name]
      targetLabel: pod
```

### PodMonitor (Alternative)

Manchmal haben Pods **keinen Service** (z.B. DaemonSets)

**PodMonitor = Scrape direkt von Pods**

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  namespaceSelector:
    matchNames:
    - monitoring
  selector:
    matchLabels:
      app: node-exporter
  podMetricsEndpoints:
  - port: metrics
    interval: 30s
```

---

## Was ist Loki?

### Definition (IKEA-Style)

**Loki** = **Grep** fÃ¼r Kubernetes Logs ğŸ”

- Sammelt **Logs** (STDOUT/STDERR von Pods)
- Speichert **komprimiert** (10x weniger als Elasticsearch!)
- Queried mit **LogQL** (wie PromQL fÃ¼r Logs)
- Integration mit **Grafana** (Logs + Metrics zusammen!)

### Loki vs Elasticsearch

**Frage:** Warum haben wir BEIDE (Loki + Elasticsearch)?

**Antwort:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LOKI = Schnelle Suche fÃ¼r Entwickler ğŸï¸                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Schnell (komprimiert)                                        â”‚
â”‚ âœ… GÃ¼nstig (wenig RAM/Disk)                                     â”‚
â”‚ âœ… Grafana-Integration (Logs + Metrics in einem Dashboard!)    â”‚
â”‚ âŒ Keine Full-Text Search (nur Label-basiert)                  â”‚
â”‚ âŒ Keine komplexen Queries                                     â”‚
â”‚                                                                 â”‚
â”‚ Use Case: "Zeig mir N8N Errors der letzten 5 Minuten"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ELASTICSEARCH = Deep Search fÃ¼r Compliance/Audit ğŸ•µï¸            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Full-Text Search (Google-like)                              â”‚
â”‚ âœ… Komplexe Aggregationen (z.B. "Top 10 Error Messages")       â”‚
â”‚ âœ… Compliance (30-day retention mit ILM)                        â”‚
â”‚ âœ… Kibana UI (professionelle Data Views)                       â”‚
â”‚ âŒ Teuer (viel RAM/Disk)                                       â”‚
â”‚ âŒ Langsamer als Loki                                          â”‚
â”‚                                                                 â”‚
â”‚ Use Case: "Zeig mir alle GDPR-relevanten User-Logins 2024"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fazit:** Beide nutzen! Loki fÃ¼r **schnelles Debugging**, Elasticsearch fÃ¼r **Compliance/Audit**! ğŸ¯

### Core Concepts

#### 1. **Stream** - Log-Kategorie

Stream = Logs mit gleichen Labels

**Beispiel:**
```
Stream: {namespace="n8n-prod", app="n8n", level="error"}
â”œâ”€ Log 1: "Database connection failed"
â”œâ”€ Log 2: "API timeout after 30s"
â””â”€ Log 3: "Workflow execution failed"
```

#### 2. **Label** - Filter

Labels = Tags fÃ¼r Streams (wie in Prometheus!)

**Best Practice:** Wenige Labels (Low Cardinality)

```
âœ… GOOD (Low Cardinality):
{namespace="n8n-prod", level="error"}
â†’ Nur wenige Streams (n8n-prod + error = 1 Stream)

âŒ BAD (High Cardinality):
{namespace="n8n-prod", user_id="123", request_id="abc", trace_id="xyz"}
â†’ Millionen Streams (jeder Request = 1 Stream) â†’ OOM! ğŸ’¥
```

#### 3. **LogQL** - Query Language

LogQL = PromQL fÃ¼r Logs

**Beispiele:**
```logql
# Alle N8N Error Logs
{namespace="n8n-prod", level="error"}

# N8N Logs mit "database" im Text
{namespace="n8n-prod"} |= "database"

# N8N Errors grouped by pod
count_over_time({namespace="n8n-prod", level="error"}[5m]) by (pod)

# Top 5 Error Messages
topk(5, count_over_time({level="error"}[1h]))
```

---

## Was ist Tempo?

### Definition (IKEA-Style)

**Tempo** = **GPS-Tracker** fÃ¼r deine Requests ğŸ›°ï¸

- Tracet **Request-Flows** (von Frontend bis Database)
- Speichert in **S3** (Ceph Object Storage)
- Queried mit **TraceQL** (Filter fÃ¼r Traces)
- Integration mit **Loki & Prometheus** (Trace â†’ Logs â†’ Metrics!)

### Warum Distributed Tracing?

**Problem ohne Tracing:**
```
User meldet: "N8N Workflow ist langsam!" ğŸ¢

Du schaust in:
â”œâ”€ Grafana Metrics â†’ CPU normal, RAM normal
â”œâ”€ Loki Logs â†’ "Workflow executed" ... aber wo ist das Problem?
â””â”€ Keine Ahnung wo die Zeit verloren geht! ğŸ˜°
```

**LÃ¶sung mit Tempo:**
```
User meldet: "N8N Workflow ist langsam!" ğŸ¢

Du schaust in Tempo Trace:
â”œâ”€ Span 1: HTTP Request â†’ N8N API (10ms) âœ…
â”œâ”€ Span 2: Workflow Start â†’ N8N Engine (5ms) âœ…
â”œâ”€ Span 3: Database Query â†’ PostgreSQL (8500ms) âŒ PROBLEM HIER!
â””â”€ Span 4: External API Call â†’ Webhook (50ms) âœ…

Problem gefunden: Database Query dauert 8.5 Sekunden! ğŸ¯
```

### The Three Pillars Visualized

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ THE THREE PILLARS (Together = Full Visibility!)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š METRICS (Prometheus)
â”œâ”€ Was: CPU 80%, Requests/sec 100, Latency 500ms
â”œâ”€ Frage: "WAS passiert?"
â””â”€ Beispiel: CPU ist hoch! Aber warum?

ğŸ“ LOGS (Loki)
â”œâ”€ Was: "Database connection timeout after 30s"
â”œâ”€ Frage: "WELCHER Error?"
â””â”€ Beispiel: Timeout Error! Aber wo im Request?

ğŸ” TRACES (Tempo)
â”œâ”€ Was: Request Flow von Frontend â†’ Backend â†’ DB
â”œâ”€ Frage: "WO ist das Problem?"
â””â”€ Beispiel: DB Query dauert 8.5s! Problem gefunden! ğŸ‰
```

**Zusammen:**
```
1. Metrics sagen: "Request Latency ist hoch!" (500ms avg)
2. Logs sagen: "Database timeout errors!"
3. Traces sagen: "Diese spezifische DB Query braucht 8.5s" â†’ FIX IT!
```

---

## OpenTelemetry - Das Universal-SDK

### Was ist OpenTelemetry?

**OpenTelemetry (OTel)** = **CNCF-Standard** fÃ¼r Observability

**Wichtig:** OpenTelemetry ist **KEIN Backend** (keine UI, keine Storage)!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenTelemetry = HOW you COLLECT data (SDK/Protocol)       â”‚
â”‚                                                             â”‚
â”‚ Tempo/Loki/Prometheus = WHERE you STORE data (Backend)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**OpenTelemetry ist:**
- âœ… Ein **SDK** (sammelt Daten in deiner App)
- âœ… Ein **Protokoll** (OTLP = wie Daten Ã¼bertragen werden)
- âœ… Ein **Format** (wie Daten strukturiert sind)

**OpenTelemetry ist NICHT:**
- âŒ Eine **Datenbank** (speichert keine Daten)
- âŒ Eine **Query Engine** (kann keine Daten abfragen)
- âŒ Ein **UI** (keine Dashboards, keine Visualisierung)

**Analogie:**
```
OpenTelemetry = USB-Kabel (Universal Interface)
Tempo/Loki/Prometheus = Festplatten (Storage)

Du brauchst beides!
```

---

### OpenTelemetry = Alle 3 Pillars mit EINEM SDK!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         OpenTelemetry Unified Framework                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  ğŸ“Š TRACES            ğŸ“ˆ METRICS           ğŸ“ LOGS            â”‚
â”‚  âœ… GA (Stable)       âœ… GA (Stable)       ğŸŸ¡ Beta            â”‚
â”‚                                                               â”‚
â”‚  â€¢ Distributed        â€¢ Counters          â€¢ Structured       â”‚
â”‚    Tracing            â€¢ Gauges            â€¢ Context-aware    â”‚
â”‚  â€¢ Spans              â€¢ Histograms        â€¢ Trace-linked     â”‚
â”‚  â€¢ Service Graph      â€¢ Summary           â€¢ Log levels       â”‚
â”‚  â€¢ Latency            â€¢ Exemplars         â€¢ Attributes       â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ein SDK â†’ Drei Datentypen â†’ Drei spezialisierte Backends!**

---

### Die komplette Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DEINE APP (z.B. N8N)                                    â”‚
â”‚                                                          â”‚
â”‚ OpenTelemetry SDK (ONE library!)                        â”‚
â”‚ â”œâ”€ Traces:  tracer.startSpan()                         â”‚
â”‚ â”œâ”€ Metrics: counter.add()                              â”‚
â”‚ â””â”€ Logs:    logger.emit()                              â”‚
â”‚                                                          â”‚
â”‚ Sendet alles via OTLP Protokoll â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenTelemetry Collector (Optional)                      â”‚
â”‚ - EmpfÃ¤ngt OTLP                                         â”‚
â”‚ - Filtert, Sampelt, Batched                            â”‚
â”‚ - Routed zu verschiedenen Backends                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚              â”‚              â”‚
     Traces  â”‚      Metrics â”‚       Logs   â”‚
             â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TEMPO     â”‚  â”‚  PROMETHEUS  â”‚  â”‚     LOKI     â”‚
â”‚             â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ S3 Storage  â”‚  â”‚ TSDB Storage â”‚  â”‚ Chunk+Index  â”‚
â”‚ Trace Query â”‚  â”‚ PromQL       â”‚  â”‚ LogQL        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚              â”‚              â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   GRAFANA             â”‚
              â”‚                       â”‚
              â”‚ - Explore (Traces)    â”‚
              â”‚ - Dashboards (Metrics)â”‚
              â”‚ - Logs (Logs)         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Warum spezialisierte Backends?

**Problem:** Jeder Datentyp hat unterschiedliche Anforderungen!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRACES (Tempo/Jaeger)                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Charakteristik:                                          â”‚
â”‚ - GroÃŸe Objects (1 Trace = 100+ Spans)                  â”‚
â”‚ - Seltene Queries (nur beim Debugging)                  â”‚
â”‚ - Long-term Storage (30 Tage)                           â”‚
â”‚                                                           â”‚
â”‚ Optimale Storage: Object Storage (S3/Ceph)              â”‚
â”‚ Query Pattern: "Find trace by ID"                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ METRICS (Prometheus)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Charakteristik:                                          â”‚
â”‚ - Kleine Time-Series Daten (Zahlen!)                    â”‚
â”‚ - SEHR hÃ¤ufige Queries (Dashboards, Alerts)             â”‚
â”‚ - Range Queries (letzte 24h)                            â”‚
â”‚                                                           â”‚
â”‚ Optimale Storage: Time-Series DB (TSDB)                 â”‚
â”‚ Query Pattern: "CPU usage last 24h"                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LOGS (Loki/Elasticsearch)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Charakteristik:                                          â”‚
â”‚ - Text-basiert (Strings!)                               â”‚
â”‚ - Full-Text Search nÃ¶tig                                â”‚
â”‚ - Label-basierte Queries                                â”‚
â”‚                                                           â”‚
â”‚ Optimale Storage: Index + Chunks                        â”‚
â”‚ Query Pattern: "Find all errors containing 'timeout'"   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ein Backend fÃ¼r alles = suboptimal!**

---

### Zusammenspiel: OpenTelemetry + Jaeger + Tempo

```
Apps mit OpenTelemetry SDK
    â”‚
    â”œâ”€ OTLP Traces â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                         â–¼
    â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    â”‚  TEMPO  â”‚ â† Storage Backend
    â”‚                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚                         â”‚
Legacy Apps mit Jaeger SDK   â”‚
    â”‚                         â”‚
    â””â”€ Jaeger Traces â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                              â”‚
                              â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ JAEGER   â”‚ â† UI Frontend
                        â”‚ QUERY UI â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                         Grafana Explore
```

**Dein Setup:**
- âœ… **Tempo** = Trace Storage (S3 Backend)
- âœ… **Jaeger UI** = Query Frontend (optional, Grafana kann das auch)
- âœ… **OpenTelemetry SDK** = Universal Interface fÃ¼r Apps

---

### OpenTelemetry Vorteile

**âŒ OHNE OpenTelemetry (Vendor Lock-in):**
```javascript
// N8N App - 3 verschiedene Libraries
const jaeger = require('jaeger-client');      // Traces
const promClient = require('prom-client');    // Metrics
const winston = require('winston');           // Logs

// Wechsel zu Zipkin/Datadog = kompletter Code-Rewrite!
```

**âœ… MIT OpenTelemetry (Vendor-neutral):**
```javascript
// N8N App - ONE Library!
const { trace, metrics, logs } = require('@opentelemetry/api');

// Wechsel Backend? Nur Config Ã¤ndern, kein Code-Touch!
```

**Backend-Switch:**
```yaml
# Heute: Homelab
exporters:
  otlp/tempo:
    endpoint: tempo-distributor.monitoring.svc:4317

# Morgen: Grafana Cloud (kein Code-Change!)
exporters:
  otlp/grafana-cloud:
    endpoint: otlp-gateway.grafana.net:443
```

---

### Core Concepts

#### 1. **Trace** - Ein kompletter Request-Flow

Trace = Die gesamte Reise eines Requests

**Beispiel: N8N Workflow Execution**
```
Trace ID: abc-123-def
â”œâ”€ Span 1: HTTP POST /workflow/execute (10ms)
â”œâ”€ Span 2: Load Workflow from DB (100ms)
â”œâ”€ Span 3: Execute Node 1 (API Call) (200ms)
â”œâ”€ Span 4: Execute Node 2 (Transform) (5ms)
â””â”€ Span 5: Save Result to DB (50ms)

Total Duration: 365ms
```

#### 2. **Span** - Ein einzelner Schritt

Span = Ein Teil des Requests (z.B. eine Function, ein API Call)

**Span Attributes:**
```yaml
Span ID: span-1
Parent ID: null  # Root span (kein Parent)
Service: n8n-prod
Operation: POST /workflow/execute
Duration: 10ms
Status: OK
Tags:
  - http.method: POST
  - http.status_code: 200
  - workflow.id: workflow-123
```

#### 3. **Service Graph** - Wer spricht mit wem?

Service Graph = Visualisierung der Microservice-Kommunikation

**Beispiel:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SERVICE GRAPH (Auto-Generated from Traces!)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Browser
    â†“ (100 req/sec)
N8N Frontend
    â†“ (100 req/sec)
N8N Backend
    â”œâ”€ â†’ PostgreSQL (80 req/sec, 50ms avg latency)
    â””â”€ â†’ Redis (20 req/sec, 2ms avg latency)
```

#### 4. **Trace-to-Logs Correlation** - Das Killer-Feature!

**Workflow:**
```
Step 1: User sieht Error in N8N
Step 2: Ã–ffnet Grafana â†’ Tempo â†’ Search for N8N traces
Step 3: Findet Trace mit Error (rot markiert!)
Step 4: Klickt auf Trace â†’ "View Logs for this Span"
Step 5: Grafana springt automatisch zu Loki Logs! ğŸ‰
Step 6: Sieht exakten Error-Log fÃ¼r diesen Request!
```

**Wie funktioniert das?**
- Tempo injiziert **Trace ID** in Logs
- Loki speichert Logs mit **Trace ID**
- Grafana verlinkt beide! â†’ Click = Jump to Logs! ğŸš€

### Tempo vs Jaeger

**Frage:** Warum haben wir BEIDE (Tempo + Jaeger)?

**Antwort:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMPO = Storage Backend ğŸ“¦                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… S3 backend (Ceph RGW) = Unlimited storage                   â”‚
â”‚ âœ… 30-day retention                                            â”‚
â”‚ âœ… Multi-protocol (OTLP, Jaeger, Zipkin)                       â”‚
â”‚ âœ… Trace-to-logs correlation                                   â”‚
â”‚ âŒ Basic UI (Grafana Explore)                                  â”‚
â”‚                                                                 â”‚
â”‚ Use Case: "Datenbank fÃ¼r Traces"                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JAEGER = Frontend UI ğŸ–¥ï¸                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Professional Trace UI                                       â”‚
â”‚ âœ… Service Dependency Graph                                    â”‚
â”‚ âœ… Trace Comparison                                            â”‚
â”‚ âœ… Advanced Search                                             â”‚
â”‚ âœ… Uses Tempo as Backend! (grpc-plugin)                        â”‚
â”‚ âŒ Kein eigener Storage                                        â”‚
â”‚                                                                 â”‚
â”‚ Use Case: "Professionelle UI fÃ¼r Trace-Analyse"                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fazit:** Tempo = Storage, Jaeger = UI! Beide nutzen! ğŸ¯

**Architecture:**
```
Apps (N8N, etc.)
    â†“ OTLP Traces
Tempo Distributor (Port 4317/4318)
    â†“
Tempo Ingester
    â†“
Ceph S3 (Storage)
    â†‘
    â”œâ”€ Grafana Explore (Basic UI)
    â””â”€ Jaeger UI (Professional UI)
```

### Welche Apps sollte ich tracen?

#### âœ… **PRODUCTION CRITICAL (100% Sampling):**

**1. N8N Production** (`n8n-prod`)
```yaml
Why:
  - Business-critical workflows
  - Complex multi-step operations
  - External API integrations
  - Database queries

What to trace:
  - Workflow execution time
  - Webhook reception
  - API calls to external services
  - Database queries
  - Node execution duration
```

**2. Kubernetes Infrastructure**
```yaml
What:
  - API Server requests (kube-apiserver)
  - etcd operations
  - kubelet operations

Why:
  - Find control plane bottlenecks
  - Debug slow kubectl commands
  - Monitor API request latency
```

#### ğŸ”„ **OPTIONAL (10% Sampling):**

**1. Audiobookshelf-dev**
```yaml
Why:
  - Development environment (low traffic)
  - Media streaming debugging

Sampling: 10% (every 10th request)
```

**2. N8N Dev**
```yaml
Why:
  - Workflow development
  - Test complex workflows

Sampling: 100% (dev environment, low traffic)
```

#### âŒ **NOT RECOMMENDED:**

```
âŒ Grafana - Monitoring the monitoring = too meta
âŒ Prometheus - Already has metrics, doesn't need traces
âŒ Loki - Log aggregator, no user traffic
âŒ Cert-Manager - Async certificate operations
âŒ ArgoCD - GitOps sync, not request-based
```

---

## Was ist Thanos?

### Definition (IKEA-Style)

**Thanos** = **Unbegrenzter Speicher** fÃ¼r Prometheus Metrics â™¾ï¸

- Speichert Metrics in **S3** (Ceph Object Storage)
- **Unlimited Retention** (fÃ¼r immer!)
- **Deduplication** (Duplikate entfernen)
- **Downsampling** (Alte Metrics komprimieren)
- **Query Frontend** (Schnelle Queries Ã¼ber S3)

### Warum Thanos?

**Problem ohne Thanos:**
```
Prometheus speichert lokal (PVC):
â”œâ”€ 30 Tage Retention
â”œâ”€ 100 GB Disk
â””â”€ Nach 30 Tagen: Metrics gelÃ¶scht ğŸ’¥

Frage: "Wie war die CPU Usage vor 3 Monaten?"
Antwort: "Keine Ahnung, Daten weg!" ğŸ˜°
```

**LÃ¶sung mit Thanos:**
```
Prometheus â†’ Thanos Sidecar â†’ Thanos Store â†’ Ceph S3
â”œâ”€ 30 Tage lokal (schnell)
â””â”€ âˆ Tage in S3 (langsam aber unbegrenzt!)

Frage: "Wie war die CPU Usage vor 3 Monaten?"
Antwort: "Hier, aus S3!" ğŸ‰
```

### Thanos Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ THANOS ARCHITECTURE (IKEA-Style)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus Pod                                                 â”‚
â”‚ â”œâ”€ Prometheus (main container)                                â”‚
â”‚ â”‚  â””â”€ TSDB: /data (PVC, 30 days)                              â”‚
â”‚ â””â”€ Thanos Sidecar (sidecar container)                         â”‚
â”‚    â””â”€ Uploads TSDB blocks to S3 every 2 hours                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“ Upload
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ceph S3 (Object Storage)                                       â”‚
â”‚ â””â”€ Bucket: thanos                                              â”‚
â”‚    â”œâ”€ 2025-01/ (January metrics, compressed)                  â”‚
â”‚    â”œâ”€ 2025-02/ (February metrics, compressed)                 â”‚
â”‚    â””â”€ 2025-03/ (March metrics, compressed)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†‘ Read
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Thanos Query (Deployment)                                      â”‚
â”‚ â””â”€ Queries both:                                               â”‚
â”‚    â”œâ”€ Prometheus (last 30 days, fast)                         â”‚
â”‚    â””â”€ S3 (older data, slow but available!)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†‘ Query
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana                                                         â”‚
â”‚ â””â”€ Datasource: Thanos Query (not Prometheus!)                 â”‚
â”‚    â†’ Can query data from 3 months ago! ğŸ‰                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Thanos Benefits

| Feature | Without Thanos | With Thanos |
|---------|----------------|-------------|
| **Retention** | 30 days | âˆ (Unlimited) |
| **Storage** | 100 GB (PVC) | Unlimited (S3) |
| **Cost** | Expensive (SSD) | Cheap (Object Storage) |
| **Query Speed** | Fast | Fast (30d) + Slow (older) |
| **HA** | âŒ Single Prometheus | âœ… Multiple Prometheus (deduplicated) |
| **Backup** | Manual | Auto (S3 versioning) |

---

## Complete Architecture

### Full Stack Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMPLETE OBSERVABILITY ARCHITECTURE (IKEA-Style)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: APPLICATIONS                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    N8N      â”‚  â”‚   Kafka     â”‚  â”‚ PostgreSQL  â”‚  â”‚   Redis     â”‚
â”‚    Pods     â”‚  â”‚   Brokers   â”‚  â”‚  Clusters   â”‚  â”‚   Caches    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                â”‚                â”‚
       â”‚ /metrics       â”‚ /metrics       â”‚ /metrics       â”‚ /metrics
       â”‚ STDOUT/STDERR  â”‚ STDOUT/STDERR  â”‚ STDOUT/STDERR  â”‚ STDOUT/STDERR
       â”‚                â”‚                â”‚                â”‚
       â–¼                â–¼                â–¼                â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: COLLECTION                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus (ServiceMonitor)      â”‚  â”‚ Promtail (DaemonSet)             â”‚
â”‚ - Scrapes /metrics every 15s     â”‚  â”‚ - Reads /var/log/pods/*.log      â”‚
â”‚ - Stores in TSDB (30 days)       â”‚  â”‚ - Parses JSON logs               â”‚
â”‚ - ServiceMonitor auto-discovery  â”‚  â”‚ - Adds Kubernetes labels         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                     â”‚
             â”‚ TSDB Blocks (2h)                    â”‚ gRPC Push
             â–¼                                     â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: STORAGE                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Thanos Sidecar                   â”‚  â”‚ Loki (StatefulSet)               â”‚
â”‚ - Uploads blocks to S3 (2h)      â”‚  â”‚ - Stores logs (7 days)           â”‚
â”‚ - Bucket: thanos                 â”‚  â”‚ - Chunks on disk (PVC 50GB)      â”‚
â”‚ - Ceph Object Storage            â”‚  â”‚ - Label-based indexing           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                     â”‚
             â”‚ S3 Upload                           â”‚ LogQL Query
             â–¼                                     â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ceph S3 (Object Storage)         â”‚  â”‚ Grafana (Deployment)             â”‚
â”‚ â””â”€ thanos/                       â”‚  â”‚ - Datasource: Thanos Query       â”‚
â”‚    â”œâ”€ 2025-01/ (Jan metrics)     â”‚  â”‚ - Datasource: Loki               â”‚
â”‚    â”œâ”€ 2025-02/ (Feb metrics)     â”‚  â”‚ - 68 Dashboards (CRDs)           â”‚
â”‚    â””â”€ 2025-03/ (Mar metrics)     â”‚  â”‚ - Unlimited retention via S3!    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–²
             â”‚ Read from S3
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Thanos Query (Deployment)        â”‚
â”‚ - Queries Prometheus (30d)       â”‚
â”‚ - Queries S3 (older data)        â”‚
â”‚ - Deduplicates metrics           â”‚
â”‚ - Downsamples old data           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–²
             â”‚ PromQL Query
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana Dashboard                â”‚
â”‚ - Panel: CPU Usage (last 90d)   â”‚
â”‚ - Data: 30d from Prometheus      â”‚
â”‚         60d from S3 (Thanos)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Sequence

**METRICS FLOW (Prometheus â†’ Thanos â†’ Grafana):**

```
Step 1: App exposes /metrics
  â”œâ”€ App: N8N Pod
  â”œâ”€ Endpoint: http://n8n-prod:5678/metrics
  â””â”€ Format: Prometheus text format

Step 2: ServiceMonitor tells Prometheus to scrape
  â”œâ”€ ServiceMonitor: servicemonitor-n8n.yaml
  â”œâ”€ Selector: app=n8n
  â””â”€ Prometheus auto-discovers all N8N pods

Step 3: Prometheus scrapes every 15 seconds
  â”œâ”€ Prometheus â†’ http://n8n-prod:5678/metrics
  â”œâ”€ Stores in TSDB (local PVC)
  â””â”€ Retention: 30 days

Step 4: Thanos Sidecar uploads to S3
  â”œâ”€ Every 2 hours
  â”œâ”€ TSDB blocks â†’ Ceph S3 bucket "thanos"
  â””â”€ Infinite retention!

Step 5: User opens Grafana Dashboard
  â”œâ”€ Dashboard: "N8N Production Metrics"
  â”œâ”€ Query: rate(http_requests_total[5m])
  â””â”€ Datasource: Thanos Query (not Prometheus!)

Step 6: Thanos Query fetches data
  â”œâ”€ Last 30 days: From Prometheus (fast)
  â”œâ”€ Older data: From S3 (slow but works!)
  â””â”€ Returns combined result to Grafana

Step 7: Grafana renders chart
  â””â”€ User sees data from last 90 days! ğŸ‰
```

**LOGS FLOW (Promtail â†’ Loki â†’ Grafana):**

```
Step 1: App writes logs to STDOUT
  â”œâ”€ App: N8N Pod
  â””â”€ Log: "Workflow execution started"

Step 2: Kubernetes captures logs
  â”œâ”€ Container Runtime: containerd
  â””â”€ File: /var/log/pods/n8n-prod_n8n-0_abc123/n8n/0.log

Step 3: Promtail tails log file
  â”œâ”€ Promtail (DaemonSet) on worker node
  â”œâ”€ Reads: /var/log/pods/**/*.log
  â””â”€ Parses JSON format

Step 4: Promtail adds labels
  â”œâ”€ namespace: n8n-prod
  â”œâ”€ pod: n8n-0
  â”œâ”€ container: n8n
  â””â”€ app: n8n

Step 5: Promtail pushes to Loki
  â”œâ”€ Protocol: gRPC
  â”œâ”€ Endpoint: http://loki:3100/loki/api/v1/push
  â””â”€ Batch: Every 10 seconds

Step 6: Loki stores logs
  â”œâ”€ Index: Labels (namespace, pod, app)
  â”œâ”€ Chunks: Log content (compressed)
  â””â”€ Retention: 7 days

Step 7: User opens Grafana Explore
  â”œâ”€ Datasource: Loki
  â”œâ”€ Query: {namespace="n8n-prod"} |= "error"
  â””â”€ Grafana shows logs! ğŸ‰
```

---

## Tempo Trace Pipeline

### ğŸ” Wie funktioniert Distributed Tracing?

**IKEA-Analogie:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metrics (Prometheus) = Geschwindigkeitsmesser ğŸï¸              â”‚
â”‚ â†’ Sagt dir: "Du fÃ¤hrst 80 km/h"                               â”‚
â”‚ â†’ Aber NICHT: Wo genau bist du auf der Strecke?               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Traces (Tempo) = GPS-Navigation ğŸ›°ï¸                            â”‚
â”‚ â†’ Sagt dir: "Du bist bei Kilometer 42"                        â”‚
â”‚ â†’ UND: "Stau auf der Strecke zwischen KM 50-55!"              â”‚
â”‚ â†’ GPS fÃ¼r jeden einzelnen Request! ğŸ¯                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Complete Trace Flow (OTLP â†’ Tempo â†’ Grafana)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRACES FLOW (App â†’ Tempo â†’ Grafana/Jaeger)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: App instruments code with OTLP SDK
  â”œâ”€ App: N8N Production
  â”œâ”€ Library: @opentelemetry/sdk-node
  â””â”€ Code: SDK auto-instruments HTTP, Database, Redis calls

Step 2: App sends traces to Tempo Distributor
  â”œâ”€ Protocol: OTLP gRPC (port 4317)
  â”œâ”€ Endpoint: http://tempo-distributor.monitoring.svc:4317
  â”œâ”€ Format: Protobuf (binary, efficient)
  â””â”€ Batch: Every 5 seconds (configurable)

Step 3: Tempo Distributor validates and forwards
  â”œâ”€ Distributor receives trace
  â”œâ”€ Validates trace format
  â”œâ”€ Adds metadata (cluster, namespace)
  â””â”€ Forwards to Tempo Ingester

Step 4: Tempo Ingester buffers traces
  â”œâ”€ Ingester receives spans
  â”œâ”€ Buffers in memory (10-15 minutes)
  â”œâ”€ Groups spans into blocks
  â””â”€ Writes blocks to local disk (PVC)

Step 5: Tempo Compactor uploads to S3
  â”œâ”€ Every 2 hours: Compactor reads blocks from Ingester
  â”œâ”€ Compresses blocks (parquet format)
  â”œâ”€ Uploads to Ceph S3: s3://tempo-traces/blocks/
  â””â”€ Retention: 30 days (same as Loki)

Step 6: Metrics Generator creates metrics FROM traces
  â”œâ”€ Reads spans from Ingester
  â”œâ”€ Generates metrics:
  â”‚  â”œâ”€ request_duration_seconds{service="n8n-prod"}
  â”‚  â”œâ”€ span_duration_seconds{span="database-query"}
  â”‚  â””â”€ service_graph_request_total{client="n8n",server="postgres"}
  â””â”€ Remote-writes to Prometheus!

Step 7: User searches traces in Grafana
  â”œâ”€ Grafana â†’ Explore â†’ Tempo Datasource
  â”œâ”€ Query: {service.name="n8n-prod" && duration>1s}
  â””â”€ Tempo Query Frontend searches:
     â”œâ”€ Recent data (last 15 min): From Ingester (fast!)
     â””â”€ Older data: From S3 (slower, but works!)

Step 8: Grafana renders trace
  â”œâ”€ Shows Trace ID, Duration, Spans
  â”œâ”€ Waterfall view of spans (Timeline)
  â”œâ”€ Click "Logs for this span" â†’ Jumps to Loki! ğŸ‰
  â””â”€ Click "Metrics for this service" â†’ Jumps to Prometheus! ğŸš€
```

### Tempo Architecture (Distributed Components)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMPO DISTRIBUTED ARCHITECTURE (Production)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apps (N8N, etc.)                 â”‚
â”‚ - OTLP SDK instrumented          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ OTLP gRPC (4317)
             â”‚ OTLP HTTP (4318)
             â”‚ Jaeger gRPC (14250)
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tempo Distributor (Deployment)   â”‚
â”‚ - Validates traces               â”‚
â”‚ - Load balancing                 â”‚
â”‚ - Forwards to Ingesters          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ gRPC
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tempo Ingester (StatefulSet)     â”‚
â”‚ - Buffers spans (10-15 min)     â”‚
â”‚ - Writes blocks to PVC           â”‚
â”‚ - Replication factor: 1          â”‚
â”‚ - Storage: 10 GB (rook-ceph-rbd) â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚              â”‚
      â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Blocks                                      â”‚ Spans (live)
      â–¼                                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tempo Compactor (Deployment)     â”‚  â”‚ Metrics Generator         â”‚
â”‚ - Reads blocks from Ingester     â”‚  â”‚ - Generates span metrics  â”‚
â”‚ - Compresses to Parquet          â”‚  â”‚ - Service graphs          â”‚
â”‚ - Uploads to S3 (every 2h)       â”‚  â”‚ - Remote-write to Prom    â”‚
â”‚ - Deletes old data (30d)         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
             â”‚ S3 Upload                          â”‚ Remote Write
             â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ceph S3 (Object Storage)         â”‚  â”‚ Prometheus                â”‚
â”‚ Bucket: tempo-traces             â”‚  â”‚ - Span duration metrics   â”‚
â”‚ Format: Parquet (columnar)       â”‚  â”‚ - Service graph metrics   â”‚
â”‚ Retention: 30 days               â”‚  â”‚ - Queryable via PromQL!   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Read
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tempo Querier (Deployment)       â”‚
â”‚ - Queries Ingester (recent)      â”‚
â”‚ - Queries S3 (historical)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tempo Query Frontend (Deploy)    â”‚
â”‚ - Caches queries                 â”‚
â”‚ - Optimizes searches             â”‚
â”‚ - API: http://tempo:3200         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana Explore                  â”‚
â”‚ - Tempo Datasource               â”‚
â”‚ - TraceQL queries                â”‚
â”‚ - Trace-to-logs correlation      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tempo vs Prometheus Data Flow (Vergleich)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROMETHEUS (Metrics) - PULL-based                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Prometheus â”€â”€â”€ scrapes every 15s â”€â”€â†’ App /metrics endpoint
    â†“
Stores in TSDB (local PVC)
    â†“
Thanos uploads to S3 (every 2h)
    â†“
Grafana queries via PromQL

Advantage: Simple (no client libraries needed)
Disadvantage: High-cardinality data = OOM! ğŸ’¥


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMPO (Traces) - PUSH-based                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

App (OTLP SDK) â”€â”€â”€ pushes â”€â”€â†’ Tempo Distributor (4317)
    â†“
Tempo Ingester buffers (10-15 min)
    â†“
Compactor uploads to S3 (every 2h)
    â†“
Grafana queries via TraceQL

Advantage: High-cardinality OK (millions of trace IDs!)
Disadvantage: Requires OTLP SDK in app
```

### How to Instrument N8N for Tracing

#### Step 1: Add OpenTelemetry to N8N (Custom Image)

**Dockerfile:**
```dockerfile
FROM n8nio/n8n:latest

# Install OpenTelemetry SDK
USER root
RUN npm install -g \
    @opentelemetry/sdk-node@^0.52.0 \
    @opentelemetry/auto-instrumentations-node@^0.47.0 \
    @opentelemetry/exporter-trace-otlp-grpc@^0.52.0

# Tracing config
COPY tracing.js /app/tracing.js

USER node
ENV NODE_OPTIONS="--require /app/tracing.js"
```

**tracing.js:**
```javascript
const { NodeSDK } = require('@opentelemetry/sdk-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-grpc');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');

const sdk = new NodeSDK({
  serviceName: 'n8n-prod',
  traceExporter: new OTLPTraceExporter({
    url: 'http://tempo-distributor.monitoring.svc:4317',
  }),
  instrumentations: [getNodeAutoInstrumentations()],
});

sdk.start();
console.log('ğŸ” OpenTelemetry Tracing started for n8n-prod');
```

#### Step 2: Update N8N Deployment

```yaml
# kubernetes/applications/n8n/prod/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: n8n
  namespace: n8n-prod
spec:
  template:
    spec:
      containers:
      - name: n8n
        image: your-registry/n8n:tracing  # Custom image!
        env:
        # OTLP Environment Variables
        - name: OTEL_SERVICE_NAME
          value: "n8n-prod"
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://tempo-distributor.monitoring.svc:4317"
        - name: OTEL_TRACES_SAMPLER
          value: "parentbased_traceidratio"
        - name: OTEL_TRACES_SAMPLER_ARG
          value: "1.0"  # 100% sampling (production critical!)

        # Existing N8N env vars
        - name: N8N_HOST
          value: "n8n.homelab.local"
        # ... other vars
```

#### Step 3: Deploy and Verify

```bash
# Build custom N8N image with OTLP
docker build -t your-registry/n8n:tracing .
docker push your-registry/n8n:tracing

# Deploy updated N8N
kubectl apply -f kubernetes/applications/n8n/prod/

# Wait for N8N to be ready
kubectl rollout status deployment/n8n -n n8n-prod

# Check if traces are being sent (look for OTLP logs)
kubectl logs -n n8n-prod deployment/n8n | grep -i "telemetry\|tracing"

# Verify in Tempo
kubectl exec -n grafana deployment/grafana-deployment -- \
  curl -s "http://tempo-query-frontend.monitoring.svc:3200/api/search?tags=service.name%3Dn8n-prod&limit=10"
```

**Expected Output:** JSON with Trace IDs! ğŸ‰

#### Step 4: Query Traces in Grafana

```
1. Open Grafana: http://grafana.homelab.local:3000

2. Go to Explore â†’ Select "Tempo" datasource

3. Query Types:

   A. Search by Service:
      {service.name="n8n-prod"}

   B. Find slow traces (> 1 second):
      {service.name="n8n-prod" && duration>1s}

   C. Find error traces:
      {service.name="n8n-prod" && status=error}

   D. Find specific workflow:
      {service.name="n8n-prod" && workflow.id="workflow-123"}

4. Click on a trace â†’ Waterfall view shows:
   â”œâ”€ All spans (HTTP, DB, Redis, etc.)
   â”œâ”€ Duration of each span
   â””â”€ Click "Logs for this span" â†’ Jumps to Loki! ğŸš€
```

### Sampling Strategies (Production Best Practices)

#### Strategy 1: 100% Sampling (Production Critical Apps)

**Use for:** N8N Production, Payment APIs, Critical Workflows

```yaml
env:
- name: OTEL_TRACES_SAMPLER
  value: "parentbased_traceidratio"
- name: OTEL_TRACES_SAMPLER_ARG
  value: "1.0"  # 100% = Every request is traced
```

**Why:**
- Business-critical â†’ Need to see EVERY request
- Low traffic (< 1000 req/sec) â†’ Storage OK
- Compliance/Audit â†’ Need complete traces

#### Strategy 2: 10% Sampling (High-Traffic Apps)

**Use for:** Audiobookshelf-dev, High-traffic APIs

```yaml
env:
- name: OTEL_TRACES_SAMPLER
  value: "parentbased_traceidratio"
- name: OTEL_TRACES_SAMPLER_ARG
  value: "0.1"  # 10% = Every 10th request
```

**Why:**
- High traffic â†’ 100% = Too much data
- Representative sample sufficient
- Saves storage costs

#### Strategy 3: Tail-based Sampling (Advanced)

**Only trace ERRORS or SLOW requests:**

```yaml
# tempo/values.yaml
tempo:
  overrides:
    defaults:
      metrics_generator:
        processors:
          - service-graphs
          - span-metrics
        filter_policies:
          - name: error-and-slow-traces
            spans_per_second: 100
            policies:
              - name: error-traces
                type: string_attribute
                key: error
                value: "true"
              - name: slow-traces
                type: numeric_attribute
                key: duration_ms
                min_value: 1000  # > 1 second
```

**Why:**
- Best of both worlds: Full coverage for problems
- Low storage: Only errors + slow traces saved
- Production-ready: Used by Uber, Netflix

---

## Production Setup

### Infrastructure Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KUBERNETES CLUSTER (Talos 1.10.6)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ctrl-0           â”‚  â”‚ worker-1         â”‚  â”‚ worker-2         â”‚
â”‚ (Control Plane)  â”‚  â”‚ (Worker)         â”‚  â”‚ (Worker)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Prometheus       â”‚  â”‚ App Pods         â”‚  â”‚ App Pods         â”‚
â”‚ Grafana          â”‚  â”‚ Promtail         â”‚  â”‚ Promtail         â”‚
â”‚ Loki             â”‚  â”‚ Node Exporter    â”‚  â”‚ Node Exporter    â”‚
â”‚ Alertmanager     â”‚  â”‚                  â”‚  â”‚                  â”‚
â”‚ Thanos Query     â”‚  â”‚                  â”‚  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployed Components

**Namespace: `monitoring`**

```yaml
Prometheus (StatefulSet):
  - Replicas: 1
  - Storage: 100 GB (Ceph RBD)
  - Retention: 30 days
  - Scrape Interval: 15s

Grafana (Deployment):
  - Replicas: 1
  - Dashboards: 68 (as CRDs)
  - Datasources: 4 (Prometheus, Loki, Tempo, Alertmanager)

Loki (StatefulSet):
  - Replicas: 1
  - Storage: 50 GB (Ceph RBD)
  - Retention: 7 days

Tempo (Distributed Architecture):
  - Distributor (Deployment): Replicas 1, receives traces (OTLP/Jaeger)
  - Ingester (StatefulSet): Replicas 1, buffers spans, 10 GB storage
  - Compactor (Deployment): Replicas 1, uploads to S3
  - Querier (Deployment): Replicas 1, queries Ingester + S3
  - Query Frontend (Deployment): Replicas 1, caching + optimization
  - Metrics Generator: Generates span metrics for Prometheus
  - S3 Backend: Ceph RGW (tempo-traces bucket)
  - Retention: 30 days
  - Protocols: OTLP gRPC (4317), OTLP HTTP (4318), Jaeger gRPC (14250)

Jaeger (Deployment):
  - Replicas: 1
  - Backend: Tempo (via grpc-plugin)
  - UI: http://jaeger:16686
  - Use Case: Professional trace visualization

Alertmanager (StatefulSet):
  - Replicas: 1
  - Routes: Tier 0-5

Thanos Sidecar (in Prometheus Pod):
  - S3 Bucket: thanos
  - Upload Interval: 2h

Thanos Query (Deployment):
  - Replicas: 1
  - Queries: Prometheus + S3

Node Exporter (DaemonSet):
  - Pods: 6 (one per node)
  - Metrics: System (CPU, RAM, Disk)

Kube State Metrics (Deployment):
  - Replicas: 1
  - Metrics: Kubernetes Objects

Promtail (DaemonSet):
  - Pods: 6 (one per node)
  - Logs: All pod logs â†’ Loki
```

### Resource Allocation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RESOURCE USAGE (Monitoring Stack)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Prometheus:
  Requests: 2 CPU, 4Gi RAM
  Limits:   4 CPU, 8Gi RAM
  Disk:     100 GB (PVC)

Grafana:
  Requests: 500m CPU, 1Gi RAM
  Limits:   1 CPU, 2Gi RAM

Loki:
  Requests: 1 CPU, 2Gi RAM
  Limits:   2 CPU, 4Gi RAM
  Disk:     50 GB (PVC)

Node Exporter (per node):
  Requests: 100m CPU, 100Mi RAM
  Limits:   200m CPU, 200Mi RAM

Promtail (per node):
  Requests: 100m CPU, 128Mi RAM
  Limits:   200m CPU, 256Mi RAM

TOTAL CLUSTER:
  CPU:    ~10 cores
  Memory: ~20 GB
  Disk:   150 GB (Prometheus + Loki)
```

---

## Quick Reference - All Tools

### Essential Commands

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROMETHEUS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Port-forward to Prometheus UI
kubectl port-forward -n monitoring svc/prometheus-operated 9090:9090
# â†’ http://localhost:9090

# Check Prometheus Targets
curl http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | {job, health}'

# Check ServiceMonitors
kubectl get servicemonitors -A

# Check PrometheusRules
kubectl get prometheusrules -A

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOKI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Port-forward to Loki
kubectl port-forward -n monitoring svc/loki 3100:3100

# Query Loki logs (via HTTP)
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query={namespace="n8n-prod"}' | jq .

# Check Loki health
curl http://localhost:3100/ready

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEMPO (DISTRIBUTED TRACING)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Port-forward to Tempo Query Frontend
kubectl port-forward -n monitoring svc/tempo-query-frontend 3200:3200
# â†’ http://localhost:3200

# Check Tempo health
curl http://localhost:3200/ready

# Search traces by service name (via HTTP API)
curl -s "http://localhost:3200/api/search?tags=service.name%3Dn8n-prod&limit=10" | jq .

# Get specific trace by ID
curl -s "http://localhost:3200/api/traces/<trace-id>" | jq .

# Check Tempo metrics (span metrics generated by Metrics Generator)
curl http://localhost:3200/metrics | grep tempo_

# Check all Tempo pods
kubectl get pods -n monitoring -l app.kubernetes.io/name=tempo

# Check Tempo Distributor (receives traces)
kubectl logs -n monitoring deployment/tempo-distributor --tail=50

# Check Tempo Ingester (buffers traces)
kubectl logs -n monitoring statefulset/tempo-ingester --tail=50

# Check Tempo Compactor (uploads to S3)
kubectl logs -n monitoring deployment/tempo-compactor --tail=50

# Check Tempo S3 bucket (Ceph)
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- \
  radosgw-admin bucket stats --bucket=tempo-traces

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# JAEGER UI (Uses Tempo as Backend)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Port-forward to Jaeger UI
kubectl port-forward -n monitoring svc/jaeger-query 16686:16686
# â†’ http://localhost:16686

# Check Jaeger health
curl http://localhost:16686/

# Jaeger uses Tempo via gRPC plugin (grpc-plugin backend)
kubectl logs -n monitoring deployment/jaeger-query | grep -i tempo

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# THANOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Port-forward to Thanos Query
kubectl port-forward -n monitoring svc/thanos-query 9090:9090
# â†’ http://localhost:9090

# Check Thanos Store status
kubectl logs -n monitoring deployment/thanos-query | grep "store"

# List S3 buckets (Ceph)
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- \
  radosgw-admin bucket list

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VELERO BACKUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# List backups
velero backup get

# Create manual backup
velero backup create monitoring-manual-$(date +%Y%m%d)

# Describe backup
velero backup describe monitoring-daily-latest

# Restore from backup
velero restore create --from-backup monitoring-daily-latest

# Check restore status
velero restore get
```

### Useful PromQL Queries

```promql
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYSTEM METRICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# CPU Usage per Node (%)
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory Usage per Node (%)
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100

# Disk Usage per Node (%)
(1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} /
     node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"})) * 100

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KUBERNETES METRICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Pod Count per Namespace
count(kube_pod_info) by (namespace)

# Container Restarts (last 1h)
increase(kube_pod_container_status_restarts_total[1h]) > 0

# Pods not Running
kube_pod_status_phase{phase!="Running"} == 1

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# APPLICATION METRICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# HTTP Request Rate (req/sec)
rate(http_requests_total[5m])

# HTTP Error Rate (%)
rate(http_requests_total{status=~"5.."}[5m]) /
rate(http_requests_total[5m]) * 100

# HTTP Latency p95 (seconds)
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
```

### Useful LogQL Queries

```logql
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOG QUERIES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# All logs from N8N
{namespace="n8n-prod"}

# Only error logs
{namespace="n8n-prod"} |= "error"

# Logs containing "database"
{namespace="n8n-prod"} |= "database"

# Error count (last 5 min)
count_over_time({namespace="n8n-prod"} |= "error" [5m])

# Top 10 error messages
topk(10, count_over_time({level="error"}[1h]))
```

---

## Summary

### What We Built

```
âœ… Enterprise Observability Stack (100% IaC)
âœ… Prometheus Operator (Auto-Discovery via ServiceMonitors)
âœ… Loki Log Aggregation (LogQL queries)
âœ… Tempo Distributed Tracing (OTLP + Jaeger)
âœ… Thanos Unlimited Storage (Ceph S3)
âœ… OpenTelemetry (Universal SDK for all 3 pillars)
âœ… Velero Backup (Everything!)
âœ… GitOps-Ready (ArgoCD Synced)
```

### Key Benefits

| Feature | Benefit |
|---------|---------|
| **Three Pillars** | Metrics + Logs + Traces = Full visibility |
| **ServiceMonitor** | Auto-discovery, No manual config |
| **Thanos** | Unlimited retention, S3 storage |
| **Loki** | Fast log queries, Low cost |
| **Tempo** | Distributed tracing, S3 storage |
| **OpenTelemetry** | Vendor-neutral, Universal SDK |
| **GitOps** | Everything in Git, ArgoCD synced |

### Files Reference

```
kubernetes/infrastructure/monitoring/
â”œâ”€ OBSERVABILITY-STACK-GUIDE.md          â† YOU ARE HERE
â”œâ”€ GRAFANA-SETUP-GUIDE.md                â† Grafana setup guide
â”œâ”€ servicemonitors/
â”‚  â”œâ”€ servicemonitor-n8n.yaml            â† Example ServiceMonitor
â”‚  â””â”€ ... (all ServiceMonitors)
â”œâ”€ kube-prometheus-stack/
â”‚  â””â”€ values.yaml                        â† Prometheus config
â””â”€ velero/
   â””â”€ schedule-monitoring-daily.yaml     â† Daily backup
```

---

**Created for:** Talos Homelab Production
**Last Updated:** 2025-10-31
**Prometheus Operator:** v0.77.0
**Loki:** v2.9.0
**Tempo:** v2.3.0
**Thanos:** v0.35.0
**OpenTelemetry Collector:** v0.92.0
**Velero:** v1.13.0
