# Complete Monitoring Stack Guide - Tier 0 Enterprise Observability

## Inhaltsverzeichnis

1. [Was ist Monitoring?](#was-ist-monitoring)
2. [Die Drei S√§ulen der Observability](#die-drei-s√§ulen-der-observability)
3. [Komponenten-√úbersicht](#komponenten-√ºbersicht)
4. [Datenfluss-Diagramme](#datenfluss-diagramme)
5. [Operator vs Helm Deployment](#operator-vs-helm-deployment)
6. [Aktuelle Homelab-Konfiguration](#aktuelle-homelab-konfiguration)
7. [Best Practices & Optimierungen](#best-practices--optimierungen)
8. [Troubleshooting Guide](#troubleshooting-guide)

---

## Was ist Monitoring?

**Monitoring** = Beobachtung und Aufzeichnung von System- und Anwendungsverhalten in Echtzeit

### Warum Monitoring?

1. **Probleme erkennen** - Bevor Benutzer sie bemerken
2. **Performance optimieren** - Bottlenecks identifizieren
3. **Kapazit√§t planen** - Wann brauche ich mehr Ressourcen?
4. **Security** - Ungew√∂hnliches Verhalten erkennen
5. **Compliance** - Audit Logs f√ºr Regulierungen

### Monitoring in diesem Homelab

**Ziel:** Enterprise-Grade Observability wie in US-Unternehmen 2025

**Stack:**
- **Prometheus** - Metrics (Zahlen: CPU, RAM, Request Rate)
- **Loki** - Logs (Text: Error Messages, Events)
- **Grafana** - Visualization (Dashboards, Alerts)
- **Jaeger** - Traces (Distributed Request Tracking)
- **Alertmanager** - Alerting (Slack, Email, PagerDuty)

---

## Die Drei S√§ulen der Observability

### 1. Metrics (Metriken) - Prometheus

**Was:** Numerische Zeitreihen-Daten

**Beispiele:**
- CPU-Auslastung: 45%
- Memory Usage: 2.5GB
- HTTP Requests/s: 1200
- Disk IOPS: 500

**Datenformat:**
```promql
# Prometheus metric
http_requests_total{method="GET", status="200", job="n8n"} 15234
```

**Vorteile:**
- ‚úÖ Sehr kompakt (nur Zahlen)
- ‚úÖ Schnelle Queries (Aggregation √ºber Millionen Datenpunkte)
- ‚úÖ Langzeit-Trends erkennbar

**Nachteile:**
- ‚ùå Kein Kontext (warum ist CPU hoch?)
- ‚ùå Keine Details (welcher Request war langsam?)

### 2. Logs (Protokolle) - Loki

**Was:** Textbasierte Event-Streams

**Beispiele:**
- `ERROR: Database connection timeout after 30s`
- `INFO: User 'tim275' logged in from 192.168.1.100`
- `WARN: Memory usage above 80% threshold`

**Datenformat:**
```
{namespace="monitoring", pod="loki-0", level="error"}
2025-10-20T03:14:08Z ERROR Failed to flush chunk: timeout
```

**Vorteile:**
- ‚úÖ Vollst√§ndiger Kontext (Stack Traces, Error Messages)
- ‚úÖ Debugging m√∂glich (genau sehen was passiert ist)
- ‚úÖ Compliance (Audit Logs f√ºr Regulierungen)

**Nachteile:**
- ‚ùå Gro√üe Datenmengen (GBs pro Tag)
- ‚ùå Langsame Queries (Text-Suche √ºber TBs)
- ‚ùå Teuer bei langer Retention

### 3. Traces (Verteilte Anfragen) - Jaeger

**Was:** End-to-End Request Flow durch Microservices

**Beispiel:**
```
User Request ‚Üí API Gateway (50ms)
            ‚Üí Auth Service (20ms)
            ‚Üí Database Query (200ms) ‚Üê SLOW!
            ‚Üí Response (10ms)
Total: 280ms
```

**Datenformat:**
```
Trace ID: abc123
Span 1: api-gateway ‚Üí auth-service (20ms)
Span 2: auth-service ‚Üí database (200ms)
Span 3: database ‚Üí response (10ms)
```

**Vorteile:**
- ‚úÖ Sieht komplette Request-Journey
- ‚úÖ Findet Bottlenecks genau
- ‚úÖ Microservices-Debugging

**Nachteile:**
- ‚ùå Nur f√ºr Microservices sinnvoll
- ‚ùå Instrumentation erforderlich (Code-√Ñnderungen)
- ‚ùå Hoher Overhead bei 100% Sampling

### Wann was nutzen?

| Frage | Tool | Beispiel |
|-------|------|----------|
| **Ist das System gesund?** | Metrics (Prometheus) | CPU < 80%, Memory < 90% |
| **Warum ist es langsam?** | Logs (Loki) | ERROR: Database timeout |
| **Welcher Service ist langsam?** | Traces (Jaeger) | Auth-Service: 200ms (slow!) |
| **Was soll ich tun?** | Alerts (Alertmanager) | Slack: API Server DOWN! |
| **Wie sieht es aus?** | Visualization (Grafana) | Dashboard mit Graphen |

---

## Komponenten-√úbersicht

### Prometheus - Metrics Collection

**Von wem:** Cloud Native Computing Foundation (CNCF)
**Lizenz:** Apache 2.0 (Open Source)
**Sprache:** Go
**Seit:** 2012 (SoundCloud)

**Was es tut:**
1. **Scraping** - Holt Metrics von Anwendungen (HTTP Pull)
2. **Storage** - Speichert Zeitreihen-Daten lokal (TSDB)
3. **Querying** - PromQL f√ºr Abfragen
4. **Alerting** - Sendet Alerts an Alertmanager

**Deployment-Modi:**
- **Standalone** - Single Binary, kein HA
- **HA Pair** - 2+ Replicas mit externem Storage
- **Federated** - Hierarchisch (Cluster ‚Üí Global)
- **Remote Write** - Zu VictoriaMetrics/Thanos

**Datenquellen:**
```
Applications (exporters)
    ‚Üì
ServiceMonitors (CRDs)
    ‚Üì
Prometheus (scrapes every 30s)
    ‚Üì
TSDB Storage (local disk)
```

### Grafana - Visualization

**Von wem:** Grafana Labs
**Lizenz:** AGPLv3 (OSS) + Enterprise (paid)
**Sprache:** TypeScript/Go
**Seit:** 2014

**Was es tut:**
1. **Dashboards** - Visualisiert Metrics/Logs
2. **Alerts** - Grafana Unified Alerting (optional)
3. **Datasources** - Verbindet zu Prometheus, Loki, Jaeger, etc.
4. **Users** - RBAC, Teams, OIDC/SAML

**Deployment-Modi:**
- **Helm Chart** - Traditionell (values.yaml)
- **Grafana Operator** - Kubernetes-native (CRDs)

**Datenquellen:**
```
Grafana
  ‚Üì
Datasources:
  - Prometheus (metrics)
  - Loki (logs)
  - Jaeger (traces)
  - Alertmanager (alerts)
  - Elasticsearch (logs alternative)
```

### Loki - Log Aggregation

**Von wem:** Grafana Labs
**Lizenz:** AGPLv3 (Open Source)
**Sprache:** Go
**Seit:** 2018

**Was es tut:**
1. **Ingestion** - Empf√§ngt Logs von Promtail/Fluent Bit
2. **Indexing** - Nur Labels indexiert (nicht Log-Content!)
3. **Storage** - Chunks in Object Storage (S3/Ceph)
4. **Querying** - LogQL (√§hnlich PromQL)

**Deployment-Modi:**
- **SingleBinary** - Monolith (1 Pod, kein HA)
- **Simple Scalable** - Read/Write/Backend (HA-ready)
- **Microservices** - Distributor, Ingester, Querier, etc.

**Datenquellen:**
```
Applications (stdout/stderr)
    ‚Üì
Promtail / Fluent Bit (log shipper)
    ‚Üì
Loki Gateway (nginx proxy)
    ‚Üì
Loki (distributor ‚Üí ingester)
    ‚Üì
Object Storage (S3/Ceph)
```

### Alertmanager - Alert Routing

**Von wem:** Cloud Native Computing Foundation (CNCF)
**Lizenz:** Apache 2.0
**Sprache:** Go
**Teil von:** Prometheus Project

**Was es tut:**
1. **Grouping** - Fasst √§hnliche Alerts zusammen
2. **Inhibition** - Unterdr√ºckt redundante Alerts
3. **Silencing** - Tempor√§r Alerts stummschalten
4. **Routing** - Nach Priority/Labels an Channels

**Notification Channels:**
- Slack
- Email
- PagerDuty
- Webhook (Keep AIOps, Opsgenie)
- Custom (Telegram, Discord, etc.)

---

## Datenfluss-Diagramme

### Metrics Flow (Prometheus ‚Üí Grafana)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    METRICS DATA FLOW                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Step 1: APPLICATION EXPOSES METRICS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  N8N Application       ‚îÇ
‚îÇ  Port: 8080            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ /metrics endpoint‚îÇ  ‚îÇ  ‚Üê Prometheus format
‚îÇ  ‚îÇ http_req_total=1 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ cpu_usage=0.45   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Step 2: SERVICEMONITOR DISCOVERS APP
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ServiceMonitor (CRD)   ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ namespace: apps  ‚îÇ   ‚îÇ
‚îÇ ‚îÇ selector:        ‚îÇ   ‚îÇ
‚îÇ ‚îÇ   app: n8n       ‚îÇ   ‚îÇ
‚îÇ ‚îÇ port: http       ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì Watched by Prometheus Operator

Step 3: PROMETHEUS SCRAPES METRICS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Prometheus            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ GET /metrics     ‚îÇ  ‚îÇ ‚Üê Every 30 seconds
‚îÇ  ‚îÇ ‚Üí Scrape N8N     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Üí Store in TSDB  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  Storage: 20GB Ceph    ‚îÇ
‚îÇ  Retention: 15 days    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì Query via PromQL

Step 4: GRAFANA VISUALIZES DATA
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Grafana Dashboard     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Datasource:      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Prometheus     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Query:           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ rate(http_req[5m])‚îÇ ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Graph: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ       /      \   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      /        ‚îÄ‚îÄ ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì User views dashboard

Step 5: USER SEES METRICS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Web Browser           ‚îÇ
‚îÇ  http://grafana:3000   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ üìä N8N Dashboard ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Requests: 1.2k/s ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Latency: 45ms    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Errors: 0.1%     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Logs Flow (Loki ‚Üí Grafana)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     LOGS DATA FLOW                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Step 1: APPLICATION WRITES LOGS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  N8N Pod               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ stdout/stderr    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ERROR: DB timeout‚îÇ  ‚îÇ
‚îÇ  ‚îÇ INFO: User login ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì Kubernetes logs (kubectl logs)

Step 2: PROMTAIL COLLECTS LOGS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Promtail DaemonSet    ‚îÇ
‚îÇ  (runs on every node)  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Read pod logs    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Add labels:      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  namespace=apps  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  pod=n8n-0       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì HTTP POST /loki/api/v1/push

Step 3: LOKI INGESTS & STORES LOGS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Loki SingleBinary     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Distributor      ‚îÇ  ‚îÇ ‚Üê Receives logs
‚îÇ  ‚îÇ    ‚Üì             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Ingester         ‚îÇ  ‚îÇ ‚Üê Creates chunks
‚îÇ  ‚îÇ    ‚Üì             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Storage          ‚îÇ  ‚îÇ ‚Üê Filesystem/S3
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  Retention: 30 days    ‚îÇ
‚îÇ  Size: 50Gi            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì Query via LogQL

Step 4: GRAFANA QUERIES LOGS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Grafana Explore       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Datasource: Loki ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Query:           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ {namespace="apps"}‚îÇ ‚îÇ
‚îÇ  ‚îÇ |= "ERROR"       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Results:         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ERROR: DB timeout‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ERROR: Auth fail ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì User searches logs

Step 5: USER DEBUGS ISSUES
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Web Browser           ‚îÇ
‚îÇ  http://grafana:3000   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ üîç Logs Viewer   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Filter: ERROR    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Time: Last 6h    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Found: 15 errors ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Complete Stack Integration

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  COMPLETE MONITORING STACK ARCHITECTURE                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TIER 3: USER APPLICATIONS                                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ  ‚îÇ   N8N    ‚îÇ  ‚îÇ   Kafka  ‚îÇ  ‚îÇ  Elastic ‚îÇ  ‚îÇ   Rook   ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  (App)   ‚îÇ  ‚îÇ  (Stream)‚îÇ  ‚îÇ  (Search)‚îÇ  ‚îÇ  (Storage)‚îÇ               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ        ‚îÇ             ‚îÇ              ‚îÇ             ‚îÇ                      ‚îÇ
‚îÇ        ‚îÇ /metrics    ‚îÇ /metrics     ‚îÇ /metrics    ‚îÇ /metrics            ‚îÇ
‚îÇ        ‚îÇ logs        ‚îÇ logs         ‚îÇ logs        ‚îÇ logs                ‚îÇ
‚îÇ        ‚îÇ             ‚îÇ              ‚îÇ             ‚îÇ                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ             ‚îÇ              ‚îÇ             ‚îÇ
         ‚Üì             ‚Üì              ‚Üì             ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TIER 1: COLLECTION LAYER                                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                           ‚îÇ
‚îÇ  METRICS                           LOGS                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ  ServiceMonitors  ‚îÇ            ‚îÇ  Promtail Pods    ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  (CRDs)           ‚îÇ            ‚îÇ  (DaemonSet)      ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Namespace:* ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ Read /var/  ‚îÇ  ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Selector:   ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ log/pods/** ‚îÇ  ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   app=n8n   ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ Add labels  ‚îÇ  ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ         ‚îÇ                                 ‚îÇ                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                                 ‚îÇ
          ‚Üì                                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TIER 0: STORAGE & PROCESSING                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ  PROMETHEUS          ‚îÇ         ‚îÇ  LOKI                ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ TSDB Storage   ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ SingleBinary   ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 20GB Ceph      ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ 50Gi Ceph      ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Retention: 15d ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ Retention: 30d ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ                ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Metrics:       ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ Logs:          ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - CPU usage    ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ - ERROR msgs   ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - Memory       ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ - INFO events  ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - Request rate ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ - WARN status  ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ             ‚îÇ                                 ‚îÇ                          ‚îÇ
‚îÇ             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îÇ                           ‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TIER 0: VISUALIZATION & ALERTING                                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ  GRAFANA OPERATOR                                             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Grafana Instance   ‚îÇ  ‚îÇ Datasources (CRDs) ‚îÇ             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îÇ Dashboards:    ‚îÇ ‚îÇ  ‚îÇ ‚îÇ - Prometheus   ‚îÇ ‚îÇ             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îÇ - Tier 0-3     ‚îÇ ‚îÇ  ‚îÇ ‚îÇ - Loki         ‚îÇ ‚îÇ             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îÇ - Executive    ‚îÇ ‚îÇ  ‚îÇ ‚îÇ - Alertmanager ‚îÇ ‚îÇ             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îÇ                ‚îÇ ‚îÇ  ‚îÇ ‚îÇ - Jaeger       ‚îÇ ‚îÇ             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îÇ Users:         ‚îÇ ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îÇ - OIDC (opt)   ‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îÇ - Admin        ‚îÇ ‚îÇ                                      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                                      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                      ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ  ALERTMANAGER (2 replicas)                                   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Alert Routing (Priority-based)                     ‚îÇ      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îÇ P1     ‚îÇ  ‚îÇ P2     ‚îÇ  ‚îÇ P3     ‚îÇ  ‚îÇ P5     ‚îÇ   ‚îÇ      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îÇ 5min   ‚îÇ  ‚îÇ 15min  ‚îÇ  ‚îÇ 1h     ‚îÇ  ‚îÇ 4h     ‚îÇ   ‚îÇ      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îÇ Slack  ‚îÇ  ‚îÇ Slack  ‚îÇ  ‚îÇ Slack  ‚îÇ  ‚îÇ Slack  ‚îÇ   ‚îÇ      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îÇ üî¥     ‚îÇ  ‚îÇ üü†     ‚îÇ  ‚îÇ üü°     ‚îÇ  ‚îÇ üîµ     ‚îÇ   ‚îÇ      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                                                    ‚îÇ      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Keep AIOps + Ollama AI (all alerts)              ‚îÇ      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  END USER ACCESS                                                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  http://grafana.timourhomelab.org                                        ‚îÇ
‚îÇ  - View dashboards                                                        ‚îÇ
‚îÇ  - Search logs                                                            ‚îÇ
‚îÇ  - Create alerts                                                          ‚îÇ
‚îÇ  - Explore metrics                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Operator vs Helm Deployment

### Was ist ein Kubernetes Operator?

**Definition:** Software die Kubernetes um dom√§nenspezifisches Wissen erweitert

**Konzept:**
```
Traditional Deployment:
You ‚Üí Helm Chart ‚Üí YAML ‚Üí Kubernetes ‚Üí Pods

Operator Pattern:
You ‚Üí CRD (Custom Resource) ‚Üí Operator ‚Üí Smart Logic ‚Üí Pods
```

**Vorteile:**
1. **Deklarativ** - Beschreibe was du willst, nicht wie
2. **Self-Healing** - Operator repariert automatisch
3. **Day 2 Operations** - Upgrades, Backups, Scaling
4. **Domain Knowledge** - Best Practices eingebaut

**Beispiel - Prometheus Scaling:**

**Ohne Operator (Helm):**
```bash
# values.yaml √§ndern
prometheus:
  replicas: 2

# Helm upgrade
helm upgrade prometheus prometheus-community/prometheus -f values.yaml

# Manuell Thanos konfigurieren
# Manuell ServiceMonitors erstellen
# Manuell Alert Rules deployen
```

**Mit Operator:**
```yaml
# Prometheus CRD
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: main
spec:
  replicas: 2
  serviceMonitorSelector: {}  # Auto-discover all ServiceMonitors
  ruleSelector: {}            # Auto-discover all PrometheusRules
```

Operator macht automatisch:
- ‚úÖ ServiceMonitor Discovery
- ‚úÖ Alert Rule Loading
- ‚úÖ Secret Management
- ‚úÖ Graceful Upgrades

### Prometheus: Operator (kube-prometheus-stack)

**Current Deployment:** Helm Chart `kube-prometheus-stack`

**Was ist kube-prometheus-stack?**
- Helm Chart das Prometheus Operator deployed
- NICHT nur Prometheus, sondern komplettes Monitoring-Stack

**Komponenten:**
1. **Prometheus Operator** - Verwaltet Prometheus Instances
2. **Prometheus** - Metrics Collection
3. **Alertmanager** - Alert Routing
4. **Grafana** - Visualization (optional, bei uns disabled)
5. **kube-state-metrics** - Kubernetes Cluster Metrics
6. **node-exporter** - Node Hardware Metrics

**CRDs (Custom Resource Definitions):**

```yaml
# 1. Prometheus Instance
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: kube-prometheus-stack
spec:
  retention: 15d
  retentionSize: 18GB
  storage:
    volumeClaimTemplate:
      spec:
        storageClassName: rook-ceph-block-enterprise
        resources:
          requests:
            storage: 20G

# 2. ServiceMonitor (auto-discovery)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: n8n-metrics
  namespace: apps
spec:
  selector:
    matchLabels:
      app: n8n
  endpoints:
    - port: http
      path: /metrics
      interval: 30s

# 3. PrometheusRule (alerts)
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: tier0-control-plane
spec:
  groups:
    - name: kubernetes
      rules:
        - alert: KubeAPIServerDown
          expr: up{job="kube-apiserver"} == 0
          for: 1m
          labels:
            severity: critical
            priority: P1
```

**Warum Operator statt Helm-Only?**

| Feature | Helm Chart | kube-prometheus-stack (Operator) |
|---------|------------|----------------------------------|
| **ServiceMonitor Auto-Discovery** | ‚ùå Manual config | ‚úÖ Automatic (label selector) |
| **Alert Rule Loading** | ‚ùå ConfigMaps | ‚úÖ PrometheusRule CRDs |
| **Multi-Namespace Scraping** | ‚ö†Ô∏è Komplex | ‚úÖ Einfach (`namespaceSelector: {}`) |
| **Upgrades** | ‚ö†Ô∏è Manual apply | ‚úÖ Operator handles |
| **Secret Rotation** | ‚ùå Manual | ‚úÖ Automatic reconciliation |
| **GitOps-Friendly** | ‚ö†Ô∏è OK | ‚úÖ Excellent (CRDs in Git) |

**Best Practice:** ‚úÖ **Operator verwenden** (kube-prometheus-stack)

### Grafana: Operator vs Helm

**Current Deployment:** Grafana Operator

**Vergleich:**

#### Grafana Helm Chart (Traditional)

```yaml
# values.yaml
datasources:
  datasources.yaml:
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        url: http://prometheus:9090
        isDefault: true

dashboards:
  default:
    my-dashboard:
      json: |
        {
          "dashboard": {...}
        }
```

**Probleme:**
- ‚ùå Dashboards als JSON in YAML (unreadable)
- ‚ùå Datasources in values.yaml (nicht versioniert separat)
- ‚ùå Kein Multi-Tenancy
- ‚ùå Manuelles Dashboard-Management

#### Grafana Operator (Kubernetes-Native)

```yaml
# 1. Grafana Instance
apiVersion: grafana.integreatly.org/v1beta1
kind: Grafana
metadata:
  name: grafana
spec:
  config:
    log:
      mode: console
  deployment:
    spec:
      template:
        spec:
          containers:
            - name: grafana
              resources:
                limits:
                  cpu: 500m
                  memory: 512Mi

# 2. Datasource (CRD)
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDatasource
metadata:
  name: prometheus
spec:
  datasource:
    name: Prometheus
    type: prometheus
    url: http://prometheus:9090
    isDefault: true

# 3. Dashboard (CRD)
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDashboard
metadata:
  name: n8n-overview
spec:
  folder: Tier 3 - Applications
  json: |
    {
      "title": "N8N Overview",
      ...
    }
```

**Vorteile Operator:**
- ‚úÖ Kubernetes-native (CRDs statt ConfigMaps)
- ‚úÖ GitOps-friendly (separate files f√ºr Datasources/Dashboards)
- ‚úÖ Auto-Reload bei √Ñnderungen
- ‚úÖ Multi-Instance support (mehrere Grafana Instanzen)
- ‚úÖ Namespace-scoped Dashboards

**Comparison Table:**

| Feature | Helm Chart | Grafana Operator |
|---------|------------|------------------|
| **Datasources** | values.yaml | GrafanaDatasource CRD |
| **Dashboards** | ConfigMaps/values | GrafanaDashboard CRD |
| **Folders** | ‚ùå Nicht m√∂glich | ‚úÖ GrafanaFolder CRD |
| **Multi-Instance** | ‚ö†Ô∏è 1 per namespace | ‚úÖ Unlimited |
| **Auto-Reload** | ‚ùå Manual restart | ‚úÖ Automatic |
| **GitOps** | ‚ö†Ô∏è OK | ‚úÖ Excellent |
| **Complexity** | ‚úÖ Simple | ‚ö†Ô∏è More CRDs |

**Best Practice:** ‚úÖ **Operator verwenden** (besonders f√ºr GitOps)

**Warum du Operator nutzt:**
- Dashboards als separate YAML files (nicht in values.yaml)
- Datasources versioniert in Git
- Automatisches Reload bei √Ñnderungen
- Cleaner separation of concerns

### Loki: Operator vs Helm

**Current Deployment:** Helm Chart `grafana/loki`

**Loki Operator Optionen:**

#### 1. Grafana Loki Operator (Official)

**GitHub:** `github.com/grafana/loki/tree/main/operator`
**Status:** Production-ready (seit 2023)

**Features:**
- ‚úÖ LokiStack CRD (deployment management)
- ‚úÖ Multi-tenancy (OpenShift integration)
- ‚úÖ Automatic mTLS (secure communication)
- ‚úÖ Object Storage management
- ‚úÖ Gateway with authentication

**CRDs:**
```yaml
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging
spec:
  size: 1x.small    # Pre-defined sizes
  storage:
    secret:
      name: loki-s3-credentials
      type: s3
    schemas:
      - version: v13
        effectiveDate: "2024-01-01"
  tenants:
    mode: openshift   # Multi-tenancy
```

**Deployment Sizes:**
```
1x.extra-small: Development (1 replica each)
1x.small:       Homelab (<100GB/day)
1x.medium:      Production (<500GB/day)
2x.small:       HA Production
```

#### 2. Helm Chart (Current)

```yaml
# values.yaml
deploymentMode: SingleBinary

loki:
  auth_enabled: false
  storage:
    type: filesystem  # or s3

  limits_config:
    retention_period: 30d
    ingestion_rate_mb: 10

singleBinary:
  replicas: 1
  persistence:
    size: 50Gi
```

**Comparison:**

| Feature | Helm Chart | Loki Operator |
|---------|------------|---------------|
| **Deployment Mode** | Manual config | Pre-defined sizes |
| **Storage Management** | Manual | Automatic (via Secret) |
| **Multi-Tenancy** | Manual config | Built-in (OpenShift mode) |
| **TLS/mTLS** | Manual certs | Automatic |
| **Upgrades** | helm upgrade | Operator handles |
| **Object Storage** | Manual setup | Secret-based config |
| **Gateway** | Optional nginx | Built-in with auth |
| **Complexity** | ‚úÖ Simple | ‚ö†Ô∏è More complex |
| **Flexibility** | ‚úÖ Full control | ‚ö†Ô∏è Opinionated |

**Wann Operator nutzen?**

‚úÖ **JA (Operator):**
- OpenShift/Enterprise environment
- Multi-Tenancy erforderlich
- Automatische TLS gew√ºnscht
- Wenig Loki-Erfahrung (pre-defined sizes helfen)

‚ùå **NEIN (Helm):**
- Homelab/Development
- Volle Kontrolle √ºber Config gew√ºnscht
- Spezielle Setup-Requirements
- Kein OpenShift

**Empfehlung f√ºr Homelab:** ‚úÖ **Helm Chart** (aktuell korrekt!)

**Warum?**
- Volle Flexibilit√§t (SingleBinary ‚Üí Simple Scalable)
- Kein OpenShift-Overhead
- Einfachere Konfiguration
- Community-Support besser f√ºr Helm

**Migration zu Operator:**
Nur wenn:
- Du zu OpenShift wechselst
- Multi-Tenancy brauchst (mehrere Teams)
- Enterprise-Support willst

### Summary: Operator vs Helm Decision Matrix

| Component | Current | Recommendation | Reason |
|-----------|---------|----------------|--------|
| **Prometheus** | kube-prometheus-stack (Operator) | ‚úÖ Keep | ServiceMonitor auto-discovery, GitOps |
| **Grafana** | Grafana Operator | ‚úÖ Keep | Dashboard CRDs, multi-instance, GitOps |
| **Loki** | Helm Chart | ‚úÖ Keep | Flexibility, homelab-friendly, simple |
| **Alertmanager** | Included in kube-prom-stack | ‚úÖ Keep | Integrated with Prometheus Operator |

**General Rule:**

```
Use Operator when:
- GitOps is critical
- Auto-discovery needed
- Multi-namespace management
- Day 2 operations complex

Use Helm when:
- Simple deployment
- Full control needed
- Homelab/Development
- Component-specific requirements
```

---

## Aktuelle Homelab-Konfiguration

### Deployment Overview

| Component | Method | Version | Replicas | Storage |
|-----------|--------|---------|----------|---------|
| **Prometheus** | kube-prometheus-stack | v3.5.0 | 1 | 20Gi Ceph |
| **Grafana** | Grafana Operator | v12.1.0 | 1 | Ephemeral |
| **Loki** | Helm Chart | v3.1.1 | 1 (SingleBinary) | 50Gi Ceph |
| **Alertmanager** | kube-prometheus-stack | v0.27.0 | 2 | No storage |
| **Jaeger** | Helm | v1.61.0 | 1 | Ephemeral |

### Applied Quick Wins (Today)

#### ‚úÖ 1. Prometheus Retention

**File:** `kubernetes/infrastructure/monitoring/kube-prometheus-stack/values.yaml`

```yaml
prometheus:
  prometheusSpec:
    retention: 15d           # Explicit retention
    retentionSize: "18GB"    # Prevents disk full
```

**Status:** ‚úÖ Applied via ArgoCD

#### ‚úÖ 2. Loki Retention & Rate Limits

**File:** `kubernetes/infrastructure/monitoring/loki/values.yaml`

```yaml
loki:
  limits_config:
    retention_period: 30d
    ingestion_rate_mb: 10
    ingestion_burst_size_mb: 20
    max_line_size: 256kb
    max_streams_per_user: 10000

  compactor:
    retention_enabled: true
```

**Status:** ‚úÖ Applied via ArgoCD

#### ‚úÖ 3. Loki Storage Increase

```yaml
singleBinary:
  persistence:
    size: 50Gi  # Increased from 10Gi
```

**Status:** ‚úÖ Applied via ArgoCD

### Current Issues

#### üî¥ CRITICAL: Grafana Datasource UID Mismatch

**Problem:** 101 Dashboard-Files, viele mit falschen Datasource UIDs

**Current UIDs:**
- Prometheus: `bcc9d3ee-2926-4b20-b364-f067529673ff`
- Loki: `d2b40721-7276-43cf-afbc-d064116217e4`
- Alertmanager: `4a200eff-39ee-4f38-9608-28b9e8535176`

**Solution:**
```json
// BAD
{"datasource": {"uid": "WRONG_UID"}}

// GOOD
{"datasource": "Prometheus"}  // Use name instead
```

**Effort:** 2-4 hours (101 files)
**Priority:** HIGH (user can't see many dashboards)

#### ‚ö†Ô∏è WARNING: Loki Filesystem Storage

**Problem:** Logs auf lokalem Disk, nicht production-ready

**Issues:**
- No HA (Logs lost if pod dies)
- No retention (will fill disk)
- Cannot scale horizontally

**Solution:** Migrate to Ceph RGW S3
- See `LOKI_BEST_PRACTICES.md` for migration guide
- Effort: 4-6 hours
- Priority: MEDIUM (works for now, but not scalable)

### Pending Tasks

**This Month:**
1. Fix Grafana datasource UIDs (2-4h)
2. Create dashboard folder hierarchy (1-2h)
3. Migrate Loki to S3 (4-6h)
4. Enable Grafana OIDC if Keycloak available (1h)

**This Quarter:**
5. Enable VictoriaMetrics remote write (2-4h)
6. Create recording rules for slow queries (4-6h)

---

## Best Practices & Optimierungen

### Prometheus Best Practices

**From `PROMETHEUS_BEST_PRACTICES.md`:**

1. ‚úÖ **Explicit Retention** (done today)
2. ‚ö†Ô∏è **HA Setup** - 2+ replicas (not yet)
3. ‚ùå **Remote Write** - VictoriaMetrics/Thanos (planned)
4. ‚ùå **Recording Rules** - For expensive queries (planned)
5. ‚úÖ **Resource Limits** - Already configured
6. ‚úÖ **ServiceMonitor Discovery** - Already enabled

**Production Readiness:** 3/9 (33%)

### Grafana Best Practices

**From `GRAFANA_SETUP_GUIDE.md`:**

1. ‚úÖ **Grafana Operator** - Using CRDs (done)
2. ‚úÖ **Datasources Configured** - Prometheus, Loki, Alertmanager
3. üî¥ **Fix Datasource UIDs** - CRITICAL (101 dashboards broken)
4. ‚ùå **Dashboard Folders** - No hierarchy yet
5. ‚ùå **OIDC** - Using default admin password
6. ‚ùå **Persistent Storage** - Ephemeral (user prefs lost on restart)

**Production Readiness:** 3/8 (38%)

### Loki Best Practices

**From `LOKI_BEST_PRACTICES.md`:**

1. ‚úÖ **Retention Configuration** (done today)
2. ‚úÖ **Rate Limits** (done today)
3. ‚úÖ **Modern TSDB v13 Schema** - Already using
4. ‚ö†Ô∏è **Filesystem Storage** - Should be S3
5. ‚ö†Ô∏è **SingleBinary Mode** - Should be Simple Scalable for production
6. ‚ùå **Label Cardinality Audit** - Not done yet

**Production Readiness:** 3/8 (38%)

### Overall Stack Health

**Summary:**
- ‚úÖ **Functional** - Metrics & Logs are collected
- ‚úÖ **Resource-Efficient** - Good for homelab
- ‚ö†Ô∏è **Production-Ready** - 40-50% (needs HA, S3, OIDC)
- üî¥ **User-Facing Issues** - Datasource UID mismatch

**Next Critical Fix:** Grafana Datasource UIDs (blocks user)

---

## Troubleshooting Guide

### Common Issues

#### Issue: "Datasource not found" in Grafana

**Symptoms:**
- Dashboard shows "datasource not found"
- Panel queries fail
- Many dashboards broken

**Root Cause:** Datasource UID mismatch

**Solution:**
```bash
# 1. Get current UIDs
kubectl get grafanadatasources -n grafana -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.uid}{"\n"}{end}'

# 2. Update dashboard JSONs
# Replace hardcoded UIDs with datasource names
```

**See:** `GRAFANA_SETUP_GUIDE.md` Section "Datasource Configuration"

#### Issue: Prometheus disk full

**Symptoms:**
- Prometheus pod CrashLoopBackOff
- Logs: `no space left on device`

**Root Cause:** No `retentionSize` configured

**Solution:**
‚úÖ Already fixed today! (`retentionSize: "18GB"`)

**Prevention:**
- Monitor PVC usage: `kubectl get pvc -n monitoring`
- Set up alert for >85% disk usage

#### Issue: Loki ingestion errors

**Symptoms:**
- Logs not appearing in Grafana
- Loki logs: `ingestion rate limit exceeded`

**Root Cause:** App sending too many logs

**Solution:**
‚úÖ Already fixed today! (rate limits configured)

**Check:**
```bash
# See dropped logs
kubectl logs -n monitoring loki-0 | grep "discarded"

# Increase limits if legitimate traffic
# In values.yaml: ingestion_rate_mb: 20  (default was 10)
```

#### Issue: Slow Grafana dashboards

**Symptoms:**
- Dashboards take >10s to load
- Queries timeout

**Root Causes:**
1. Too many panels (>30)
2. Long time range (30d instead of 6h)
3. High-resolution queries
4. No recording rules

**Solutions:**
1. Split dashboard into multiple (use rows)
2. Reduce default time range: `from: now-6h`
3. Use `$__rate_interval` instead of hardcoded `[5m]`
4. Create recording rules for expensive queries

**See:** `GRAFANA_SETUP_GUIDE.md` Section "Performance Optimization"

#### Issue: High memory usage (Prometheus)

**Symptoms:**
- Prometheus pod OOMKilled
- High memory usage (>4GB)

**Root Causes:**
1. Too many metrics series (high cardinality)
2. Large queries
3. Under-provisioned memory

**Solutions:**
```bash
# Check cardinality
kubectl exec -n monitoring prometheus-0 -- \
  promtool tsdb analyze /prometheus

# Top 10 high-cardinality metrics
topk(10, count by (__name__)({__name__=~".+"}))

# Solutions:
# 1. Drop high-cardinality metrics (relabeling)
# 2. Increase memory limits
# 3. Enable remote_write to VictoriaMetrics
```

---

## Quick Reference

### Important URLs

```
Grafana:      http://grafana.timourhomelab.org
Prometheus:   http://prometheus.timourhomelab.org (internal only)
Alertmanager: http://alertmanager.timourhomelab.org (internal only)
```

### Important Commands

```bash
# Check ArgoCD sync status
kubectl get application kube-prometheus-stack loki -n argocd

# Force ArgoCD sync
kubectl patch application loki -n argocd --type merge \
  -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"HEAD"}}}'

# Check Prometheus targets
kubectl port-forward -n monitoring svc/prometheus-operated 9090:9090
# Open: http://localhost:9090/targets

# Check Loki logs
kubectl logs -n monitoring loki-0 -f

# Check Grafana datasources
kubectl get grafanadatasources -n grafana

# Check Grafana dashboards
kubectl get grafanadashboard -A
```

### Important Files

```
Prometheus Config:
  kubernetes/infrastructure/monitoring/kube-prometheus-stack/values.yaml

Loki Config:
  kubernetes/infrastructure/monitoring/loki/values.yaml

Grafana Config:
  kubernetes/infrastructure/monitoring/grafana/grafana.yaml

Grafana Datasources:
  kubernetes/infrastructure/monitoring/grafana/datasources/*.yaml

Alert Rules:
  kubernetes/infrastructure/monitoring/alertmanager/tier*.yaml

Dashboards:
  kubernetes/infrastructure/monitoring/grafana/dashboards/**/*.yaml
```

### Documentation Index

- `MONITORING_STACK_OVERVIEW.md` - High-level architecture
- `PROMETHEUS_BEST_PRACTICES.md` - Prometheus setup & optimization
- `GRAFANA_SETUP_GUIDE.md` - Grafana configuration & troubleshooting
- `LOKI_BEST_PRACTICES.md` - Loki deployment & best practices
- `COMPLETE_MONITORING_GUIDE.md` - This file (comprehensive guide)

---

**Document Version:** 1.0
**Last Updated:** 2025-10-20
**Status:** Complete - Production guide for Tier 0 monitoring stack
**Similar to:** `kubernetes/infrastructure/observability/elasticsearch/LICENSE_COMPARISON.md` (comprehensive Elasticsearch guide)
