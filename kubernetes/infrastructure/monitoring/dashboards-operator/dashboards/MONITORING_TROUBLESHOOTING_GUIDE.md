# ğŸš¨ ENTERPRISE MONITORING TROUBLESHOOTING GUIDE

## âŒ HÃ„UFIGE "DATASOURCE NOT FOUND" / "NO DATA" PROBLEME

### ğŸ”§ PROBLEM 1: Datasource Name Mismatch
**Fehler**: `Datasource prometheus not found` oder `DS_PROMETHEUS not found`

**âœ… LÃ–SUNG**:
```bash
# 1. Check Grafana Datasource Namen
kubectl get grafanadatasource -n grafana -o json | jq '.items[].spec.datasource.name'

# 2. Fix Dashboard Datasource Mapping
# In GrafanaDashboard YAML:
datasources:
  - inputName: "DS_PROMETHEUS"
    datasourceName: "Prometheus"  # âš ï¸ WICHTIG: Exakt wie in Grafana definiert
```

### ğŸ”§ PROBLEM 2: ServiceMonitor im falschen Namespace
**Fehler**: Metrics kommen nicht an, obwohl Service lÃ¤uft

**âœ… LÃ–SUNG**:
```bash
# 1. ServiceMonitor MUSS im 'monitoring' Namespace sein
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-service-monitor
  namespace: monitoring  # âš ï¸ KRITISCH: monitoring namespace!
  labels:
    release: prometheus-operator  # âš ï¸ KRITISCH: fÃ¼r kube-prometheus-stack
```

### ğŸ”§ PROBLEM 3: Service ohne Metrics Port
**Fehler**: ServiceMonitor findet keinen metrics port

**âœ… LÃ–SUNG**:
```bash
# 1. Check ob Service metrics port hat
kubectl get service <service-name> -n <namespace> -o yaml

# 2. Wenn kein metrics port â†’ Service erstellen:
apiVersion: v1
kind: Service
metadata:
  name: my-service-metrics
  namespace: <target-namespace>
  labels:
    app.kubernetes.io/name: <app-name>
    app.kubernetes.io/component: metrics
spec:
  ports:
  - name: metrics
    port: 8080        # âš ï¸ Muss mit Pod containerPort Ã¼bereinstimmen
    targetPort: 8080
  selector:
    app.kubernetes.io/name: <app-name>
```

### ğŸ”§ PROBLEM 4: Falsche ServiceMonitor Selectors
**Fehler**: ServiceMonitor findet Service nicht

**âœ… LÃ–SUNG**:
```bash
# 1. Check Service Labels
kubectl get service <service-name> -n <namespace> --show-labels

# 2. ServiceMonitor selector anpassen:
spec:
  namespaceSelector:
    matchNames:
    - <target-namespace>
  selector:
    matchLabels:
      app.kubernetes.io/name: <app-name>  # âš ï¸ MUSS mit Service Labels Ã¼bereinstimmen
```

### ğŸ”§ PROBLEM 5: Falscher Metrics Path/Port
**Fehler**: Target ist "down" in Prometheus

**âœ… LÃ–SUNG**:
```bash
# 1. Test metrics endpoint direkt:
kubectl port-forward -n <namespace> service/<service-name> 8080:8080
curl http://localhost:8080/metrics

# 2. ServiceMonitor path/port korrigieren:
spec:
  endpoints:
  - port: metrics          # âš ï¸ Name vom Service port
    interval: 30s
    path: /metrics         # âš ï¸ Korrekter path (nicht /stats/prometheus)
    scheme: http           # âš ï¸ http oder https je nach Service
```

## ğŸ› ï¸ STEP-BY-STEP DEBUGGING PROZESS

### 1ï¸âƒ£ PROMETHEUS TARGETS CHECKEN
```bash
kubectl port-forward -n monitoring service/kube-prometheus-stack-prometheus 9090:9090
curl -s "http://localhost:9090/api/v1/targets" | jq '.data.activeTargets[] | select(.health == "down")'
```

### 2ï¸âƒ£ GRAFANA DATASOURCES VERIFIZIEREN
```bash
kubectl get grafanadatasource -n grafana -o yaml
# Check name, url, isDefault fields
```

### 3ï¸âƒ£ SERVICEMONITOR LABELS & SELECTORS PRÃœFEN
```bash
kubectl get servicemonitor -A
kubectl describe servicemonitor <name> -n monitoring
```

### 4ï¸âƒ£ SERVICE PORTS & LABELS CHECKEN
```bash
kubectl get service <service> -n <namespace> -o yaml
# Check ports.name, labels, selector
```

### 5ï¸âƒ£ POD METRICS ENDPOINT TESTEN
```bash
kubectl port-forward -n <namespace> pod/<pod-name> 8080:8080
curl http://localhost:8080/metrics | head -10
```

## ğŸ—ï¸ ENTERPRISE SERVICEMONITOR TEMPLATE

```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ .Values.serviceName }}-metrics
  namespace: {{ .Values.namespace }}
  labels:
    app.kubernetes.io/name: {{ .Values.appName }}
    app.kubernetes.io/component: metrics
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "{{ .Values.metricsPort }}"
    prometheus.io/path: "/metrics"
spec:
  ports:
  - name: metrics
    port: {{ .Values.metricsPort }}
    targetPort: {{ .Values.metricsPort }}
    protocol: TCP
  selector:
    app.kubernetes.io/name: {{ .Values.appName }}
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: {{ .Values.serviceName }}-metrics
  namespace: monitoring  # âš ï¸ IMMER monitoring namespace
  labels:
    app.kubernetes.io/name: {{ .Values.appName }}
    app.kubernetes.io/component: metrics
    release: prometheus-operator  # âš ï¸ KRITISCH fÃ¼r kube-prometheus-stack
spec:
  namespaceSelector:
    matchNames:
    - {{ .Values.namespace }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ .Values.appName }}
      app.kubernetes.io/component: metrics
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
    scheme: http
    # Optional fÃ¼r HTTPS:
    # tlsConfig:
    #   insecureSkipVerify: true
```

## ğŸ¯ ENTERPRISE DASHBOARD TEMPLATE

```yaml
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDashboard
metadata:
  name: {{ .Values.dashboardName }}
  labels:
    app: grafana
    tier: {{ .Values.tier }}  # infrastructure/platform/application
    component: {{ .Values.component }}
spec:
  instanceSelector:
    matchLabels:
      app: grafana  # âš ï¸ MUSS mit Grafana CR label Ã¼bereinstimmen
  folder: "{{ .Values.folderName }}"
  grafanaCom:
    id: {{ .Values.grafanaComId }}
    revision: {{ .Values.revision }}
  datasources:
    - inputName: "DS_PROMETHEUS"
      datasourceName: "Prometheus"  # âš ï¸ EXAKT wie GrafanaDatasource.spec.datasource.name
```

## ğŸš¨ TROUBLESHOOTING CHECKLISTE

- [ ] ServiceMonitor im `monitoring` namespace?
- [ ] ServiceMonitor hat `release: prometheus-operator` label?
- [ ] Service hat korrekten metrics port mit name `metrics`?
- [ ] ServiceMonitor selector matched Service labels?
- [ ] Metrics endpoint auf `/metrics` erreichbar?
- [ ] GrafanaDashboard datasourceName = GrafanaDatasource name?
- [ ] Prometheus target ist "up" status?
- [ ] Pod exportiert metrics auf konfiguriertem port?

## ğŸ” DEBUGGING COMMANDS

```bash
# Prometheus Targets Status
kubectl port-forward -n monitoring service/kube-prometheus-stack-prometheus 9090:9090 &
curl -s "http://localhost:9090/api/v1/targets" | jq '.data.activeTargets[] | select(.health != "up")'

# Grafana Datasources
kubectl get grafanadatasource -n grafana -o jsonpath='{.items[*].spec.datasource.name}'

# ServiceMonitor Discovery
kubectl get servicemonitor -A --show-labels

# Service & Pod Metrics Test
kubectl get pods -A -o wide | grep <app-name>
kubectl port-forward -n <namespace> service/<service> <port>:<port>
curl http://localhost:<port>/metrics
```

## ğŸ¯ ERFOLGREICH IMPLEMENTIERTE LÃ–SUNGEN (ENTERPRISE DASHBOARD REFACTOR)

### âœ… ARCHITEKTUR TRANSFORMATION (2025-09-29)

**VON**: Abstract tier-based Structure â†’ **ZU**: Application-based Structure
**ERGEBNIS**: ğŸš€ Enterprise-grade monitoring mit yamllint compliance

#### ğŸ“‚ **NEUE STRUKTUR:**
```
infrastructure/monitoring/dashboards-operator/
â”œâ”€â”€ servicemonitors/           # âœ… Cross-namespace ServiceMonitors
â”‚   â”œâ”€â”€ kafka-exporter.yaml   # âœ… Kafka metrics - monitoring/â†’kafka
â”‚   â”œâ”€â”€ cnpg-controller.yaml  # âœ… CloudNativePG HTTP scheme fix
â”‚   â””â”€â”€ sail-operator.yaml    # âœ… Sail Operator HTTPS mit TLS config
â””â”€â”€ dashboards/               # âœ… Application-based GrafanaDashboards
    â”œâ”€â”€ n8n/                 # ğŸ”§ N8N workflow automation (dev/prod)
    â”œâ”€â”€ audiobookshelf/      # ğŸ§ Media server dashboards (dev/prod)
    â”œâ”€â”€ elasticsearch/       # ğŸ” Logging stack (ES/Kibana/Fluentd/Vector)
    â”œâ”€â”€ istio/              # ğŸŒŠ Service mesh (Sail/Control/Traffic)
    â”œâ”€â”€ ceph/               # ğŸ’¾ Storage cluster (Rook/Pools/OSD/CNPG)
    â”œâ”€â”€ argocd/             # ğŸš€ GitOps platform (Overview/Performance/Rollouts)
    â””â”€â”€ kubernetes/         # â˜¸ï¸ Infrastructure (Cluster/Node Exporter)
```

#### ğŸ”§ **GELÃ–STE SERVICEMONITOR PROBLEME:**

**1. Kafka-Exporter ServiceMonitor:**
```yaml
# âŒ VORHER: servicemonitor in grafana namespace â†’ keine discovery
# âœ… NACHHER: servicemonitor in monitoring namespace mit cross-namespace selector
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kafka-exporter-fixed
  namespace: monitoring  # âš ï¸ KRITISCH: monitoring namespace!
  labels:
    release: prometheus-operator  # âš ï¸ KRITISCH: fÃ¼r kube-prometheus-stack
spec:
  namespaceSelector:
    matchNames:
      - kafka  # Cross-namespace: monitoring â†’ kafka
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka-exporter
      strimzi.io/kind: KafkaExporter
```

**2. CloudNativePG ServiceMonitor:**
```yaml
# âŒ VORHER: scheme: https â†’ "server gave HTTP response to HTTPS client"
# âœ… NACHHER: scheme: http nach direktem endpoint test
endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
    scheme: http  # âœ… FIX: HTTP statt HTTPS
```

**3. Sail Operator ServiceMonitor:**
```yaml
# âŒ VORHER: Kein ServiceMonitor fÃ¼r Istio Sail Operator
# âœ… NACHHER: HTTPS ServiceMonitor mit TLS config
endpoints:
  - port: https-metrics
    interval: 30s
    path: /metrics
    scheme: https
    scrapeTimeout: 10s
    tlsConfig:
      insecureSkipVerify: true  # âœ… FÃ¼r self-signed certs
```

#### ğŸ—ï¸ **ENTERPRISE BENEFITS:**

âœ… **Developer-Friendly**: Find dashboards by application (n8n/, ceph/, etc.)
âœ… **Cross-Namespace Discovery**: ServiceMonitors in monitoring namespace
âœ… **yamllint Compliance**: Single-document YAML files enterprise-grade
âœ… **Centralized Deployment**: Single infrastructure-dashboards-operator app
âœ… **Netflix/Google Pattern**: Application-based statt abstract tiers
âœ… **Sail Operator Included**: Komplette Istio service mesh monitoring

#### ğŸ’¡ **KEY LEARNINGS:**

- **ServiceMonitors MÃœSSEN in monitoring namespace** fÃ¼r kube-prometheus-stack discovery
- **Cross-namespace funktioniert** via namespaceSelector.matchNames
- **HTTP/HTTPS scheme testing** ist kritisch vor ServiceMonitor deployment
- **Application-based structure** ist praktischer als tier-based abstractions
- **yamllint compliance** ist enterprise standard (single-document files)

#### ğŸš€ **DEPLOYMENT STATUS:**
- âœ… **Git committed & pushed**: Enterprise dashboard refactor deployed
- âœ… **ArgoCD sync**: infrastructure-dashboards-operator wird deployed
- âœ… **ServiceMonitors**: Kafka, CNPG, Sail Operator cross-namespace ready
- âœ… **GrafanaDashboards**: 20+ enterprise applications organiziert
