# ğŸ” GRAFANA DASHBOARD TROUBLESHOOTING GUIDE

## âœ… DASHBOARD STATUS ÃœBERSICHT (2025-10-02)

### ğŸ“Š TOTAL: 61 Enterprise Dashboards Deployed

#### ğŸ—ï¸ **Infrastructure Dashboards (10)**
| Dashboard | ID | Status | Datasource |
|-----------|-----|--------|------------|
| k8s-views-global | 15757 | âœ… Working | Prometheus |
| k8s-views-namespaces | 15758 | âœ… Working | Prometheus |
| k8s-views-nodes | 15759 | âœ… Working | Prometheus |
| k8s-views-pods | 15760 | âœ… Working | Prometheus |
| k8s-system-api-server | 15761 | âœ… NEW | Prometheus |
| k8s-system-coredns | 15762 | âœ… NEW | Prometheus |
| k8s-addons-prometheus | 19105 | âœ… NEW | Prometheus |
| talos-cluster-health | 5312 | âœ… FIXED | Prometheus |
| talos-control-plane | Custom | âœ… Working | Prometheus |
| talos-etcd | Custom | âœ… Working | Prometheus |

#### ğŸŒ **Network Dashboards (3)**
| Dashboard | ID | Status | Datasource |
|-----------|-----|--------|------------|
| cilium-agent | v1.12 | âœ… Working | Prometheus |
| cilium-operator | v1.12 | âœ… Working | Prometheus |
| cilium-hubble | v1.12 | âœ… Working | Prometheus |

#### ğŸ’¾ **Storage Dashboards (3)**
| Dashboard | ID | Status | Datasource |
|-----------|-----|--------|------------|
| rook-ceph-cluster | 2842 | âœ… FIXED | Prometheus |
| rook-ceph-pools | Custom | âœ… Working | Prometheus |
| rook-ceph-osd | Custom | âœ… Working | Prometheus |

#### ğŸ˜ **Database Dashboards (6)**
| Dashboard | ID | Status | Datasource |
|-----------|-----|--------|------------|
| cloudnativepg-cluster | 20417 | âœ… Working | Prometheus |
| postgresql-database | 9628 | âœ… Working | Prometheus |
| postgresql-kube-prometheus | Custom | âœ… Working | Prometheus |
| postgresql-exporter-quickstart | Custom | âœ… Working | Prometheus |
| postgresql-overview | Custom | âœ… Working | Prometheus |
| elasticsearch-cluster | Custom | âœ… Working | Prometheus |

#### ğŸ¯ **GitOps & Platform (6)**
| Dashboard | ID | Status | Datasource |
|-----------|-----|--------|------------|
| argocd-app | Custom | âœ… Working | Prometheus |
| argocd-operational | Custom | âœ… Working | Prometheus |
| argocd-official | Custom | âœ… Working | Prometheus |
| argocd-notifications | Custom | âœ… Working | Prometheus |
| sealed-secrets-controller | Custom | âœ… Working | Prometheus |
| kafka (Strimzi) | Custom | âœ… Working | Prometheus |

#### ğŸ“Š **Observability Dashboards (10)**
| Dashboard | ID | Status | Datasource |
|-----------|-----|--------|------------|
| loki-stack-monitoring | Custom | âœ… Working | Prometheus |
| loki-logs-dashboard | Custom | âœ… Working | Prometheus |
| loki-promtail | Custom | âœ… Working | Prometheus |
| jaeger-all-in-one | Custom | âš ï¸ Partial | Prometheus |
| otel-collector | Custom | âœ… Working | Prometheus |
| vector-cluster | Custom | âš ï¸ Partial | Prometheus |

---

## ğŸš¨ PROBLEM: "No Data" in Dashboards

### ROOT CAUSES IDENTIFIED:

#### 1ï¸âƒ£ **ServiceMonitor Label Mismatch** (FIXED âœ…)
**Problem**: ServiceMonitors had wrong `release` label
```yaml
# âŒ WRONG (old):
release: prometheus-operator

# âœ… CORRECT (fixed):
release: kube-prometheus-stack
```

**Fixed Files**:
- `rook-ceph/servicemonitor-ceph-mgr.yaml`
- `rook-ceph/servicemonitor-ceph-exporter.yaml`

#### 2ï¸âƒ£ **ServiceMonitor Selector Mismatch** (FIXED âœ…)
**Problem**: ServiceMonitor selector didn't match Service labels
```yaml
# âŒ WRONG:
selector:
  matchLabels:
    ceph_daemon_type: mgr  # Service doesn't have this label!

# âœ… CORRECT:
selector:
  matchLabels:
    app: rook-ceph-mgr
    rook_cluster: rook-ceph
```

#### 3ï¸âƒ£ **Wrong Dashboard Import** (FIXED âœ…)
**Problem**: Dashboard 8073 is generic Kubernetes, not Talos-optimized

**Solution**: Replaced with Dashboard 5312 (Kubernetes Cluster Health - Prometheus)

#### 4ï¸âƒ£ **Missing Metrics Services** (PARTIALLY FIXED âš ï¸)
**Problem**: Some services don't expose metrics endpoints

**Known Issues**:
- âŒ Jaeger Query: Port mismatch (1 target DOWN)
- âŒ Vector Aggregator: Service down (2 targets DOWN)
- âŒ Loki Gateway: 1 target DOWN

---

## ğŸ”§ DEBUGGING WORKFLOW

### Step 1: Check Prometheus Targets
```bash
export KUBECONFIG=/path/to/kubeconfig
kubectl exec -n monitoring prometheus-kube-prometheus-stack-prometheus-0 -- \\
  wget -qO- 'http://localhost:9090/api/v1/targets' | jq '.data.activeTargets[] | select(.health!="up")'
```

### Step 2: Check ServiceMonitor Labels
```bash
kubectl get servicemonitor -n <namespace> -o yaml | grep -A 3 "labels:"
```

**Required Labels**:
```yaml
metadata:
  labels:
    release: kube-prometheus-stack  # âš ï¸ CRITICAL!
```

### Step 3: Verify Service Selector
```bash
# Get ServiceMonitor selector
kubectl get servicemonitor <name> -n <namespace> -o jsonpath='{.spec.selector.matchLabels}'

# Get Service labels
kubectl get service <name> -n <namespace> -o jsonpath='{.metadata.labels}'

# âœ… They MUST match!
```

### Step 4: Test Metrics Endpoint
```bash
kubectl exec -n <namespace> <pod-name> -- wget -qO- http://localhost:<port>/metrics
```

### Step 5: Query Prometheus
```bash
kubectl exec -n monitoring prometheus-kube-prometheus-stack-prometheus-0 -- \\
  wget -qO- 'http://localhost:9090/api/v1/query?query=up{job="<job-name>"}'
```

---

## ğŸ“‹ CRITICAL METRICS REFERENCE

### **API Server**
```promql
up{job="apiserver"}                                    # API Server is UP
apiserver_request_total                                # Total API requests
apiserver_request_duration_seconds                     # Request latency
```

### **ETCD**
```promql
up{job="kube-etcd"}                                    # ETCD is UP
etcd_server_has_leader                                 # Leader status
etcd_disk_backend_commit_duration_seconds              # Disk performance
```

### **Nodes**
```promql
kube_node_info                                         # Node information
kube_node_status_condition{condition="Ready"}          # Node readiness
node_memory_MemAvailable_bytes                         # Available memory
node_cpu_seconds_total                                 # CPU usage
```

### **Pods**
```promql
kube_pod_info                                          # Pod information
kube_pod_status_phase{phase="Running"}                # Running pods
kube_pod_container_status_restarts_total               # Restart count
```

### **Ceph Storage**
```promql
ceph_cluster_total_bytes                               # Total storage
ceph_cluster_total_used_bytes                          # Used storage
ceph_health_status                                     # Ceph health (0=OK)
ceph_osd_up                                            # OSDs UP
```

---

## ğŸ¯ DASHBOARD RECOMMENDATIONS

### **Best Talos/Kubernetes Dashboards**:

1. **k8s-views-global** (15757) - Best overall cluster view
2. **k8s-system-api-server** (15761) - API server deep dive
3. **talos-cluster-health** (5312) - Cluster health overview
4. **rook-ceph-cluster** (2842) - Storage monitoring
5. **k8s-addons-prometheus** (19105) - Prometheus self-monitoring

### **Avoid These**:
- âŒ Dashboard 8073 (old, generic Kubernetes)
- âŒ Dashboard 315 (outdated queries)
- âŒ Any dashboard without datasource override

---

## âœ… VERIFICATION CHECKLIST

Before reporting "No Data":

- [ ] ServiceMonitor has `release: kube-prometheus-stack` label
- [ ] ServiceMonitor selector matches Service labels
- [ ] Service has `metrics` port defined
- [ ] Metrics endpoint returns data (curl test)
- [ ] Prometheus target shows "up" status
- [ ] Dashboard datasourceName = "Prometheus"
- [ ] Grafana can query Prometheus datasource

---

## ğŸš€ FIXES APPLIED (2025-10-02)

1. âœ… Fixed Ceph ServiceMonitor labels â†’ Metrics now flowing
2. âœ… Fixed Ceph ServiceMonitor selectors â†’ All targets UP
3. âœ… Replaced Dashboard 8073 with 5312 â†’ Better queries
4. âœ… Added k8s-system-api-server (15761) â†’ API server visibility
5. âœ… Added k8s-system-coredns (15762) â†’ CoreDNS monitoring
6. âœ… Added k8s-addons-prometheus (19105) â†’ Prometheus health

---

## ğŸ“Š PROMETHEUS TARGETS STATUS

**Total Targets: 119**

- âœ… UP: 116 targets (97.5%)
- âŒ DOWN: 3 targets (2.5%)
  - jaeger-query (port mismatch)
  - loki-gateway (config issue)
  - vector-aggregator (service down)

**Critical Services: ALL UP âœ…**
- API Server: âœ… UP
- ETCD: âœ… UP
- Controller Manager: âœ… UP
- Scheduler: âœ… UP
- Kubelet: âœ… UP (21 targets)
- Node Exporter: âœ… UP (7 targets)
- Ceph: âœ… UP (7 targets)

---

## ğŸ”— REFERENCES

- [dotdc/grafana-dashboards-kubernetes](https://github.com/dotdc/grafana-dashboards-kubernetes)
- [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack)
- [Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator)
- [ServiceMonitor Spec](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor)
