# âš™ï¸ ROBUSTA HELM VALUES - ENTERPRISE ALERT ENRICHMENT WITH AI
# ==============================================================
# StateLESS architecture = SealedSecrets compatible! âœ…
# HolmesGPT with Ollama = Self-hosted AI, kostenlos! ğŸ¤–

# ğŸ” CRITICAL: SealedSecrets Pattern
# Reference existing secret created separately
runner:
  # ğŸ“Š Enterprise resource limits
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

  # âœ… SealedSecrets Compatible Pattern
  # Secret created separately, referenced here
  additional_env_vars:
    - name: SLACK_API_KEY
      valueFrom:
        secretKeyRef:
          name: robusta-slack-token
          key: api_key

# ğŸ“¨ Slack Integration (OAuth Bot Token)
sinksConfig:
  - slack_sink:
      name: homelab_slack
      api_key: "{{ env.SLACK_API_KEY }}"
      slack_channel: "#homelab-alerts"

# ğŸ¯ Global Configuration
clusterName: "timour-homelab-talos"  # REQUIRED by Robusta (TOP-LEVEL!)

globalConfig:
  # ğŸ“Š Prometheus Integration
  prometheus_url: "http://kube-prometheus-stack-prometheus.monitoring.svc:9090"
  alertmanager_url: "http://kube-prometheus-stack-alertmanager.monitoring.svc:9093"

# ğŸ”’ Security Context
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

# ğŸ“Š Monitoring
enablePrometheusStack: false  # We already have kube-prometheus-stack
enableServiceMonitors: true

# ğŸ¤– AI-POWERED DEBUGGING WITH HOLMESGPT + OLLAMA
# ================================================
# âœ… Self-hosted AI (kostenlos, privacy-first!)
# âœ… Uses local Ollama in ai-inference namespace
# âœ… Phi3:mini model (2.2GB, Microsoft, excellent for technical tasks)
enableHolmesGPT: true

holmesGPT:
  # ğŸ¯ Ollama endpoint (internal cluster service)
  apiUrl: "http://ollama.ai-inference.svc:11434/v1"

  # ğŸ§  Model: phi3:mini (Microsoft Phi-3)
  # Best for: Code analysis, technical reasoning, debugging
  model: "phi3:mini"

  # ğŸ”“ No API key needed for self-hosted Ollama!
  # apiKey: ""  # Not required for Ollama

  # ğŸ›ï¸ AI Configuration
  temperature: 0.2  # Lower = more focused, deterministic (good for debugging)
  maxTokens: 2048   # Enough for detailed analysis

  # âš¡ Performance
  timeout: 60  # 60 seconds timeout for AI responses

# ğŸš€ Playbooks
enablePlatformPlaybooks: true

# ğŸ¨ AI Features Configuration
aiAnalysis:
  # Enable AI-powered root cause analysis
  enabled: true

  # Analyze these alert types with AI
  alertTypes:
    - "CrashLoopBackOff"
    - "OOMKilled"
    - "ImagePullBackOff"
    - "NodeNotReady"
    - "PodPending"
    - "DeploymentReplicasMismatch"

  # Add AI insights to Slack messages
  enrichSlackAlerts: true
