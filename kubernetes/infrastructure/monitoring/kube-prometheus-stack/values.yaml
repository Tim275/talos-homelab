# yamllint disable rule:line-length
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
prometheus:
  prometheusSpec:
    # Retention configuration (prevents disk full)
    retention: 15d           # 15 days retention (homelab standard)
    retentionSize: "45GB"    # 90% of 50GB PVC (prevents disk full)

    # ğŸ”§ Extended startup probe for slow Ceph storage WAL loading
    # WAL loading takes ~2-3 min per segment on Ceph block storage
    # With 68 WAL segments, startup can take 1.5-2 hours
    startupProbe:
      enabled: true
      initialDelaySeconds: 60        # Wait 60s before first probe
      periodSeconds: 30              # Check every 30s
      timeoutSeconds: 10             # 10s timeout per probe
      failureThreshold: 300          # Allow 300 failures = 2.5+ hours startup time

    podMonitorNamespaceSelector: {}
    podMonitorSelectorNilUsesHelmValues: false
    podMonitorSelector: {}
    serviceMonitorNamespaceSelector: {}
    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector: {}
    # nodeSelector:
    #   topology.kubernetes.io/zone: abel
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: rook-ceph-block-enterprise  # Use your correct storage class
          # volumeName: pv-prometheus  # Let Kubernetes auto-provision
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 50G
          # selector:  # Remove specific PV selector, let Kubernetes handle it
          #   matchLabels:
          #     app: prometheus

# https://github.com/siderolabs/talos/discussions/7214
kubeControllerManager:
  enabled: true
  serviceMonitor:
    relabelings:
      - sourceLabels: [ __meta_kubernetes_pod_node_name ]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace
    metricRelabelings:
      - action: labeldrop
        regex: pod

kubeEtcd:
  enabled: true
  service:
    selector:
      # etcd doesn't run as a container,
      # but most probably runs on the same nodes that host a controller
      k8s-app: kube-controller-manager
  serviceMonitor:
    relabelings:
      - sourceLabels: [ __meta_kubernetes_pod_node_name ]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace
    metricRelabelings:
      - action: labeldrop
        regex: pod

kubeProxy:
  # Cilium replaces Kube Proxy
  enabled: false

kubeScheduler:
  enabled: true
  serviceMonitor:
    relabelings:
      - sourceLabels: [ __meta_kubernetes_pod_node_name ]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace
    metricRelabelings:
      - action: labeldrop
        regex: pod

nodeExporter:
  enabled: true

grafana:
  enabled: false
  forceDeployDatasources: false
  forceDeployDashboards: false
  defaultDashboardsEnabled: false
  defaultDashboardsTimezone: Europe/Berlin  # Fixed timezone for Germany
  operator:
    dashboardsConfigMapRefEnabled: true
    matchLabels:
      app: grafana
    folder: Kubernetes

# Disable all default Prometheus rules - we use custom enterprise rules
defaultRules:
  create: false
  rules:
    alertmanager: false
    etcd: false
    configReloaders: false
    general: false
    k8s: false
    kubeApiserverAvailability: false
    kubeApiserverBurnrate: false
    kubeApiserverHistogram: false
    kubeApiserverSlos: false
    kubeControllerManager: false
    kubelet: false
    kubeProxy: false
    kubePrometheusGeneral: false
    kubePrometheusNodeRecording: false
    kubernetesApps: false
    kubernetesResources: false
    kubernetesStorage: false
    kubernetesSystem: false
    kubeSchedulerAlerting: false
    kubeSchedulerRecording: false
    kubeStateMetrics: false
    network: false
    node: false
    nodeExporterAlerting: false
    nodeExporterRecording: false
    prometheus: false
    prometheusOperator: false
    windows: false

prometheusOperator:
  admissionWebhooks:
    enabled: false  # Disable admission webhooks completely
    patch:
      enabled: false
    certManager:
      enabled: false
  # Disable TLS completely to avoid cert mount issues
  tls:
    enabled: false
  # ğŸ›¡ï¸ Homelab Stability: Resource limits for Prometheus Operator
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 1000m     # 10x burst for CRD operations
      memory: 1Gi    # 4x headroom for operator

# ğŸ›¡ï¸ Homelab Stability: Resource limits for Kube State Metrics
kube-state-metrics:
  resources:
    requests:
      cpu: 50m       # Actual: ~7m CPU
      memory: 64Mi   # Actual: ~27Mi memory
    limits:
      cpu: 500m      # 10x burst for metrics collection
      memory: 512Mi  # ~20x headroom for cluster growth

  # ğŸ”§ Fix CrashLoopBackOff: Increase probe delays (was 5s, now 30s)
  # Root cause: kube-state-metrics needs time to start and expose health endpoints
  livenessProbe:
    initialDelaySeconds: 30  # Changed from default 5s to prevent premature probe failures
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  readinessProbe:
    initialDelaySeconds: 30  # Changed from default 5s to allow full startup
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

# ============================================================================
# ENTERPRISE TIER-0 ALERTMANAGER CONFIG
# Based on: Google SRE Workbook, ITIL Priority Matrix, Prometheus Best Practices
# References:
#   - https://sre.google/workbook/alerting-on-slos/
#   - https://www.sysdig.com/blog/prometheus-alertmanager
#   - https://fibery.io/blog/product-management/p0-p1-p2-p3-p4/
# ============================================================================
#
# PRIORITY LEVELS (ITIL-based):
# â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Prio â”‚ Response    â”‚ Channel         â”‚ Description                        â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ P0   â”‚ Immediate   â”‚ #alerts-criticalâ”‚ Total outage, data loss risk       â”‚
# â”‚ P1   â”‚ < 15 min    â”‚ #alerts-criticalâ”‚ Major degradation, users impacted  â”‚
# â”‚ P2   â”‚ < 1 hour    â”‚ #alerts         â”‚ Partial degradation, workaround    â”‚
# â”‚ P3   â”‚ < 4 hours   â”‚ #alerts         â”‚ Minor issue, no user impact        â”‚
# â”‚ P4   â”‚ Next day    â”‚ #alerts-info    â”‚ Informational, maintenance         â”‚
# â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# SEVERITY LEVELS:
#   critical = Service down, immediate action required
#   warning  = Degraded, action needed soon
#   info     = Informational, no action needed
# ============================================================================

alertmanager:
  alertmanagerSpec:
    replicas: 1
    retention: 120h
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: rook-ceph-block-enterprise
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi
    secrets:
      - slack-webhook-url
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 256Mi

  config:
    global:
      resolve_timeout: 5m
      slack_api_url_file: /etc/alertmanager/secrets/slack-webhook-url/url

    # ========================================================================
    # ROUTING TREE - Priority-based with severity fallback
    # ========================================================================
    route:
      receiver: 'slack-default'
      group_by: ['alertname', 'namespace', 'severity']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h

      routes:
        # ============ P0: CRITICAL OUTAGE ============
        # Total service failure, data loss risk
        - matchers:
            - severity="critical"
            - priority=~"p0|P0"
          receiver: 'slack-critical'
          group_wait: 10s
          group_interval: 1m
          repeat_interval: 30m
          continue: false

        # ============ P1: MAJOR INCIDENT ============
        # Major degradation, users actively impacted
        - matchers:
            - severity="critical"
          receiver: 'slack-critical'
          group_wait: 10s
          group_interval: 2m
          repeat_interval: 1h
          continue: false

        - matchers:
            - priority=~"p1|P1"
          receiver: 'slack-critical'
          group_wait: 15s
          group_interval: 3m
          repeat_interval: 1h
          continue: false

        # ============ P2: DEGRADED SERVICE ============
        # Partial issues, workaround available
        - matchers:
            - severity="warning"
          receiver: 'slack-default'
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 2h
          continue: false

        - matchers:
            - priority=~"p2|P2"
          receiver: 'slack-default'
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 2h
          continue: false

        # ============ P3/P4: LOW PRIORITY ============
        # Minor issues, informational
        - matchers:
            - severity="info"
          receiver: 'slack-info'
          group_wait: 1m
          group_interval: 10m
          repeat_interval: 12h
          continue: false

        - matchers:
            - priority=~"p3|P3|p4|P4"
          receiver: 'slack-info'
          group_wait: 1m
          group_interval: 10m
          repeat_interval: 12h
          continue: false

    # ========================================================================
    # INHIBITION RULES - Suppress lower priority when higher is firing
    # Based on: https://www.neteye-blog.com/2025/06/alertmanager-alert-filtering-rules/
    # ========================================================================
    inhibit_rules:
      # Critical suppresses Warning for same alert
      - source_matchers:
          - severity="critical"
        target_matchers:
          - severity="warning"
        equal: ['alertname', 'namespace']

      # Critical suppresses Info
      - source_matchers:
          - severity="critical"
        target_matchers:
          - severity="info"
        equal: ['alertname', 'namespace']

      # Warning suppresses Info
      - source_matchers:
          - severity="warning"
        target_matchers:
          - severity="info"
        equal: ['alertname', 'namespace']

      # P0 suppresses P1/P2/P3/P4
      - source_matchers:
          - priority=~"p0|P0"
        target_matchers:
          - priority=~"p1|P1|p2|P2|p3|P3|p4|P4"
        equal: ['job', 'namespace']

      # P1 suppresses P2/P3/P4
      - source_matchers:
          - priority=~"p1|P1"
        target_matchers:
          - priority=~"p2|P2|p3|P3|p4|P4"
        equal: ['job', 'namespace']

      # Node down suppresses all pod alerts on that node
      - source_matchers:
          - alertname="NodeDown"
        target_matchers:
          - alertname=~"Pod.*|Container.*|Kube.*"
        equal: ['node']

      # Cluster down suppresses node alerts
      - source_matchers:
          - alertname=~".*ClusterDown|.*ClusterUnhealthy"
        target_matchers:
          - alertname=~".*NodeDown|.*OSDDown"
        equal: ['cluster']

      # API Server down suppresses controller alerts
      - source_matchers:
          - alertname="KubeAPIServerDown"
        target_matchers:
          - alertname=~"KubeController.*|KubeScheduler.*|ArgoCD.*"

    # ========================================================================
    # RECEIVERS - All with unified detailed templates
    # ========================================================================
    receivers:
      # ============ CRITICAL: P0/P1 ============
      - name: 'slack-critical'
        slack_configs:
          - channel: '#alerts'
            send_resolved: true
            title: |-
              {{ if eq .Status "firing" }}:rotating_light:{{ else }}:white_check_mark:{{ end }} [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}
            text: |-
              {{ range .Alerts }}
              *Alert:* {{ .Labels.alertname }}
              *Severity:* :red_circle: {{ .Labels.severity | toUpper }}
              {{ if .Labels.priority }}*Priority:* {{ .Labels.priority | toUpper }}{{ end }}
              {{ if .Labels.namespace }}*Namespace:* `{{ .Labels.namespace }}`{{ end }}
              {{ if .Labels.pod }}*Pod:* `{{ .Labels.pod }}`{{ end }}
              {{ if .Labels.container }}*Container:* `{{ .Labels.container }}`{{ end }}
              {{ if .Labels.node }}*Node:* `{{ .Labels.node }}`{{ end }}
              {{ if .Labels.job }}*Job:* `{{ .Labels.job }}`{{ end }}
              {{ if .Labels.component }}*Component:* `{{ .Labels.component }}`{{ end }}
              {{ if .Labels.layer }}*Layer:* {{ .Labels.layer }}{{ end }}
              {{ if .Labels.tier }}*Tier:* {{ .Labels.tier }}{{ end }}
              *Summary:* {{ .Annotations.summary }}
              {{ if .Annotations.description }}*Description:* {{ .Annotations.description }}{{ end }}
              {{ if .Annotations.impact }}*Impact:* {{ .Annotations.impact }}{{ end }}
              {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
              {{ end }}
            color: |-
              {{ if eq .Status "firing" }}danger{{ else }}good{{ end }}
            actions:
              - type: button
                text: 'Prometheus'
                url: '{{ (index .Alerts 0).GeneratorURL }}'
              - type: button
                text: 'Grafana'
                url: 'https://grafana.timourhomelab.org'
              - type: button
                text: 'Silence 2h'
                url: '{{ .ExternalURL }}/#/silences/new?filter=%7Balertname%3D%22{{ .CommonLabels.alertname }}%22%7D'

      # ============ DEFAULT: P2/Warning ============
      - name: 'slack-default'
        slack_configs:
          - channel: '#alerts'
            send_resolved: true
            title: |-
              {{ if eq .Status "firing" }}:warning:{{ else }}:white_check_mark:{{ end }} [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}
            text: |-
              {{ range .Alerts }}
              *Alert:* {{ .Labels.alertname }}
              *Severity:* :large_orange_circle: {{ .Labels.severity | toUpper }}
              {{ if .Labels.priority }}*Priority:* {{ .Labels.priority | toUpper }}{{ end }}
              {{ if .Labels.namespace }}*Namespace:* `{{ .Labels.namespace }}`{{ end }}
              {{ if .Labels.pod }}*Pod:* `{{ .Labels.pod }}`{{ end }}
              {{ if .Labels.container }}*Container:* `{{ .Labels.container }}`{{ end }}
              {{ if .Labels.node }}*Node:* `{{ .Labels.node }}`{{ end }}
              {{ if .Labels.job }}*Job:* `{{ .Labels.job }}`{{ end }}
              {{ if .Labels.component }}*Component:* `{{ .Labels.component }}`{{ end }}
              *Summary:* {{ .Annotations.summary }}
              {{ if .Annotations.description }}*Description:* {{ .Annotations.description }}{{ end }}
              {{ if .Annotations.impact }}*Impact:* {{ .Annotations.impact }}{{ end }}
              {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
              {{ end }}
            color: |-
              {{ if eq .Status "firing" }}warning{{ else }}good{{ end }}
            actions:
              - type: button
                text: 'Prometheus'
                url: '{{ (index .Alerts 0).GeneratorURL }}'
              - type: button
                text: 'Grafana'
                url: 'https://grafana.timourhomelab.org'

      # ============ INFO: P3/P4 ============
      - name: 'slack-info'
        slack_configs:
          - channel: '#alerts'
            send_resolved: false
            title: |-
              :information_source: [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}
            text: |-
              {{ range .Alerts }}
              *Alert:* {{ .Labels.alertname }}
              *Severity:* :large_blue_circle: {{ .Labels.severity | toUpper }}
              {{ if .Labels.priority }}*Priority:* {{ .Labels.priority | toUpper }}{{ end }}
              {{ if .Labels.namespace }}*Namespace:* `{{ .Labels.namespace }}`{{ end }}
              {{ if .Labels.pod }}*Pod:* `{{ .Labels.pod }}`{{ end }}
              {{ if .Labels.job }}*Job:* `{{ .Labels.job }}`{{ end }}
              *Summary:* {{ .Annotations.summary }}
              {{ if .Annotations.description }}*Description:* {{ .Annotations.description }}{{ end }}
              {{ end }}
            color: '#439FE0'
            actions:
              - type: button
                text: 'View Details'
                url: '{{ (index .Alerts 0).GeneratorURL }}'
