# ============================================================================
# LOKI LOG AGGREGATION ALERTS
# Enterprise Tier-0: Monitor log infrastructure
# ============================================================================
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: loki-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus-stack
    release: kube-prometheus-stack
    tier: observability
spec:
  groups:
    # ========================================================================
    # LOKI CRITICAL ALERTS
    # ========================================================================
    - name: loki.critical
      interval: 30s
      rules:
        # ====== P1: CRITICAL - Loki Down ======
        - alert: LokiDown
          expr: |
            up{job=~"loki.*"} == 0
          for: 5m
          labels:
            severity: critical
            priority: p1
            component: loki
            tier: observability
            layer: observability
          annotations:
            summary: "Loki is DOWN"
            description: |
              Loki log aggregation service is unreachable!
              LOGS ARE NOT BEING COLLECTED!

              Job: {{ $labels.job }}
              Instance: {{ $labels.instance }}
              Duration: >5 minutes

              Impact: No log visibility, debugging impossible
              Team: @platform-oncall

              Check: kubectl -n monitoring get pods -l app.kubernetes.io/name=loki

        # ====== P1: CRITICAL - Loki All Replicas Down ======
        - alert: LokiAllReplicasDown
          expr: |
            count(up{job=~"loki.*"} == 1) == 0
          for: 3m
          labels:
            severity: critical
            priority: p1
            component: loki
            tier: observability
            layer: observability
          annotations:
            summary: "All Loki replicas are DOWN"
            description: |
              All Loki instances are unreachable!
              COMPLETE LOG INFRASTRUCTURE FAILURE!

              Healthy Replicas: 0

              Impact: No logs collected, complete observability blind spot
              Team: @platform-oncall

    # ========================================================================
    # LOKI HIGH ALERTS
    # ========================================================================
    - name: loki.high
      interval: 30s
      rules:
        # ====== P2: HIGH - Loki Ingestion Rate Throttled ======
        - alert: LokiIngestionRateThrottled
          expr: |
            sum(rate(loki_distributor_bytes_received_total[5m])) by (job)
            / 1024 / 1024 > 50
          for: 15m
          labels:
            severity: warning
            priority: p2
            component: loki
            tier: observability
            layer: observability
          annotations:
            summary: "Loki ingestion rate very high (>50MB/s)"
            description: |
              Loki is receiving logs at {{ $value | printf "%.1f" }}MB/s.
              Risk of throttling and log loss!

              Current Rate: {{ $value | printf "%.1f" }}MB/s
              Threshold: 50MB/s

              Action: Check for log spam, noisy applications

        # ====== P2: HIGH - Loki Query Errors ======
        - alert: LokiQueryErrors
          expr: |
            sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[5m])) by (job)
            /
            sum(rate(loki_request_duration_seconds_count[5m])) by (job)
            > 0.05
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: loki
            tier: observability
            layer: observability
          annotations:
            summary: "Loki query error rate >5%"
            description: |
              Loki is returning errors for {{ $value | humanizePercentage }} of queries.

              Error Rate: {{ $value | humanizePercentage }}
              Threshold: 5%

              Impact: Log queries failing, troubleshooting impaired
              Action: Check Loki logs for errors

        # ====== P2: HIGH - Loki Request Latency High ======
        # NOTE: Excludes tail endpoints which are long-lived WebSocket connections
        # Tail connections naturally have high "latency" because they stream continuously
        # Route formats: /loki/api/v1/tail OR loki_api_v1_tail (underscore variant)
        - alert: LokiRequestLatencyHigh
          expr: |
            histogram_quantile(0.99,
              sum(rate(loki_request_duration_seconds_bucket{route!~".*[/_]tail|.*[/_]Tail"}[5m])) by (job, le, route)
            ) > 10
          for: 15m
          labels:
            severity: warning
            priority: p2
            component: loki
            tier: observability
            layer: observability
          annotations:
            summary: "Loki query latency >10s (p99)"
            description: |
              Loki p99 query latency is {{ $value | printf "%.1f" }}s.

              Route: {{ $labels.route }}
              p99 Latency: {{ $value | printf "%.1f" }}s
              Threshold: 10s

              Action: Check Loki resources and query patterns

        # ====== P2: HIGH - Loki Ingester Not Ready ======
        - alert: LokiIngesterNotReady
          expr: |
            loki_ingester_checkpoint_logged_bytes > 0
            unless
            loki_ingester_checkpoint_logged_bytes
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: loki
            tier: observability
            layer: observability
          annotations:
            summary: "Loki ingester not ready"
            description: |
              Loki ingester is not ready to accept logs.
              Logs may be dropped!

              Action: Check Loki ingester logs and state

        # ====== P2: HIGH - Loki S3 Errors ======
        - alert: LokiS3Errors
          expr: |
            sum(rate(loki_boltdb_shipper_request_duration_seconds_count{status="error"}[5m])) > 0
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: loki
            tier: observability
            layer: observability
          annotations:
            summary: "Loki S3 storage errors"
            description: |
              Loki is experiencing errors communicating with S3 storage.
              Log persistence at risk!

              Action: Check Ceph RGW status and Loki S3 credentials

    # ========================================================================
    # LOKI MEDIUM ALERTS
    # ========================================================================
    - name: loki.medium
      interval: 60s
      rules:
        # ====== P3: MEDIUM - Loki Compactor Not Running ======
        - alert: LokiCompactorNotRunning
          expr: |
            loki_compactor_running == 0
          for: 30m
          labels:
            severity: info
            priority: p3
            component: loki
            tier: observability
            layer: observability
          annotations:
            summary: "Loki compactor not running"
            description: |
              Loki compactor is not running.
              Retention and deduplication not active!

              Impact: Storage not being cleaned up
              Action: Check compactor configuration

        # ====== P3: MEDIUM - Loki Canary Latency High ======
        - alert: LokiCanaryLatencyHigh
          expr: |
            histogram_quantile(0.99,
              sum(rate(loki_canary_response_latency_seconds_bucket[5m])) by (le)
            ) > 5
          for: 15m
          labels:
            severity: info
            priority: p3
            component: loki
            tier: observability
            layer: observability
          annotations:
            summary: "Loki canary latency >5s"
            description: |
              Loki canary write-to-read latency is {{ $value | printf "%.1f" }}s.
              Log visibility is delayed.

              p99 Latency: {{ $value | printf "%.1f" }}s
              Threshold: 5s

        # ====== P3: MEDIUM - Loki Stream Rate Limit ======
        - alert: LokiStreamRateLimited
          expr: |
            sum(rate(loki_discarded_samples_total{reason="rate_limited"}[5m])) > 0
          for: 15m
          labels:
            severity: info
            priority: p3
            component: loki
            tier: observability
            layer: observability
          annotations:
            summary: "Loki dropping logs due to rate limiting"
            description: |
              Loki is rate limiting log streams.
              Some logs are being dropped!

              Dropped Rate: {{ $value }}/s

              Action: Increase per_stream_rate_limit or reduce log volume
