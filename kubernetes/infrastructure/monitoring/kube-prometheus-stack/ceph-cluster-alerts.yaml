apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ceph-cluster-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus-stack
    release: kube-prometheus-stack
    layer: storage
spec:
  groups:
    # ============================================================================
    # CEPH CLUSTER HEALTH ALERTS
    # Enterprise-grade Ceph monitoring based on SUSE/Red Hat best practices
    # ============================================================================
    - name: ceph.health.critical
      interval: 30s
      rules:
        # ====== P1: CRITICAL - Ceph Health ERROR ======
        - alert: CephHealthError
          expr: |
            ceph_health_status == 2
          for: 5m
          labels:
            severity: critical
            priority: p1
            component: ceph
            tier: storage
          annotations:
            summary: "CRITICAL: Ceph cluster is in HEALTH_ERR state"
            description: |
              Ceph Cluster ist in HEALTH_ERR! Daten sind möglicherweise nicht verfügbar!

              Impact: I/O Fehler, Datenkorruption möglich
              Action: SOFORT ceph health detail prüfen!
              Team: @storage-oncall

              kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph health detail

        # ====== P1: CRITICAL - Multiple OSDs Down ======
        - alert: CephMultipleOSDsDown
          expr: |
            (ceph_osd_up == 0)
            and
            count(ceph_osd_up == 0) >= 2
          for: 5m
          labels:
            severity: critical
            priority: p1
            component: ceph-osd
            tier: storage
          annotations:
            summary: "CRITICAL: Multiple Ceph OSDs are DOWN"
            description: |
              Mehrere OSDs sind down! Data redundancy gefährdet!

              Action: OSD Status prüfen und wiederherstellen
              kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd tree

        # ====== P1: CRITICAL - Ceph MON Quorum Lost ======
        - alert: CephMonQuorumLost
          expr: |
            ceph_mon_quorum_status < 1
          for: 2m
          labels:
            severity: critical
            priority: p1
            component: ceph-mon
            tier: storage
          annotations:
            summary: "CRITICAL: Ceph MON has lost quorum"
            description: |
              MON {{ $labels.ceph_daemon }} ist out of quorum!

              Impact: Cluster kann keine Metadaten-Operationen durchführen
              Action: MON Status prüfen

    - name: ceph.health.warning
      interval: 30s
      rules:
        # ====== P2: WARNING - Ceph Health WARN ======
        - alert: CephHealthWarning
          expr: |
            ceph_health_status == 1
          for: 15m
          labels:
            severity: warning
            priority: p2
            component: ceph
            tier: storage
          annotations:
            summary: "Ceph cluster is in HEALTH_WARN state"
            description: |
              Ceph Cluster ist in HEALTH_WARN seit >15 Minuten.

              Mögliche Issues:
              - Degraded PGs
              - Slow OPS
              - MON down
              - OSD nearfull

              Action: ceph health detail

        # ====== P2: WARNING - Single OSD Down ======
        - alert: CephOSDDown
          expr: |
            ceph_osd_up == 0
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: ceph-osd
            tier: storage
          annotations:
            summary: "Ceph OSD {{ $labels.ceph_daemon }} is DOWN"
            description: |
              OSD {{ $labels.ceph_daemon }} ist seit >10 Minuten down.

              Impact: Reduced redundancy, recovery läuft
              Action: OSD logs prüfen, ggf. Pod restarten

        # ====== P2: WARNING - PGs Degraded ======
        - alert: CephPGsDegraded
          expr: |
            ceph_pg_degraded > 0
          for: 15m
          labels:
            severity: warning
            priority: p2
            component: ceph-pg
            tier: storage
          annotations:
            summary: "Ceph has {{ $value }} degraded PGs"
            description: |
              {{ $value }} Placement Groups sind degraded.

              Impact: Data redundancy temporarily reduced
              Action: Recovery abwarten oder OSD Status prüfen

        # ====== P2: WARNING - Slow OPS ======
        - alert: CephSlowOps
          expr: |
            ceph_healthcheck_slow_ops > 0
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: ceph-osd
            tier: storage
          annotations:
            summary: "Ceph has {{ $value }} slow operations"
            description: |
              {{ $value }} Operationen sind langsam (blocked).

              Mögliche Ursachen:
              - Disk I/O bottleneck
              - Network latency
              - OSD überlastet
              - Zu viel Recovery traffic

              Action: ceph daemon osd.X perf dump

        # ====== P2: WARNING - MON Down (but quorum intact) ======
        - alert: CephMonDown
          expr: |
            ceph_mon_num_sessions < ceph_mon_metadata
          for: 5m
          labels:
            severity: warning
            priority: p2
            component: ceph-mon
            tier: storage
          annotations:
            summary: "One or more Ceph MONs are down"
            description: |
              Ein oder mehrere MONs sind down, aber Quorum ist intakt.

              Action: MON Pods und logs prüfen

        # ====== P2: WARNING - OSD Nearfull ======
        - alert: CephOSDNearfull
          expr: |
            ceph_osd_stat_bytes_used / ceph_osd_stat_bytes > 0.80
          for: 5m
          labels:
            severity: warning
            priority: p2
            component: ceph-osd
            tier: storage
          annotations:
            summary: "Ceph OSD {{ $labels.ceph_daemon }} is >80% full"
            description: |
              OSD {{ $labels.ceph_daemon }} ist bei {{ $value | humanizePercentage }} Kapazität.

              Bei 85% wird OSD nearfull, bei 95% full (read-only)!

              Action: Storage erweitern oder Daten löschen

        # ====== P1: CRITICAL - OSD Full ======
        - alert: CephOSDFull
          expr: |
            ceph_osd_stat_bytes_used / ceph_osd_stat_bytes > 0.90
          for: 2m
          labels:
            severity: critical
            priority: p1
            component: ceph-osd
            tier: storage
          annotations:
            summary: "CRITICAL: Ceph OSD {{ $labels.ceph_daemon }} is >90% full"
            description: |
              OSD {{ $labels.ceph_daemon }} ist bei {{ $value | humanizePercentage }}!
              Bei 95% wird OSD read-only!

              Action: SOFORT Storage erweitern oder Daten löschen!

    - name: ceph.rgw.alerts
      interval: 30s
      rules:
        # ====== P2: WARNING - RGW High Latency ======
        - alert: CephRGWHighLatency
          expr: |
            rate(ceph_rgw_op_put_obj_lat_sum[5m]) / rate(ceph_rgw_op_put_obj_lat_count[5m]) > 1000
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: ceph-rgw
            tier: storage
          annotations:
            summary: "Ceph RGW PUT latency is high"
            description: |
              RGW PUT Object Latency ist >1 Sekunde.

              Impact: S3 clients (Loki, Velero) werden timeout haben
              Action: RGW und OSD Performance prüfen

        # ====== P2: WARNING - RGW Pod CrashLooping ======
        - alert: CephRGWCrashLooping
          expr: |
            rate(kube_pod_container_status_restarts_total{
              namespace="rook-ceph",
              container=~"rgw.*"
            }[15m]) * 60 * 15 > 3
          for: 5m
          labels:
            severity: warning
            priority: p2
            component: ceph-rgw
            tier: storage
          annotations:
            summary: "Ceph RGW is crash looping"
            description: |
              RGW Pod startet mehrfach neu.

              Impact: S3 storage unavailable
              Action: RGW logs prüfen
