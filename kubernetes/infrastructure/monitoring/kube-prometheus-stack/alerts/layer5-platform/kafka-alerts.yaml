apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kafka-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus-stack
    release: kube-prometheus-stack
    layer: platform
spec:
  groups:
    - name: platform.kafka.critical
      interval: 30s
      rules:
        # ====== P1: CRITICAL - Kafka Broker Down ======
        # Mit 3 Brokern kann ein Broker 0 Leader haben (normal nach Rebalancing)
        # PrÃ¼fe stattdessen ob Broker komplett weg ist (kein UP metric)
        - alert: KafkaBrokerDown
          expr: |
            up{job="kafka/kafka-broker-metrics"} == 0
          for: 3m
          labels:
            severity: critical
            priority: p1
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka broker {{ $labels.pod }} is DOWN"
            description: |
              Kafka broker {{ $labels.pod }} has no leader partitions.
              Broker is unreachable or crashed!

              Impact: Reduced capacity, potential message loss
              Team: @platform-oncall

              Check: kubectl -n kafka get pods -l app.kubernetes.io/name=kafka

        # ====== P1: CRITICAL - Kafka Under-Replicated Partitions ======
        - alert: KafkaUnderReplicatedPartitions
          expr: |
            kafka_server_replicamanager_underreplicatedpartitions > 0
          for: 5m
          labels:
            severity: critical
            priority: p1
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka has {{ $value }} under-replicated partitions on {{ $labels.pod }}"
            description: |
              Kafka broker {{ $labels.pod }} has {{ $value }} under-replicated partitions.
              Replicas are not syncing - DATA LOSS RISK!

              Impact: Reduced fault tolerance, potential data loss
              Duration: >5 minutes

              Check: kubectl -n kafka exec {{ $labels.pod }} -- \
                kafka-topics.sh --bootstrap-server localhost:9092 --describe --under-replicated-partitions

        # ====== P1: CRITICAL - Kafka Offline Partitions ======
        - alert: KafkaOfflinePartitions
          expr: |
            kafka_controller_kafkacontroller_offlinepartitionscount > 0
          for: 3m
          labels:
            severity: critical
            priority: p1
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka has {{ $value }} offline partitions"
            description: |
              Kafka cluster has {{ $value }} offline partitions.
              No available in-sync replicas - COMPLETE DATA UNAVAILABILITY!

              Impact: Message writes and reads FAILING
              Team: @platform-oncall

        # ====== P1: CRITICAL - Unclean Leader Election (DATA LOSS!) ======
        - alert: KafkaUncleanLeaderElection
          expr: |
            rate(kafka_controller_kafkacontroller_uncleanleaderelectionspersec[5m]) > 0
          for: 1m
          labels:
            severity: critical
            priority: p1
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "UNCLEAN LEADER ELECTION DETECTED - DATA LOSS!"
            description: |
              Kafka cluster experienced an unclean leader election!
              An out-of-sync replica became leader - MESSAGES LOST FOREVER!

              Impact: PERMANENT DATA LOSS
              Team: @platform-oncall IMMEDIATELY

              This happens when no in-sync replica is available and
              unclean.leader.election.enable=true (should be false in production!)

              Check: Review broker logs for election events

        # ====== P1: CRITICAL - Kafka Disk Usage Critical ======
        - alert: KafkaDiskUsageCritical
          expr: |
            kubelet_volume_stats_used_bytes{persistentvolumeclaim=~"data-my-cluster.*"}
            / kubelet_volume_stats_capacity_bytes > 0.9
          for: 5m
          labels:
            severity: critical
            priority: p1
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka disk usage > 90% on {{ $labels.persistentvolumeclaim }}"
            description: |
              Kafka broker PVC {{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full.
              IMMEDIATE ACTION REQUIRED - Broker will crash!

              Action: Reduce retention or expand storage NOW

              Fix: kubectl -n kafka exec my-cluster-dual-role-0 -- \
                kafka-configs.sh --bootstrap-server localhost:9092 \
                --entity-type topics --entity-name orders \
                --alter --add-config retention.ms=86400000

    - name: platform.kafka.high
      interval: 30s
      rules:
        # ====== P2: HIGH - Kafka Disk Usage High (Warning) ======
        - alert: KafkaDiskUsageHigh
          expr: |
            kubelet_volume_stats_used_bytes{persistentvolumeclaim=~"data-my-cluster.*"}
            / kubelet_volume_stats_capacity_bytes > 0.8
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka disk usage > 80% on {{ $labels.persistentvolumeclaim }}"
            description: |
              Kafka broker PVC {{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full.
              Plan storage expansion or reduce retention soon.

              Action: Monitor closely, will become critical at 90%

        # ====== P2: HIGH - Kafka Request Latency High ======
        - alert: KafkaHighRequestLatency
          expr: |
            histogram_quantile(0.99,
              sum(rate(kafka_network_requestmetrics_totaltimems_bucket[5m])) by (le, request)
            ) > 1000
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka {{ $labels.request }} request latency p99 > 1s"
            description: |
              Kafka request type {{ $labels.request }} has p99 latency of {{ $value }}ms.
              Broker may be overloaded or experiencing I/O issues.

              Duration: >10 minutes
              Action: Check broker resources, disk I/O, network

        # ====== P2: HIGH - Kafka Consumer Lag High ======
        - alert: KafkaConsumerLagHigh
          expr: |
            kafka_consumergroup_lag > 10000
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka consumer group {{ $labels.consumergroup }} lag is {{ $value }}"
            description: |
              Consumer group {{ $labels.consumergroup }} on topic {{ $labels.topic }} has lag {{ $value }}.
              Consumers not keeping up with producers!

              Duration: >10 minutes
              Action: Scale consumers or investigate processing delays

              Check: kubectl -n kafka exec my-cluster-dual-role-0 -- \
                kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group {{ $labels.consumergroup }}

        # ====== P2: HIGH - Kafka ISR Shrink Rate High ======
        - alert: KafkaISRShrinkRateHigh
          expr: |
            rate(kafka_server_replicamanager_isrshrinkspersec[5m]) > 0
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka ISR shrinking on {{ $labels.pod }}"
            description: |
              Kafka broker {{ $labels.pod }} ISR is shrinking at {{ $value }}/sec.
              Replicas falling behind - potential replication lag!

              Duration: >10 minutes
              Action: Check broker health and network connectivity

        # ====== P2: HIGH - Kafka Active Controller Missing ======
        - alert: KafkaNoActiveController
          expr: |
            sum(kafka_controller_kafkacontroller_activecontrollercount) == 0
          for: 3m
          labels:
            severity: warning
            priority: p2
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka cluster has no active controller"
            description: |
              No Kafka controller is currently active.
              Split-brain or controller election in progress!

              Impact: Cannot handle partition leadership changes
              Duration: >3 minutes

    - name: platform.kafka.medium
      interval: 60s
      rules:
        # ====== P3: MEDIUM - Kafka Request Handler Idle Low ======
        - alert: KafkaRequestHandlerIdleLow
          expr: |
            avg(kafka_server_kafkarequesthandlerpool_requesthandleravgidlepercent) < 0.2
          for: 15m
          labels:
            severity: info
            priority: p3
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka request handlers are {{ $value | humanizePercentage }} idle (high load)"
            description: |
              Kafka request handlers are only {{ $value | humanizePercentage }} idle.
              Brokers under high load - may need scaling!

              Duration: >15 minutes
              Action: Consider increasing broker resources or adding brokers

        # ====== P3: MEDIUM - Kafka Network Processor Idle Low ======
        - alert: KafkaNetworkProcessorIdleLow
          expr: |
            avg(kafka_network_socketserver_networkprocessoravgidlepercent) < 0.3
          for: 15m
          labels:
            severity: info
            priority: p3
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka network processors are {{ $value | humanizePercentage }} idle"
            description: |
              Kafka network processors are only {{ $value | humanizePercentage }} idle.
              Network layer is saturated - may need more network threads.

              Duration: >15 minutes
              Action: Consider increasing num.network.threads

        # ====== P3: MEDIUM - Kafka Log Directory Offline ======
        - alert: KafkaLogDirectoryOffline
          expr: |
            kafka_log_logmanager_offlinelogdirectorycount > 0
          for: 5m
          labels:
            severity: info
            priority: p3
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka has {{ $value }} offline log directories on {{ $labels.pod }}"
            description: |
              Kafka broker {{ $labels.pod }} has {{ $value }} offline log directories.
              Disk failure or mount issue detected!

              Duration: >5 minutes
              Action: Check disk health and mount points

        # ====== P3: MEDIUM - Kafka Partition Imbalance ======
        - alert: KafkaPartitionImbalance
          expr: |
            stddev(kafka_server_replicamanager_partitioncount) by (strimzi_io_cluster) > 10
          for: 15m
          labels:
            severity: info
            priority: p3
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka partition distribution is uneven"
            description: |
              Partition distribution across brokers has high variance (stddev > 10).
              Some brokers are overloaded while others are idle.

              Duration: >15 minutes
              Action: Consider running Cruise Control rebalance

              Check: kubectl -n kafka exec my-cluster-dual-role-0 -- \
                kafka-topics.sh --bootstrap-server localhost:9092 --describe | grep Leader

        # ====== P3: MEDIUM - Kafka Consumer Rebalancing Frequent ======
        - alert: KafkaConsumerRebalancingFrequent
          expr: |
            increase(kafka_consumer_coordinator_rebalance_total[10m]) > 5
          for: 10m
          labels:
            severity: warning
            priority: p3
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka consumer group rebalancing too frequently"
            description: |
              Consumer group is rebalancing more than 5 times in 10 minutes.
              During rebalancing, no messages are processed!

              Duration: >10 minutes
              Causes: Unstable consumers, too short session timeout, slow processing

              Fix: Increase session.timeout.ms, reduce max.poll.records, or use static membership

        # ====== P3: MEDIUM - Kafka Deserialization Errors ======
        - alert: KafkaDeserializationErrors
          expr: |
            increase(spring_kafka_listener_seconds_count{exception!="none"}[5m]) > 0
          for: 5m
          labels:
            severity: warning
            priority: p3
            component: kafka
            tier: platform
            layer: layer5
          annotations:
            summary: "Kafka consumer deserialization errors detected"
            description: |
              Consumer is failing to deserialize messages.
              Likely schema incompatibility between producer and consumer!

              Duration: >5 minutes
              Action: Check schema compatibility, review recent producer changes

              Check: kubectl logs -l app=payment-service -n kafka-saga | grep -i deserialize
