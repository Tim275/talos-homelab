# ============================================================================
# PROMETHEUS & ALERTMANAGER SELF-MONITORING ALERTS
# Enterprise Tier-0: Monitor the monitoring system itself
# ============================================================================
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: prometheus-self-monitoring-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus-stack
    release: kube-prometheus-stack
    tier: observability
spec:
  groups:
    # ========================================================================
    # PROMETHEUS CRITICAL ALERTS
    # ========================================================================
    - name: prometheus.critical
      interval: 30s
      rules:
        # ====== P1: CRITICAL - Prometheus Storage Full ======
        - alert: PrometheusStorageFull
          expr: |
            (
              prometheus_tsdb_storage_blocks_bytes
              /
              (prometheus_tsdb_retention_limit_bytes > 0 or vector(53687091200))
            ) > 0.90
          for: 10m
          labels:
            severity: critical
            priority: p1
            component: prometheus
            tier: observability
            layer: observability
          annotations:
            summary: "Prometheus storage >90% full"
            description: |
              Prometheus TSDB storage is {{ $value | humanizePercentage }} full!
              Metrics ingestion may stop soon.

              Current Usage: {{ $value | humanizePercentage }}
              Threshold: 90%

              Impact: Risk of metrics loss
              Team: @platform-oncall

              Action:
              - Reduce retention period
              - Delete old data
              - Expand storage

        # ====== P1: CRITICAL - Prometheus Down ======
        - alert: PrometheusDown
          expr: |
            up{job="prometheus-kube-prometheus-prometheus"} == 0
          for: 3m
          labels:
            severity: critical
            priority: p1
            component: prometheus
            tier: observability
            layer: observability
          annotations:
            summary: "Prometheus is DOWN"
            description: |
              Prometheus instance is unreachable for 3 minutes!
              NO METRICS COLLECTION - Alerting is BLIND!

              Impact: Complete loss of monitoring visibility
              Team: @platform-oncall

              Check: kubectl -n monitoring get pods -l app.kubernetes.io/name=prometheus

    # ========================================================================
    # PROMETHEUS HIGH ALERTS
    # ========================================================================
    - name: prometheus.high
      interval: 30s
      rules:
        # ====== P2: HIGH - Prometheus Target Down ======
        - alert: PrometheusTargetDown
          expr: |
            up == 0
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: prometheus
            tier: observability
            layer: observability
          annotations:
            summary: "Prometheus target {{ $labels.job }} is DOWN"
            description: |
              Scrape target {{ $labels.job }} ({{ $labels.instance }}) is unreachable.
              Metrics from this target are NOT being collected!

              Job: {{ $labels.job }}
              Instance: {{ $labels.instance }}
              Duration: >10 minutes

              Action: Check if the target service is running

        # ====== P2: HIGH - Prometheus Rule Evaluation Failures ======
        - alert: PrometheusRuleFailures
          expr: |
            increase(prometheus_rule_evaluation_failures_total[5m]) > 0
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: prometheus
            tier: observability
            layer: observability
          annotations:
            summary: "Prometheus rule evaluation failures"
            description: |
              Prometheus is failing to evaluate alert/recording rules.
              Some alerts may NOT fire!

              Failed Rules: {{ $value }}
              Duration: >10 minutes

              Action: Check Prometheus logs for PromQL errors
              Check: kubectl -n monitoring logs -l app.kubernetes.io/name=prometheus --tail=100

        # ====== P2: HIGH - Prometheus TSDB Compactions Failing ======
        - alert: PrometheusTSDBCompactionsFailing
          expr: |
            increase(prometheus_tsdb_compactions_failed_total[1h]) > 0
          for: 15m
          labels:
            severity: warning
            priority: p2
            component: prometheus
            tier: observability
            layer: observability
          annotations:
            summary: "Prometheus TSDB compaction failures"
            description: |
              Prometheus TSDB is failing to compact data blocks.
              Storage efficiency degraded, queries may slow down.

              Failed Compactions (1h): {{ $value }}

              Action: Check disk space and Prometheus logs

        # ====== P2: HIGH - Prometheus Config Reload Failed ======
        - alert: PrometheusConfigReloadFailed
          expr: |
            prometheus_config_last_reload_successful == 0
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: prometheus
            tier: observability
            layer: observability
          annotations:
            summary: "Prometheus config reload FAILED"
            description: |
              Prometheus failed to reload its configuration.
              New scrape configs and rules are NOT active!

              Action: Check config syntax
              Check: kubectl -n monitoring logs -l app.kubernetes.io/name=prometheus --tail=50

        # ====== P2: HIGH - Prometheus Scrape Duration High ======
        - alert: PrometheusScrapeDurationHigh
          expr: |
            prometheus_target_scrape_pool_sync_total > 0
            and
            rate(prometheus_target_scrape_pool_sync_total[5m]) > 0.1
          for: 15m
          labels:
            severity: warning
            priority: p2
            component: prometheus
            tier: observability
            layer: observability
          annotations:
            summary: "Prometheus scrape pool sync taking too long"
            description: |
              Prometheus is having trouble keeping up with scrape targets.
              Scrape intervals may be delayed.

              Action: Check target count and scrape intervals

    # ========================================================================
    # ALERTMANAGER CRITICAL ALERTS
    # ========================================================================
    - name: alertmanager.critical
      interval: 30s
      rules:
        # ====== P1: CRITICAL - Alertmanager Down ======
        - alert: AlertmanagerDown
          expr: |
            up{job="alertmanager-operated"} == 0
          for: 3m
          labels:
            severity: critical
            priority: p1
            component: alertmanager
            tier: observability
            layer: observability
          annotations:
            summary: "Alertmanager is DOWN"
            description: |
              Alertmanager is unreachable for 3 minutes!
              ALERTS ARE NOT BEING SENT!

              Impact: No alert notifications - complete alerting failure
              Team: @platform-oncall

              Check: kubectl -n monitoring get pods -l app.kubernetes.io/name=alertmanager

        # ====== P1: CRITICAL - Alertmanager Cluster Down ======
        - alert: AlertmanagerClusterDown
          expr: |
            count(up{job="alertmanager-operated"} == 1) < 1
          for: 5m
          labels:
            severity: critical
            priority: p1
            component: alertmanager
            tier: observability
            layer: observability
          annotations:
            summary: "Alertmanager cluster has NO healthy members"
            description: |
              All Alertmanager instances are down!
              NO ALERTS CAN BE SENT!

              Healthy Members: 0
              Required: >= 1

              Impact: Complete alerting system failure
              Team: @platform-oncall

    # ========================================================================
    # ALERTMANAGER HIGH ALERTS
    # ========================================================================
    - name: alertmanager.high
      interval: 30s
      rules:
        # ====== P1: HIGH - Alertmanager Failed to Send Alerts ======
        - alert: AlertmanagerFailedToSendAlerts
          expr: |
            rate(alertmanager_notifications_failed_total[5m]) > 0
          for: 10m
          labels:
            severity: warning
            priority: p1
            component: alertmanager
            tier: observability
            layer: observability
          annotations:
            summary: "Alertmanager failing to send notifications"
            description: |
              Alertmanager is failing to send notifications to {{ $labels.integration }}.
              ALERTS MAY NOT BE DELIVERED!

              Integration: {{ $labels.integration }}
              Failed Rate: {{ $value }}/sec

              Action: Check Alertmanager logs and integration config
              Check: kubectl -n monitoring logs -l app.kubernetes.io/name=alertmanager --tail=100

        # ====== P2: HIGH - Alertmanager Config Reload Failed ======
        - alert: AlertmanagerConfigReloadFailed
          expr: |
            alertmanager_config_last_reload_successful == 0
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: alertmanager
            tier: observability
            layer: observability
          annotations:
            summary: "Alertmanager config reload FAILED"
            description: |
              Alertmanager failed to reload its configuration.
              New routing rules are NOT active!

              Action: Check config syntax in values.yaml

        # ====== P2: HIGH - Alertmanager Cluster Not Synced ======
        - alert: AlertmanagerClusterNotSynced
          expr: |
            min(alertmanager_cluster_members) < 2
            and
            count(up{job="alertmanager-operated"} == 1) > 1
          for: 10m
          labels:
            severity: warning
            priority: p2
            component: alertmanager
            tier: observability
            layer: observability
          annotations:
            summary: "Alertmanager cluster members not synced"
            description: |
              Alertmanager HA cluster is not fully synced.
              Alert deduplication may not work correctly.

              Cluster Members: {{ $value }}
              Expected: >= 2 (if HA enabled)

              Action: Check network connectivity between Alertmanager pods

    # ========================================================================
    # PROMETHEUS MEDIUM ALERTS
    # ========================================================================
    - name: prometheus.medium
      interval: 60s
      rules:
        # ====== P3: MEDIUM - Prometheus High Memory ======
        - alert: PrometheusHighMemory
          expr: |
            process_resident_memory_bytes{job="prometheus-kube-prometheus-prometheus"}
            / 1024 / 1024 / 1024 > 4
          for: 30m
          labels:
            severity: info
            priority: p3
            component: prometheus
            tier: observability
            layer: observability
          annotations:
            summary: "Prometheus using >4GB memory"
            description: |
              Prometheus is using {{ $value | printf "%.1f" }}GB of memory.
              Consider reducing cardinality or retention.

              Current: {{ $value | printf "%.1f" }}GB
              Threshold: 4GB

        # ====== P3: MEDIUM - Prometheus High Cardinality ======
        - alert: PrometheusHighCardinality
          expr: |
            prometheus_tsdb_head_series > 1000000
          for: 1h
          labels:
            severity: info
            priority: p3
            component: prometheus
            tier: observability
            layer: observability
          annotations:
            summary: "Prometheus high series cardinality"
            description: |
              Prometheus is tracking {{ $value | humanize }} time series.
              High cardinality impacts query performance.

              Current Series: {{ $value | humanize }}
              Threshold: 1,000,000

              Action: Review metric labels and drop high-cardinality metrics
