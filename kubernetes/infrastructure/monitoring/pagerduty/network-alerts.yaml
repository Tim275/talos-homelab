## Network Infrastructure Alerts - Cilium CNI, Envoy Gateway
## P0: Complete networking failure, P1: CNI degradation, P2: Performance issues
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: network-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus-stack
    role: alert-rules
spec:
  groups:
  - name: cilium-cni
    interval: 30s
    rules:
    - alert: CiliumOperatorDown
      expr: |
        absent(up{namespace="kube-system",job="cilium-operator"}) or count(up{namespace="kube-system",job="cilium-operator"} == 1) == 0
      for: 2m
      labels:
        severity: critical
        priority: P1
        component: network
        tier: tier-0
      annotations:
        summary: "Cilium Operator DOWN - CNI management failing"
        description: "All Cilium operator instances are down. CNI cannot provision networking for new pods."
        action: "kubectl get pods -n kube-system -l app.kubernetes.io/name=cilium-operator && kubectl logs -n kube-system -l app.kubernetes.io/name=cilium-operator --tail=100"
        impact: "New pods cannot get networking, cluster expansion blocked"

    - alert: CiliumAgentDown
      expr: |
        count(up{namespace="kube-system",job="cilium-agent"} == 0) > 1
      for: 2m
      labels:
        severity: critical
        priority: P1
        component: network
        tier: tier-0
      annotations:
        summary: "{{ $value }} Cilium agents DOWN - Node networking broken"
        description: "Multiple Cilium DaemonSet pods are down. Affected nodes have no pod networking."
        action: "kubectl get pods -n kube-system -l k8s-app=cilium -o wide && kubectl describe pods -n kube-system -l k8s-app=cilium | grep -A 10 'Events:'"
        current_value: "{{ $value }} agents down"

    - alert: CiliumIdentityAllocationFailure
      expr: |
        rate(cilium_identity_label_allocation_errors_total[5m]) > 0
      for: 5m
      labels:
        severity: warning
        priority: P2
        component: network
        tier: tier-0
      annotations:
        summary: "Cilium identity allocation errors - NetworkPolicy may fail"
        description: "Cilium cannot allocate security identities. Network policies may not be enforced correctly."
        action: "kubectl exec -n kube-system ds/cilium -- cilium status --verbose"
        current_value: "{{ $value }} errors/sec"

    - alert: CiliumEndpointNotReady
      expr: |
        sum(cilium_endpoint_state{endpoint_state!="ready"}) > 10
      for: 5m
      labels:
        severity: warning
        priority: P2
        component: network
        tier: tier-0
      annotations:
        summary: "{{ $value }} Cilium endpoints not ready - Pod networking degraded"
        description: "Multiple pod endpoints are not in ready state. Affected pods may have networking issues."
        action: "kubectl exec -n kube-system ds/cilium -- cilium endpoint list | grep -v ready"
        current_value: "{{ $value }} endpoints"

  - name: envoy-gateway
    interval: 30s
    rules:
    - alert: EnvoyGatewayDown
      expr: |
        absent(up{namespace="envoy-gateway-system",job="envoy-gateway"}) or count(up{namespace="envoy-gateway-system",job="envoy-gateway"} == 1) == 0
      for: 2m
      labels:
        severity: critical
        priority: P1
        component: gateway
        tier: tier-0
      annotations:
        summary: "Envoy Gateway DOWN - Gateway API unavailable"
        description: "Envoy Gateway control plane is down. Cannot manage Gateway API resources."
        action: "kubectl get pods -n envoy-gateway-system && kubectl logs -n envoy-gateway-system -l control-plane=envoy-gateway --tail=100"

    - alert: EnvoyProxyDown
      expr: |
        absent(up{namespace="envoy-gateway-system",job=~".*envoy-proxy.*"}) or count(up{namespace="envoy-gateway-system",job=~".*envoy-proxy.*"} == 1) == 0
      for: 2m
      labels:
        severity: critical
        priority: P1
        component: gateway
        tier: tier-0
      annotations:
        summary: "Envoy Proxy DOWN - Gateway traffic blocked"
        description: "Envoy proxy data plane is down. Traffic cannot flow through Gateway."
        action: "kubectl get pods -n envoy-gateway-system -l gateway.envoyproxy.io/owning-gateway-name && kubectl logs -n envoy-gateway-system -l gateway.envoyproxy.io/owning-gateway-name --tail=50"

  - name: coredns
    interval: 30s
    rules:
    - alert: CoreDNSDown
      expr: |
        absent(up{namespace="kube-system",job="coredns"}) or count(up{namespace="kube-system",job="coredns"} == 1) == 0
      for: 2m
      labels:
        severity: critical
        priority: P1
        component: dns
        tier: tier-0
      annotations:
        summary: "CoreDNS DOWN - DNS resolution broken"
        description: "All CoreDNS instances are down. Cluster DNS resolution is broken. Services cannot be discovered."
        action: "kubectl get pods -n kube-system -l k8s-app=kube-dns && kubectl logs -n kube-system -l k8s-app=kube-dns --tail=100"
        impact: "DNS resolution broken - services unreachable by name"

    - alert: CoreDNSHighErrorRate
      expr: |
        rate(coredns_dns_responses_total{rcode="SERVFAIL"}[5m]) > 10
      for: 5m
      labels:
        severity: warning
        priority: P2
        component: dns
        tier: tier-0
      annotations:
        summary: "CoreDNS returning {{ $value }} SERVFAIL/sec - DNS errors"
        description: "High DNS error rate. Services may experience intermittent resolution failures."
        action: "kubectl logs -n kube-system -l k8s-app=kube-dns --tail=200 | grep -i error"
        current_value: "{{ $value }} errors/sec"
