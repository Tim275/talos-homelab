## Platform Critical Services - OpenTelemetry, LLDAP, Grafana, Istio
## P1: Critical service degradation, P2: Performance issues
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: platform-critical-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus-stack
    role: alert-rules
spec:
  groups:
  - name: opentelemetry-collector
    interval: 30s
    rules:
    - alert: OTelCollectorDown
      expr: |
        absent(up{namespace="opentelemetry",job=~".*otel-collector.*"}) or count(up{namespace="opentelemetry",job=~".*otel-collector.*"} == 1) == 0
      for: 2m
      labels:
        severity: critical
        priority: P1
        component: telemetry
        tier: tier-1
      annotations:
        summary: "OpenTelemetry Collector DOWN - Metrics/Traces collection broken"
        description: "All OpenTelemetry collectors are down. No metrics or traces are being collected from applications."
        action: "kubectl get pods -n opentelemetry && kubectl logs -n opentelemetry -l app.kubernetes.io/component=opentelemetry-collector --tail=100"

    - alert: OTelCollectorMemoryPressure
      expr: |
        sum(container_memory_working_set_bytes{namespace="opentelemetry",pod=~"otel-collector.*"}) / sum(container_spec_memory_limit_bytes{namespace="opentelemetry",pod=~"otel-collector.*"}) > 0.85
      for: 5m
      labels:
        severity: warning
        priority: P2
        component: telemetry
        tier: tier-1
      annotations:
        summary: "OpenTelemetry Collector high memory usage - {{ $value | humanizePercentage }}"
        description: "OpenTelemetry collector memory usage is above 85%. May start dropping metrics/traces."
        action: "kubectl top pods -n opentelemetry && kubectl logs -n opentelemetry -l app.kubernetes.io/component=opentelemetry-collector --tail=50 | grep memory"
        current_value: "{{ $value | humanizePercentage }} memory used"

    - alert: OTelCollectorExportFailures
      expr: |
        rate(otelcol_exporter_send_failed_metric_points{namespace="opentelemetry"}[5m]) > 10
      for: 5m
      labels:
        severity: warning
        priority: P2
        component: telemetry
        tier: tier-1
      annotations:
        summary: "OpenTelemetry Collector dropping metrics - {{ $value }} failures/sec"
        description: "OpenTelemetry collector is failing to export metrics. Data loss occurring."
        action: "kubectl logs -n opentelemetry -l app.kubernetes.io/component=opentelemetry-collector --tail=100 | grep -i 'error\\|fail\\|drop'"
        current_value: "{{ $value }} export failures/sec"

  - name: lldap-directory
    interval: 30s
    rules:
    - alert: LLDAPDown
      expr: |
        absent(up{namespace="lldap"}) or count(up{namespace="lldap"} == 1) == 0
      for: 2m
      labels:
        severity: critical
        priority: P1
        component: auth
        tier: tier-1
      annotations:
        summary: "LLDAP LDAP Directory DOWN - Authentication broken"
        description: "LLDAP service is down. LDAP authentication is unavailable. Users cannot authenticate."
        action: "kubectl get pods -n lldap && kubectl logs -n lldap -l app.kubernetes.io/name=lldap --tail=100"
        impact: "LDAP authentication broken - SSO login failures"

    - alert: LLDAPHighConnectionCount
      expr: |
        lldap_connection_count{namespace="lldap"} > 100
      for: 5m
      labels:
        severity: warning
        priority: P2
        component: auth
        tier: tier-1
      annotations:
        summary: "LLDAP high connection count - {{ $value }} connections"
        description: "LLDAP has unusually high number of connections. Possible authentication storm or attack."
        action: "kubectl logs -n lldap -l app.kubernetes.io/name=lldap --tail=100"
        current_value: "{{ $value }} connections"

  - name: grafana-dashboards
    interval: 30s
    rules:
    - alert: GrafanaDown
      expr: |
        absent(up{namespace="grafana",job="grafana"}) or count(up{namespace="grafana",job="grafana"} == 1) == 0
      for: 2m
      labels:
        severity: critical
        priority: P1
        component: monitoring
        tier: tier-1
      annotations:
        summary: "Grafana DOWN - Dashboards unavailable"
        description: "Grafana is down. All monitoring dashboards are unavailable. Cannot visualize metrics."
        action: "kubectl get pods -n grafana && kubectl logs -n grafana -l app.kubernetes.io/name=grafana --tail=100"
        impact: "Monitoring dashboards unavailable - no visibility into cluster metrics"

    - alert: GrafanaHighCPU
      expr: |
        sum(rate(container_cpu_usage_seconds_total{namespace="grafana",pod=~"grafana.*"}[5m])) > 0.8
      for: 5m
      labels:
        severity: warning
        priority: P2
        component: monitoring
        tier: tier-1
      annotations:
        summary: "Grafana high CPU usage - {{ $value | humanize }} cores"
        description: "Grafana is using high CPU. Dashboards may load slowly."
        action: "kubectl top pods -n grafana"
        current_value: "{{ $value | humanize }} CPU cores"

  - name: istio-service-mesh
    interval: 30s
    rules:
    - alert: IstioControlPlaneDown
      expr: |
        absent(up{namespace="istio-system",job="istiod"}) or count(up{namespace="istio-system",job="istiod"} == 1) < 2
      for: 2m
      labels:
        severity: critical
        priority: P1
        component: service-mesh
        tier: tier-0
      annotations:
        summary: "Istio Control Plane DOWN - Service mesh degraded"
        description: "Less than 2 istiod replicas are running. Service mesh control plane is degraded. Config updates may fail."
        action: "kubectl get pods -n istio-system -l app=istiod && kubectl logs -n istio-system -l app=istiod --tail=100"
        impact: "Service mesh control plane degraded - mTLS policy updates blocked"

    - alert: IstioHighGlobalErrorRate
      expr: |
        sum(rate(istio_requests_total{response_code=~"5.."}[5m])) / sum(rate(istio_requests_total[5m])) > 0.05
      for: 5m
      labels:
        severity: warning
        priority: P2
        component: service-mesh
        tier: tier-0
      annotations:
        summary: "Istio high 5xx error rate across mesh - {{ $value | humanizePercentage }}"
        description: "Service mesh is experiencing high error rate. Multiple services may be failing."
        action: "kubectl exec -n istio-system deploy/istiod -- pilot-discovery request GET /debug/config_dump | grep -i error"
        current_value: "{{ $value | humanizePercentage }} error rate"

    - alert: IstioProxyNotReady
      expr: |
        sum(pilot_proxy_convergence_time_bucket{le=\"10\"}) / sum(pilot_proxy_convergence_time_count) < 0.95
      for: 5m
      labels:
        severity: warning
        priority: P2
        component: service-mesh
        tier: tier-0
      annotations:
        summary: "Istio proxies slow to receive config updates"
        description: "Istio sidecar proxies are taking >10s to receive config updates. Service mesh configuration propagation is slow."
        action: "kubectl logs -n istio-system -l app=istiod --tail=50 | grep -i 'push\\|config'"

    - alert: IstioCertificateExpiringSoon
      expr: |
        (pilot_cert_expiry_seconds < 604800)
      for: 1h
      labels:
        severity: warning
        priority: P2
        component: service-mesh
        tier: tier-0
      annotations:
        summary: "Istio certificate expiring in less than 7 days"
        description: "Istio control plane certificate will expire in {{ $value | humanizeDuration }}. mTLS will fail after expiry."
        action: "kubectl get secrets -n istio-system | grep istio && kubectl describe secret -n istio-system istio-ca-secret"
        current_value: "Expires in {{ $value | humanizeDuration }}"
