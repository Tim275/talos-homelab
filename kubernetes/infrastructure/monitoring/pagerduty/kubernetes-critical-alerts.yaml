# ðŸš¨ Top 10 Critical Kubernetes Alerts for PagerDuty
# Based on enterprise production best practices
# Sync Wave 5 - After Prometheus/Alertmanager

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: pagerduty-kubernetes-critical-alerts
  namespace: monitoring
  labels:
    app.kubernetes.io/name: pagerduty
    app.kubernetes.io/component: alerting
    prometheus: kube-prometheus
  annotations:
    argocd.argoproj.io/sync-wave: "5"
spec:
  groups:
  - name: kubernetes.critical
    interval: 30s
    rules:
    # ============================================================
    # 1. High CPU Limit Usage
    # ============================================================
    - alert: HighCPULimitUsage
      expr: |
        sum(rate(container_cpu_usage_seconds_total[5m])) by (namespace, pod, container)
        / sum(container_spec_cpu_quota/container_spec_cpu_period) by (namespace, pod, container) > 0.8
      for: 5m
      labels:
        severity: warning
        component: pod
        tier: critical
      annotations:
        summary: "Pod {{ $labels.pod }} exceeds CPU limit"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its CPU limit"
        action: "kubectl top pod {{ $labels.pod }} -n {{ $labels.namespace }}"

    # ============================================================
    # 2. CPU Limit Usage Reached (100%)
    # ============================================================
    - alert: CPULimitReached
      expr: |
        sum(rate(container_cpu_usage_seconds_total[5m])) by (namespace, pod, container)
        / sum(container_spec_cpu_quota/container_spec_cpu_period) by (namespace, pod, container) >= 0.95
      for: 2m
      labels:
        severity: critical
        component: pod
        tier: critical
      annotations:
        summary: "Pod {{ $labels.pod }} reached CPU limit"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has reached 95%+ of its CPU limit ({{ $value | humanizePercentage }})"
        action: "kubectl edit pod {{ $labels.pod }} -n {{ $labels.namespace }} # Increase CPU limits"

    # ============================================================
    # 3. Kubelet Volume Manager Unavailable
    # ============================================================
    - alert: KubeletVolumeManagerDown
      expr: |
        up{job="kubelet", metrics_path="/metrics"} == 0
      for: 3m
      labels:
        severity: critical
        component: kubelet
        tier: critical
      annotations:
        summary: "Kubelet volume manager down on {{ $labels.node }}"
        description: "Kubelet volume manager on node {{ $labels.node }} is unavailable - pod storage may be affected"
        action: "kubectl get pods -n kube-system | grep kubelet && kubectl logs <kubelet_pod> -n kube-system"

    # ============================================================
    # 4. Kubernetes API Server Errors
    # ============================================================
    - alert: KubernetesAPIServerErrors
      expr: |
        sum(rate(apiserver_request_total{code=~"5.."}[5m])) by (verb, resource) > 0.01
      for: 5m
      labels:
        severity: critical
        component: apiserver
        tier: critical
      annotations:
        summary: "High API server error rate"
        description: "API server is returning {{ $value }} errors/sec for {{ $labels.verb }} {{ $labels.resource }}"
        action: "kubectl logs -n kube-system -l component=kube-apiserver --tail=50"

    # ============================================================
    # 5. Node Under Pressure
    # ============================================================
    - alert: NodeUnderPressure
      expr: |
        kube_node_status_condition{condition=~"MemoryPressure|DiskPressure|PIDPressure",status="true"} == 1
      for: 5m
      labels:
        severity: warning
        component: node
        tier: critical
      annotations:
        summary: "Node {{ $labels.node }} under {{ $labels.condition }}"
        description: "Node {{ $labels.node }} is experiencing {{ $labels.condition }} - pod scheduling may be affected"
        action: "kubectl describe node {{ $labels.node }} && kubectl top node {{ $labels.node }}"

    # ============================================================
    # 6. Anomalous Node CPU/Memory Capacity
    # ============================================================
    - alert: NodeResourceAnomalous
      expr: |
        (
          node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes < 0.1
          or
          (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance)) > 0.9
        )
      for: 10m
      labels:
        severity: warning
        component: node
        tier: critical
      annotations:
        summary: "Node {{ $labels.instance }} resource anomaly detected"
        description: "Node {{ $labels.instance }} has anomalous resource usage - available memory < 10% or CPU usage > 90%"
        action: "kubectl top node && kubectl get pods -o wide --all-namespaces --field-selector spec.nodeName={{ $labels.instance }}"

    # ============================================================
    # 7. Missing Pod Replicas (Deployments/StatefulSets)
    # ============================================================
    - alert: MissingPodReplicas
      expr: |
        (
          kube_deployment_spec_replicas != kube_deployment_status_replicas_available
          or
          kube_statefulset_replicas != kube_statefulset_status_replicas_ready
        )
      for: 5m
      labels:
        severity: critical
        component: workload
        tier: critical
      annotations:
        summary: "Missing replicas for {{ $labels.deployment }}{{ $labels.statefulset }}"
        description: "Workload {{ $labels.deployment }}{{ $labels.statefulset }} in namespace {{ $labels.namespace }} has missing replicas"
        action: "kubectl get deployment,statefulset -n {{ $labels.namespace }} && kubectl describe pod -n {{ $labels.namespace }}"

    # ============================================================
    # 8. Pod Status Issues (Pending/Failed/Unknown)
    # ============================================================
    - alert: PodStatusIssues
      expr: |
        kube_pod_status_phase{phase=~"Pending|Failed|Unknown"} > 0
      for: 5m
      labels:
        severity: warning
        component: pod
        tier: critical
      annotations:
        summary: "Pod {{ $labels.pod }} in {{ $labels.phase }} state"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been in {{ $labels.phase }} state for > 5 minutes"
        action: "kubectl describe pod {{ $labels.pod }} -n {{ $labels.namespace }} && kubectl logs {{ $labels.pod }} -n {{ $labels.namespace }}"

    # ============================================================
    # 9. Pod Restart and OOM Scenarios
    # ============================================================
    - alert: PodFrequentRestarts
      expr: |
        rate(kube_pod_container_status_restarts_total[15m]) > 0.1
      for: 5m
      labels:
        severity: warning
        component: pod
        tier: critical
      annotations:
        summary: "Pod {{ $labels.pod }} restarting frequently"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has restarted {{ $value | humanize }} times in the last 15 minutes"
        action: "kubectl logs {{ $labels.pod }} -n {{ $labels.namespace }} --previous && kubectl describe pod {{ $labels.pod }} -n {{ $labels.namespace }}"

    - alert: PodOOMKilled
      expr: |
        kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} == 1
      for: 1m
      labels:
        severity: critical
        component: pod
        tier: critical
      annotations:
        summary: "Pod {{ $labels.pod }} killed due to OOM"
        description: "Container {{ $labels.container }} in pod {{ $labels.pod }} (namespace {{ $labels.namespace }}) was OOM killed"
        action: "kubectl edit pod {{ $labels.pod }} -n {{ $labels.namespace }} # Increase memory limits"

    # ============================================================
    # 10. ETCD Leader Change / No Leader
    # ============================================================
    - alert: ETCDHighLeaderChanges
      expr: |
        rate(etcd_server_leader_changes_seen_total[10m]) > 3
      for: 5m
      labels:
        severity: critical
        component: etcd
        tier: critical
      annotations:
        summary: "ETCD cluster experiencing frequent leader changes"
        description: "ETCD cluster has {{ $value | humanize }} leader changes in the last 10 minutes - cluster instability detected"
        action: "kubectl get pods -n kube-system -l component=etcd && kubectl logs -n kube-system -l component=etcd --tail=100"

    - alert: ETCDNoLeader
      expr: |
        etcd_server_has_leader == 0
      for: 1m
      labels:
        severity: critical
        component: etcd
        tier: critical
      annotations:
        summary: "ETCD cluster has no leader"
        description: "ETCD cluster member {{ $labels.instance }} reports no leader - cluster is DOWN"
        action: "etcdctl endpoint health && etcdctl endpoint status"
