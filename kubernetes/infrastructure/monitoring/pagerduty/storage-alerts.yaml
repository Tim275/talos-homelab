apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: storage-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus-stack
    role: alert-rules
spec:
  groups:
  - name: storage-ceph
    interval: 30s
    rules:
    - alert: CephClusterHealthError
      expr: |
        ceph_health_status == 2
      for: 2m
      labels:
        severity: critical
        priority: P1
        component: storage
        tier: tier-0
      annotations:
        summary: "Ceph Cluster Health is ERROR - Data loss risk!"
        description: "Ceph cluster health is in ERROR state. Immediate action required to prevent data loss."
        action: "kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph health detail"
        dashboard_url: "https://grafana.homelab.local/d/ceph-cluster"
        runbook_url: "https://docs.ceph.com/en/latest/rados/operations/health-checks/"
        current_value: "ERROR"
        threshold: "HEALTH_OK"

    - alert: CephClusterHealthWarning
      expr: |
        ceph_health_status == 1
      for: 10m
      labels:
        severity: warning
        priority: P2
        component: storage
        tier: tier-0
      annotations:
        summary: "Ceph Cluster Health is WARNING"
        description: "Ceph cluster health has been in WARNING state for more than 10 minutes. Check for degraded PGs or slow requests."
        action: "kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph health detail"
        dashboard_url: "https://grafana.homelab.local/d/ceph-cluster"

    - alert: CephOSDDown
      expr: |
        count(ceph_osd_up == 0) > 1
      for: 5m
      labels:
        severity: critical
        priority: P1
        component: storage
        tier: tier-0
      annotations:
        summary: "{{ $value }} Ceph OSDs are down - Replication at risk!"
        description: "Multiple OSDs are down. Data replication may be compromised. Check node status immediately."
        action: "kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd tree"
        current_value: "{{ $value }} OSDs down"
        threshold: "0 OSDs down"

    - alert: CephOSDNearFull
      expr: |
        ceph_osd_utilization > 85
      for: 5m
      labels:
        severity: warning
        priority: P2
        component: storage
        tier: tier-0
      annotations:
        summary: "Ceph OSD {{ $labels.ceph_daemon }} is {{ $value }}% full"
        description: "OSD utilization is above 85%. Risk of write failures if capacity reaches 95%. Add more storage or rebalance data."
        action: "kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd df tree"
        current_value: "{{ $value }}%"
        threshold: "85%"

    - alert: CephPoolNearFull
      expr: |
        ceph_pool_percent_used > 85
      for: 5m
      labels:
        severity: critical
        priority: P1
        component: storage
        tier: tier-0
      annotations:
        summary: "Ceph pool {{ $labels.name }} is {{ $value }}% full - Write failures imminent!"
        description: "Storage pool is critically full. Applications will experience write failures soon. Immediate capacity expansion required."
        action: "kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph df detail"
        current_value: "{{ $value }}%"
        threshold: "85%"

    - alert: CephSlowRequests
      expr: |
        ceph_healthcheck_slow_ops > 0
      for: 5m
      labels:
        severity: warning
        priority: P2
        component: storage
        tier: tier-0
      annotations:
        summary: "Ceph has {{ $value }} slow requests - Performance degraded"
        description: "Slow operations detected in Ceph cluster. Check for network issues, disk performance, or OSD problems."
        action: "kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph health detail && ceph -s"
        current_value: "{{ $value }} slow ops"
        threshold: "0 slow ops"

    - alert: CephMonitorQuorumLost
      expr: |
        count(ceph_mon_quorum_status == 1) < 2
      for: 1m
      labels:
        severity: critical
        priority: P1
        component: storage
        tier: tier-0
      annotations:
        summary: "Ceph Monitor quorum LOST - Cluster unavailable!"
        description: "Ceph monitor quorum has been lost. Cluster operations are blocked. Check monitor pod status immediately."
        action: "kubectl get pods -n rook-ceph | grep mon && kubectl logs -n rook-ceph -l app=rook-ceph-mon --tail=50"

    - alert: CephPGsDown
      expr: |
        ceph_pg_down > 0
      for: 5m
      labels:
        severity: critical
        priority: P1
        component: storage
        tier: tier-0
      annotations:
        summary: "{{ $value }} Ceph placement groups are down - Data unavailable!"
        description: "Placement groups are in down state. Some data is currently unavailable to applications."
        action: "kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph pg dump_stuck inactive"
        current_value: "{{ $value }} PGs down"
        threshold: "0 PGs down"

    - alert: CephPGsDegraded
      expr: |
        ceph_pg_degraded > 0
      for: 10m
      labels:
        severity: warning
        priority: P2
        component: storage
        tier: tier-0
      annotations:
        summary: "{{ $value }} Ceph PGs degraded - Replication incomplete"
        description: "Placement groups are degraded for more than 10 minutes. Data replication is incomplete but accessible."
        action: "kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph pg dump | grep -v active+clean | head -20"
        current_value: "{{ $value }} PGs degraded"

    - alert: PVCProvisioningFailed
      expr: |
        increase(storage_operation_duration_seconds_count{status="fail-unknown"}[10m]) > 0
      for: 2m
      labels:
        severity: warning
        priority: P2
        component: storage
        tier: tier-0
      annotations:
        summary: "PVC provisioning failures detected - Apps cannot start!"
        description: "{{ $value }} storage provisioning failures in last 10 minutes. Applications waiting for volumes cannot start."
        action: "kubectl get pvc --all-namespaces | grep Pending && kubectl get events --all-namespaces --field-selector reason=FailedProvisioning"
        current_value: "{{ $value }} failures"
