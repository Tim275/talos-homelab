# ðŸš¨ TIER-0 ENTERPRISE PROMETHEUS RULES
# Based on Google SRE Golden Signals, Netflix CORE, AWS CloudWatch
# These are CRITICAL alerts that page on-call engineers immediately

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: tier-0-critical-alerts
  namespace: monitoring
  labels:
    tier: "0"
    prometheus: kube-prometheus
    alertmanager: main
spec:
  groups:
  # ================================================
  # KUBERNETES CONTROL PLANE (Google SRE Pattern)
  # ================================================
  - name: kubernetes.control-plane
    interval: 30s
    rules:
    # API Server Availability
    - alert: KubernetesAPIServerDown
      expr: up{job="kubernetes-apiservers"} == 0
      for: 1m
      labels:
        severity: critical
        tier: "0"
        component: control-plane
      annotations:
        summary: "Kubernetes API server is DOWN"
        description: "API Server {{ $labels.instance }} has been down for more than 1 minute. CLUSTER CONTROL PLANE FAILURE!"
        runbook_url: "https://runbooks.homelab.io/KubernetesAPIServerDown"
        dashboard_url: "https://grafana.homelab.io/d/kubernetes-control-plane"

    # API Server Latency (Google SRE Golden Signal)
    - alert: KubernetesAPIServerHighLatency
      expr: |
        histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb!~"WATCH|LIST"}[5m])) by (le)) > 1
      for: 5m
      labels:
        severity: critical
        tier: "0"
      annotations:
        summary: "API Server latency > 1s (99th percentile)"
        description: "API Server requests are taking > 1 second at 99th percentile. Current: {{ $value }}s"

    # ETCD Leader Elections (Netflix Pattern)
    - alert: ETCDNoLeader
      expr: etcd_server_has_leader == 0
      for: 1m
      labels:
        severity: critical
        tier: "0"
        component: control-plane
      annotations:
        summary: "ETCD cluster has NO LEADER"
        description: "ETCD cluster has no leader for > 1 minute. Cluster state is CRITICAL!"

    - alert: ETCDHighNumberOfLeaderChanges
      expr: rate(etcd_server_leader_changes_seen_total[15m]) > 3
      for: 1m
      labels:
        severity: critical
        tier: "0"
      annotations:
        summary: "ETCD unstable - frequent leader changes"
        description: "ETCD leader changed {{ $value }} times in 15 minutes. Cluster stability at risk!"

    # ETCD Database Size
    - alert: ETCDDatabaseQuotaExhaustion
      expr: (etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes) > 0.95
      for: 1m
      labels:
        severity: critical
        tier: "0"
      annotations:
        summary: "ETCD database near quota limit (95%+)"
        description: "ETCD database size is at {{ $value | humanizePercentage }} of limit. Writes will fail soon!"

  # ================================================
  # NODE FAILURES (Uber Ring0 Pattern)
  # ================================================
  - name: kubernetes.nodes
    interval: 30s
    rules:
    - alert: KubernetesNodeNotReady
      expr: |
        kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Node {{ $labels.node }} is NOT READY"
        description: "Node has been unready for > 5 minutes. Workloads are being rescheduled."

    - alert: KubernetesNodeUnreachable
      expr: |
        kube_node_spec_taint{key="node.kubernetes.io/unreachable"} == 1
      for: 2m
      labels:
        severity: critical
        tier: "0"
      annotations:
        summary: "Node {{ $labels.node }} is UNREACHABLE"
        description: "Node is completely unreachable. All pods on this node are terminating!"

    # Node Pressure Conditions
    - alert: KubernetesNodeMemoryPressure
      expr: |
        kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 2m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Node {{ $labels.node }} under MEMORY PRESSURE"
        description: "Node is experiencing memory pressure. Pod evictions may occur."

    - alert: KubernetesNodeDiskPressure
      expr: |
        kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 2m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Node {{ $labels.node }} under DISK PRESSURE"
        description: "Node is experiencing disk pressure. Pod evictions imminent."

  # ================================================
  # RESOURCE SATURATION (Google Golden Signal)
  # ================================================
  - name: resource.saturation
    interval: 30s
    rules:
    # CPU Saturation
    - alert: NodeCPUSaturation
      expr: |
        (1 - avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m]))) > 0.95
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Node {{ $labels.instance }} CPU saturated (>95%)"
        description: "CPU usage is at {{ $value | humanizePercentage }}. System is CPU bound!"

    # Memory Saturation
    - alert: NodeMemorySaturation
      expr: |
        (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.95
      for: 5m
      labels:
        severity: critical
        tier: "0"
      annotations:
        summary: "Node {{ $labels.instance }} MEMORY CRITICAL (>95%)"
        description: "Memory usage is at {{ $value | humanizePercentage }}. OOM killer may activate!"

    # Disk Saturation
    - alert: NodeDiskSaturation
      expr: |
        (node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs|squashfs|vfat"} - node_filesystem_avail_bytes) / node_filesystem_size_bytes > 0.90
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Disk {{ $labels.device }} on {{ $labels.instance }} >90% full"
        description: "Disk usage at {{ $value | humanizePercentage }}. Free space critical!"

    # Network Saturation
    - alert: NodeNetworkSaturation
      expr: |
        rate(node_network_receive_drop_total[5m]) + rate(node_network_transmit_drop_total[5m]) > 100
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Network drops on {{ $labels.instance }}"
        description: "Network is dropping {{ $value }} packets/sec. Network saturation detected."

  # ================================================
  # STORAGE SYSTEM (Ceph/Rook) - Critical for Data
  # ================================================
  - name: storage.ceph
    interval: 30s
    rules:
    - alert: CephClusterErrorState
      expr: ceph_health_status > 1
      for: 1m
      labels:
        severity: critical
        tier: "0"
        component: storage
      annotations:
        summary: "Ceph cluster in ERROR state"
        description: "Ceph cluster health is CRITICAL. Data availability at risk!"

    - alert: CephOSDDown
      expr: ceph_osd_up == 0
      for: 1m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Ceph OSD {{ $labels.osd }} is DOWN"
        description: "OSD has been down for > 1 minute. Data redundancy reduced!"

    - alert: CephPGUnavailable
      expr: ceph_pg_active == 0
      for: 1m
      labels:
        severity: critical
        tier: "0"
      annotations:
        summary: "Ceph PG UNAVAILABLE - DATA OFFLINE"
        description: "Placement group {{ $labels.pg }} is not active. DATA IS UNAVAILABLE!"

    - alert: CephPoolNearFull
      expr: |
        (ceph_pool_bytes_used / ceph_pool_max_avail) > 0.90
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Ceph pool {{ $labels.pool }} >90% full"
        description: "Storage pool at {{ $value | humanizePercentage }} capacity. Writes may fail soon!"

  # ================================================
  # SERVICE MESH (Istio/Cilium) - Netflix Pattern
  # ================================================
  - name: service.mesh
    interval: 30s
    rules:
    - alert: ServiceMesh5xxErrors
      expr: |
        sum(rate(istio_request_total{response_code=~"5.."}[5m])) by (destination_service_name)
        /
        sum(rate(istio_request_total[5m])) by (destination_service_name) > 0.01
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Service {{ $labels.destination_service_name }} error rate >1%"
        description: "5xx error rate is {{ $value | humanizePercentage }}. Service degraded!"

    - alert: ServiceMeshHighLatency
      expr: |
        histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket[5m])) by (destination_service_name, le)) > 1000
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Service {{ $labels.destination_service_name }} latency >1s (p99)"
        description: "99th percentile latency is {{ $value }}ms. Users experiencing delays."

  # ================================================
  # CERTIFICATE EXPIRY (Critical for HTTPS)
  # ================================================
  - name: certificates
    interval: 1h
    rules:
    - alert: CertificateExpiringSoon
      expr: |
        certmanager_certificate_ready_time_seconds - time() < 7 * 24 * 3600
      for: 1h
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Certificate {{ $labels.name }} expires in < 7 days"
        description: "Certificate will expire in {{ $value | humanizeDuration }}. Renewal required!"

    - alert: CertificateExpiryCritical
      expr: |
        certmanager_certificate_ready_time_seconds - time() < 24 * 3600
      for: 1m
      labels:
        severity: critical
        tier: "0"
      annotations:
        summary: "Certificate {{ $labels.name }} expires in < 24 HOURS!"
        description: "CRITICAL: Certificate expires in {{ $value | humanizeDuration }}. Service outage imminent!"

  # ================================================
  # SLO/ERROR BUDGET (Google SRE Pattern)
  # ================================================
  - name: slo.error.budget
    interval: 30s
    rules:
    # Fast burn rate - 2% budget in 1 hour
    - alert: SLOErrorBudgetBurnRateHigh
      expr: |
        (
          rate(http_requests_total{code=~"5.."}[1h])
          /
          rate(http_requests_total[1h])
        ) > 0.02
      for: 5m
      labels:
        severity: critical
        tier: "1"
        alertname: "ErrorBudgetBurnRate"
      annotations:
        summary: "Error budget burn rate >2%/hour for {{ $labels.service }}"
        description: "At current rate, monthly error budget will be exhausted in {{ $value }} hours!"

    # Slow burn rate - 5% budget in 6 hours
    - alert: SLOErrorBudgetBurnRateMedium
      expr: |
        (
          rate(http_requests_total{code=~"5.."}[6h])
          /
          rate(http_requests_total[6h])
        ) > 0.05
      for: 30m
      labels:
        severity: warning
        tier: "2"
        alertname: "ErrorBudgetBurnRate"
      annotations:
        summary: "Error budget burn rate >5%/6h for {{ $labels.service }}"
        description: "Sustained error rate depleting monthly budget. Investigation needed."

  # ================================================
  # DATABASE HEALTH (Critical for Applications)
  # ================================================
  - name: database.health
    interval: 30s
    rules:
    - alert: PostgreSQLDown
      expr: pg_up == 0
      for: 1m
      labels:
        severity: critical
        tier: "0"
        component: database
      annotations:
        summary: "PostgreSQL instance {{ $labels.instance }} is DOWN"
        description: "Database has been down for > 1 minute. Applications affected!"

    - alert: PostgreSQLTooManyConnections
      expr: |
        sum by (instance) (pg_stat_activity_count)
        /
        sum by (instance) (pg_settings_max_connections) > 0.95
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "PostgreSQL connection pool nearly exhausted (>95%)"
        description: "{{ $value | humanizePercentage }} of connections used. New connections will fail!"

    - alert: PostgreSQLSlowQueries
      expr: |
        rate(pg_stat_statements_mean_exec_time_seconds[5m]) > 1
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "PostgreSQL slow queries detected"
        description: "Average query time > 1 second. Database performance degraded."

  # ================================================
  # AUTHENTICATION (Authelia) - Security Critical
  # ================================================
  - name: authentication
    interval: 30s
    rules:
    - alert: AutheliaDown
      expr: up{job="authelia"} == 0
      for: 1m
      labels:
        severity: critical
        tier: "0"
        component: authentication
      annotations:
        summary: "Authelia authentication service is DOWN"
        description: "Authentication service unavailable. Users cannot login!"

    - alert: AutheliaHighFailureRate
      expr: |
        rate(authelia_authentication_attempts_total{success="false"}[5m])
        /
        rate(authelia_authentication_attempts_total[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
        tier: "2"
        alertname: "Security"
      annotations:
        summary: "High authentication failure rate (>10%)"
        description: "{{ $value | humanizePercentage }} of auth attempts failing. Possible attack or misconfiguration."