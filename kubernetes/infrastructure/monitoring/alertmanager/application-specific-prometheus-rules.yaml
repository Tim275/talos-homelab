# ðŸŽ¯ APPLICATION-SPECIFIC PROMETHEUS RULES
# Enterprise monitoring for ALL cluster applications
# Based on your actual running services

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: application-specific-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
    alertmanager: main
spec:
  groups:
  # ================================================
  # ELASTICSEARCH CLUSTER (ELK Stack)
  # ================================================
  - name: elasticsearch.cluster
    interval: 30s
    rules:
    # Cluster Health Status
    - alert: ElasticsearchClusterRed
      expr: elasticsearch_cluster_health_status{color="red"} == 1
      for: 1m
      labels:
        severity: critical
        tier: "0"
        component: elasticsearch
      annotations:
        summary: "ðŸ”´ Elasticsearch cluster is RED - DATA UNAVAILABLE!"
        description: "Cluster {{ $labels.cluster }} is RED. Some shards are unassigned. DATA LOSS RISK!"
        runbook_url: "https://runbooks.homelab.io/ElasticsearchClusterRed"

    - alert: ElasticsearchClusterYellow
      expr: elasticsearch_cluster_health_status{color="yellow"} == 1
      for: 10m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "ðŸŸ¡ Elasticsearch cluster is YELLOW"
        description: "Cluster {{ $labels.cluster }} is YELLOW for > 10 minutes. Replica shards unassigned."

    # Node Issues
    - alert: ElasticsearchNodeDiskUsageHigh
      expr: |
        (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes)
        / elasticsearch_filesystem_data_size_bytes > 0.85
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Elasticsearch node disk >85% full"
        description: "Node {{ $labels.node }} disk usage at {{ $value | humanizePercentage }}."

    # JVM Memory Pressure
    - alert: ElasticsearchJVMMemoryPressure
      expr: |
        elasticsearch_jvm_memory_used_bytes{area="heap"}
        / elasticsearch_jvm_memory_max_bytes{area="heap"} > 0.9
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Elasticsearch JVM heap >90%"
        description: "JVM heap usage at {{ $value | humanizePercentage }}. GC storms imminent!"

    # Pending Tasks
    - alert: ElasticsearchPendingTasksHigh
      expr: elasticsearch_cluster_health_number_of_pending_tasks > 100
      for: 10m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Elasticsearch has {{ $value }} pending tasks"
        description: "Cluster overloaded with pending operations. Performance degraded."

  # ================================================
  # ROOK-CEPH ADVANCED MONITORING
  # ================================================
  - name: ceph.advanced
    interval: 30s
    rules:
    # MON Quorum
    - alert: CephMonQuorumLost
      expr: |
        ceph_mon_quorum_status == 0
      for: 1m
      labels:
        severity: critical
        tier: "0"
        component: storage
      annotations:
        summary: "Ceph MON quorum LOST!"
        description: "Monitor {{ $labels.ceph_daemon }} lost quorum. Cluster decisions impossible!"

    # MDS Status
    - alert: CephMDSDown
      expr: ceph_mds_metadata{state!="up:active"} == 1
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "CephFS MDS {{ $labels.ceph_daemon }} is DOWN"
        description: "Metadata server not active. CephFS unavailable!"

    # OSD Near Full
    - alert: CephOSDNearFull
      expr: ceph_osd_stat_bytes_used / ceph_osd_stat_bytes > 0.85
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Ceph OSD {{ $labels.ceph_daemon }} >85% full"
        description: "OSD at {{ $value | humanizePercentage }} capacity. Rebalancing needed."

    # Slow Operations
    - alert: CephSlowOperations
      expr: ceph_osd_op_r_latency_sum > 1000 or ceph_osd_op_w_latency_sum > 1000
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Ceph slow operations detected"
        description: "OSD {{ $labels.ceph_daemon }} has slow {{ $labels.op_type }} operations > 1s."

    # PG States
    - alert: CephPGInconsistent
      expr: ceph_pg_inconsistent > 0
      for: 1m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Ceph has {{ $value }} inconsistent PGs"
        description: "Data inconsistency detected! Manual repair required."

  # ================================================
  # ISTIO SERVICE MESH
  # ================================================
  - name: istio.mesh
    interval: 30s
    rules:
    # Control Plane Health
    - alert: IstioControlPlaneNotReady
      expr: up{job="istiod"} == 0
      for: 2m
      labels:
        severity: critical
        tier: "1"
        component: service-mesh
      annotations:
        summary: "Istio control plane (istiod) is DOWN"
        description: "Istiod has been down for > 2 minutes. Service mesh configuration frozen!"

    # Pilot Errors
    - alert: IstioPilotPushErrors
      expr: rate(pilot_xds_push_errors[5m]) > 0
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Istio Pilot pushing errors to Envoy"
        description: "Configuration push errors at {{ $value }} per second. Proxies may be misconfigured."

    # Envoy Proxy Issues
    - alert: IstioHighEnvoyRejections
      expr: |
        rate(envoy_http_inbound_downstream_rq_rejected[5m]) > 10
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "High Envoy rejection rate"
        description: "Envoy rejecting {{ $value }} requests/sec. Circuit breaker or rate limit triggered."

    # mTLS Issues
    - alert: IstioMTLSConfigurationError
      expr: |
        sum(rate(citadel_server_csr_sign_error_count[5m])) > 0
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Istio mTLS certificate signing errors"
        description: "Certificate signing failures. Service-to-service authentication broken!"

  # ================================================
  # CILIUM CNI
  # ================================================
  - name: cilium.cni
    interval: 30s
    rules:
    # Agent Health
    - alert: CiliumAgentNotReady
      expr: |
        cilium_agent_unreachable_nodes > 0
      for: 5m
      labels:
        severity: critical
        tier: "1"
        component: networking
      annotations:
        summary: "Cilium has {{ $value }} unreachable nodes"
        description: "Network connectivity issues. Pods on affected nodes cannot communicate!"

    # Endpoint Issues
    - alert: CiliumEndpointStateInvalid
      expr: |
        cilium_endpoint_state{state="invalid"} > 0
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Cilium endpoints in invalid state"
        description: "{{ $value }} endpoints are invalid. Network policies may not apply correctly."

    # Policy Drops
    - alert: CiliumHighPolicyDropRate
      expr: |
        rate(cilium_drop_count_total{reason="Policy denied"}[5m]) > 100
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "High network policy drop rate"
        description: "Cilium dropping {{ $value }} packets/sec due to policy. Possible misconfiguration."

    # BPF Issues
    - alert: CiliumBPFMapPressure
      expr: |
        cilium_bpf_map_pressure > 0.9
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Cilium BPF map pressure high"
        description: "BPF map at {{ $value | humanizePercentage }} capacity. Performance degradation possible."

  # ================================================
  # ARGOCD GITOPS
  # ================================================
  - name: argocd.gitops
    interval: 30s
    rules:
    # App Sync Failures
    - alert: ArgocdApplicationSyncFailed
      expr: |
        argocd_app_health_total{health_status="Degraded"} > 0
      for: 10m
      labels:
        severity: warning
        tier: "2"
        component: gitops
      annotations:
        summary: "ArgoCD application {{ $labels.name }} sync failed"
        description: "Application has been degraded for > 10 minutes. Manual intervention may be required."

    # Sync Out of Sync
    - alert: ArgocdApplicationOutOfSync
      expr: |
        argocd_app_sync_total{sync_status="OutOfSync"} > 0
      for: 30m
      labels:
        severity: info
        tier: "3"
      annotations:
        summary: "ArgoCD application {{ $labels.name }} out of sync"
        description: "Application drift detected for > 30 minutes."

    # Repo Server Issues
    - alert: ArgocdRepoServerNotReady
      expr: up{job="argocd-repo-server"} == 0
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "ArgoCD repo server is DOWN"
        description: "Repository server unavailable. New deployments blocked!"

    # Redis Issues
    - alert: ArgocdRedisDown
      expr: up{job="argocd-redis"} == 0
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "ArgoCD Redis is DOWN"
        description: "Redis cache unavailable. ArgoCD performance severely degraded!"

  # ================================================
  # VELERO BACKUP
  # ================================================
  - name: velero.backup
    interval: 30s
    rules:
    - alert: VeleroBackupFailed
      expr: |
        velero_backup_failure_total > 0
      for: 1m
      labels:
        severity: critical
        tier: "1"
        component: backup
      annotations:
        summary: "Velero backup FAILED"
        description: "Backup {{ $labels.backup }} failed. Disaster recovery compromised!"

    - alert: VeleroBackupPartialFailure
      expr: |
        velero_backup_partial_failure_total > 0
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Velero backup partially failed"
        description: "Backup {{ $labels.backup }} had partial failures. Some resources not backed up."

    - alert: VeleroBackupNotRunning
      expr: |
        time() - velero_backup_last_successful_timestamp > 86400
      for: 1h
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "No successful Velero backup in 24h"
        description: "Last successful backup was {{ $value | humanizeDuration }} ago."

  # ================================================
  # SEALED SECRETS
  # ================================================
  - name: sealed.secrets
    interval: 30s
    rules:
    - alert: SealedSecretsControllerDown
      expr: up{job="sealed-secrets-controller"} == 0
      for: 5m
      labels:
        severity: critical
        tier: "1"
        component: security
      annotations:
        summary: "Sealed Secrets controller is DOWN"
        description: "Cannot decrypt secrets. New deployments will fail!"

    - alert: SealedSecretsUnsealError
      expr: |
        rate(sealed_secrets_controller_unseal_errors_total[5m]) > 0
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Sealed Secrets unseal errors"
        description: "Failed to unseal {{ $value }} secrets/sec. Encryption key issue!"

  # ================================================
  # LOKI LOGGING
  # ================================================
  - name: loki.logging
    interval: 30s
    rules:
    - alert: LokiIngesterDown
      expr: up{job="loki-ingester"} == 0
      for: 5m
      labels:
        severity: critical
        tier: "1"
        component: logging
      annotations:
        summary: "Loki ingester is DOWN"
        description: "Log ingestion stopped. Logs are being lost!"

    - alert: LokiRequestErrors
      expr: |
        rate(loki_request_errors_total[5m]) > 0.05
      for: 10m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Loki request error rate >5%"
        description: "Error rate {{ $value | humanizePercentage }}. Log queries failing."

    - alert: LokiIngestionRateHigh
      expr: |
        rate(loki_ingester_streams_created_total[5m]) > 1000
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Loki stream creation rate very high"
        description: "Creating {{ $value }} streams/sec. Cardinality explosion risk!"

  # ================================================
  # VECTOR LOG AGGREGATION
  # ================================================
  - name: vector.logs
    interval: 30s
    rules:
    - alert: VectorComponentError
      expr: |
        rate(vector_component_errors_total[5m]) > 0
      for: 5m
      labels:
        severity: warning
        tier: "2"
        component: logging
      annotations:
        summary: "Vector component {{ $labels.component }} has errors"
        description: "Component error rate {{ $value }}/sec. Log pipeline degraded."

    - alert: VectorHighMemoryUsage
      expr: |
        vector_memory_used_bytes / vector_memory_total_bytes > 0.9
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Vector memory usage >90%"
        description: "Memory at {{ $value | humanizePercentage }}. Buffer overflow risk."

  # ================================================
  # JAEGER TRACING
  # ================================================
  - name: jaeger.tracing
    interval: 30s
    rules:
    - alert: JaegerCollectorDown
      expr: up{job="jaeger-collector"} == 0
      for: 5m
      labels:
        severity: warning
        tier: "2"
        component: observability
      annotations:
        summary: "Jaeger collector is DOWN"
        description: "Trace collection stopped. Distributed tracing unavailable."

    - alert: JaegerSpanDropped
      expr: |
        rate(jaeger_collector_spans_dropped_total[5m]) > 100
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Jaeger dropping spans"
        description: "Dropping {{ $value }} spans/sec. Trace data being lost!"

  # ================================================
  # OPENTELEMETRY COLLECTOR
  # ================================================
  - name: opentelemetry.collector
    interval: 30s
    rules:
    - alert: OtelCollectorDataLoss
      expr: |
        rate(otelcol_processor_dropped_spans[5m]) > 0 or
        rate(otelcol_processor_dropped_metric_points[5m]) > 0
      for: 5m
      labels:
        severity: warning
        tier: "2"
        component: observability
      annotations:
        summary: "OpenTelemetry collector dropping data"
        description: "Telemetry data being lost at {{ $value }}/sec."

    - alert: OtelCollectorQueueFull
      expr: |
        otelcol_exporter_queue_size / otelcol_exporter_queue_capacity > 0.9
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "OpenTelemetry export queue >90% full"
        description: "Queue at {{ $value | humanizePercentage }} capacity. Back pressure building."

  # ================================================
  # HUBBLE NETWORK OBSERVABILITY
  # ================================================
  - name: hubble.observability
    interval: 30s
    rules:
    - alert: HubbleRelayDown
      expr: up{job="hubble-relay"} == 0
      for: 5m
      labels:
        severity: warning
        tier: "2"
        component: observability
      annotations:
        summary: "Hubble Relay is DOWN"
        description: "Network flow visibility unavailable. Cannot observe traffic patterns."

    - alert: HubbleHighDropRate
      expr: |
        rate(hubble_flows_dropped_total[5m]) > 1000
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Hubble dropping network flows"
        description: "Dropping {{ $value }} flows/sec. Network visibility degraded."

  # ================================================
  # CLOUDNATIVE-PG POSTGRESQL
  # ================================================
  - name: cnpg.postgresql
    interval: 30s
    rules:
    - alert: CNPGClusterNotHealthy
      expr: |
        cnpg_cluster_status{status!="Healthy"} == 1
      for: 5m
      labels:
        severity: critical
        tier: "1"
        component: database
      annotations:
        summary: "CNPG cluster {{ $labels.cluster }} not healthy"
        description: "PostgreSQL cluster status: {{ $labels.status }}. Database availability at risk!"

    - alert: CNPGReplicationLag
      expr: |
        cnpg_replication_lag_seconds > 10
      for: 5m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "CNPG replication lag >10s"
        description: "Replica {{ $labels.instance }} lagging by {{ $value }}s. Data consistency risk."

    - alert: CNPGBackupFailed
      expr: |
        cnpg_backup_failed == 1
      for: 1m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "CNPG backup FAILED"
        description: "PostgreSQL backup for {{ $labels.cluster }} failed. Recovery point objective compromised!"

  # ================================================
  # INFLUXDB TIME SERIES
  # ================================================
  - name: influxdb.timeseries
    interval: 30s
    rules:
    - alert: InfluxDBDown
      expr: up{job="influxdb"} == 0
      for: 5m
      labels:
        severity: critical
        tier: "1"
        component: database
      annotations:
        summary: "InfluxDB is DOWN"
        description: "Time series database unavailable. Metrics storage offline!"

    - alert: InfluxDBHighCardinality
      expr: |
        influxdb_database_series_count > 1000000
      for: 10m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "InfluxDB series cardinality >1M"
        description: "Database {{ $labels.database }} has {{ $value }} series. Performance impact!"

  # ================================================
  # OPENCOST FINOPS
  # ================================================
  - name: opencost.finops
    interval: 30s
    rules:
    - alert: OpencostHighSpend
      expr: |
        opencost_namespace_hourly_cost > 10
      for: 1h
      labels:
        severity: warning
        tier: "2"
        component: finops
      annotations:
        summary: "Namespace {{ $labels.namespace }} cost >$10/hour"
        description: "Spending ${{ $value }}/hour. Budget review needed!"

    - alert: OpencostDataUnavailable
      expr: up{job="opencost"} == 0
      for: 30m
      labels:
        severity: info
        tier: "3"
      annotations:
        summary: "OpenCost metrics unavailable"
        description: "Cost visibility lost. Cannot track cloud spend."