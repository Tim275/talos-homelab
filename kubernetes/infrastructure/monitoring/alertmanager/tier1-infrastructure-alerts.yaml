# yamllint disable rule:line-length
apiVersion: v1
kind: ConfigMap
metadata:
  name: tier1-infrastructure-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus-stack
data:
  tier1-infrastructure.yaml: |
    groups:
      # ========================================
      # ðŸŸ  TIER-1: INFRASTRUCTURE & NODES
      # ========================================
      # Core infrastructure - P1/P2/P3 priority
      # SLA: P1 = 5min, P2 = 15min, P3 = 1hr

      - name: tier1.nodes.critical
        interval: 30s
        rules:
          # ====== NODE AVAILABILITY (P1 - DRINGEND) ======
          - alert: NodeDown
            expr: up{job="node-exporter"} == 0
            for: 3m
            labels:
              severity: critical
              priority: P1
              tier: "1"
              component: node
            annotations:
              summary: "Node {{ $labels.instance }} is DOWN"
              description: |
                Node {{ $labels.instance }} has been unreachable for 3 minutes. All workloads on this node are affected!
              current_value: "Node unreachable"
              threshold: "100% availability required"
              dashboard_url: "https://grafana.homelab.local/d/k8s-views-nodes/kubernetes-views-nodes"
              runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/node/nodedown"

          - alert: NodeNotReady
            expr: kube_node_status_condition{condition="Ready",status="true"} == 0
            for: 5m
            labels:
              severity: critical
              priority: P1
              tier: "1"
              component: node
            annotations:
              summary: "Node {{ $labels.node }} is NotReady"
              description: |
                Node {{ $labels.node }} has been in NotReady state for 5 minutes. Kubelet or networking issue detected.
              current_value: "NotReady"
              threshold: "Ready status required"
              dashboard_url: "https://grafana.homelab.local/d/k8s-views-nodes/kubernetes-views-nodes"
              runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready"

          # ====== NODE RESOURCES (P2 - WICHTIG) ======
          - alert: NodeMemoryPressure
            expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
            for: 5m
            labels:
              severity: critical
              priority: P2
              tier: "1"
              component: node
            annotations:
              summary: "Node {{ $labels.node }} under memory pressure"
              description: "Node {{ $labels.node }} is experiencing memory pressure. Pod evictions may occur!"
              current_value: "Memory pressure detected"
              threshold: "No memory pressure allowed"
              dashboard_url: "https://grafana.homelab.local/d/k8s-views-nodes/kubernetes-views-nodes"
              runbook_url: "https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/"

          - alert: NodeDiskPressure
            expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
            for: 5m
            labels:
              severity: critical
              priority: P2
              tier: "1"
              component: node
            annotations:
              summary: "Node {{ $labels.node }} under disk pressure"
              description: "Node {{ $labels.node }} is experiencing disk pressure. Pod evictions imminent!"
              current_value: "Disk pressure detected"
              threshold: "No disk pressure allowed"
              dashboard_url: "https://grafana.homelab.local/d/k8s-views-nodes/kubernetes-views-nodes"
              runbook_url: "https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/"

          - alert: NodeHighCPUUsage
            expr: (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance)) * 100 > 90
            for: 15m
            labels:
              severity: warning
              priority: P3
              tier: "1"
              component: node
            annotations:
              summary: "Node {{ $labels.instance }} CPU > 90%"
              description: |
                Node {{ $labels.instance }} CPU usage is {{ $value | humanize }}% for 15 minutes. Resource saturation detected.
              current_value: "{{ $value | humanize }}%"
              threshold: "90%"
              dashboard_url: "https://grafana.homelab.local/d/talos-node-exporter-full/node-exporter-full"
              runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/node/nodehighcpuload"

          - alert: NodeHighMemoryUsage
            expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
            for: 10m
            labels:
              severity: warning
              priority: P3
              tier: "1"
              component: node
            annotations:
              summary: "Node {{ $labels.instance }} memory > 90%"
              description: |
                Node {{ $labels.instance }} memory usage is {{ $value | humanize }}% for 10 minutes. Approaching memory limits.
              current_value: "{{ $value | humanize }}%"
              threshold: "90%"
              dashboard_url: "https://grafana.homelab.local/d/talos-node-exporter-full/node-exporter-full"
              runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/node/nodehighmemoryusage"

          - alert: NodeFilesystemSpaceCritical
            expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.*"} / node_filesystem_size_bytes)) * 100 > 85
            for: 10m
            labels:
              severity: critical
              priority: P2
              tier: "1"
              component: node
            annotations:
              summary: "Node {{ $labels.instance }} filesystem {{ $labels.mountpoint }} > 85% full"
              description: |
                Filesystem {{ $labels.mountpoint }} on {{ $labels.instance }} is {{ $value | humanize }}% full. Critical disk space!
              current_value: "{{ $value | humanize }}%"
              threshold: "85%"
              dashboard_url: "https://grafana.homelab.local/d/talos-node-exporter-full/node-exporter-full"
              runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace"

          - alert: NodeFilesystemInodesAlmostFull
            expr: (1 - (node_filesystem_files_free / node_filesystem_files)) * 100 > 90
            for: 10m
            labels:
              severity: critical
              priority: P2
              tier: "1"
              component: node
            annotations:
              summary: "Node {{ $labels.instance }} filesystem {{ $labels.mountpoint }} inodes > 90%"
              description: |
                Filesystem {{ $labels.mountpoint }} on {{ $labels.instance }} has {{ $value | humanize }}% inodes used. Risk of 'No space left' errors!
              current_value: "{{ $value | humanize }}%"
              threshold: "90%"
              dashboard_url: "https://grafana.homelab.local/d/talos-node-exporter-full/node-exporter-full"
              runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup"

          # ====== NETWORK (P2/P3) ======
          - alert: CiliumAgentDown
            expr: up{job="cilium-agent"} == 0
            for: 3m
            labels:
              severity: critical
              priority: P2
              tier: "1"
              component: network
            annotations:
              summary: "Cilium agent DOWN on {{ $labels.instance }}"
              description: "Cilium agent on {{ $labels.instance }} is unreachable. Networking disrupted on this node!"
              current_value: "Cilium agent unreachable"
              threshold: "100% availability required"
              dashboard_url: "https://grafana.homelab.local/d/cilium-agent/cilium-agent-metrics"
              runbook_url: "https://docs.cilium.io/en/stable/operations/troubleshooting/"

          - alert: CiliumOperatorDown
            expr: up{job="cilium-operator"} == 0
            for: 5m
            labels:
              severity: critical
              priority: P2
              tier: "1"
              component: network
            annotations:
              summary: "Cilium operator is DOWN"
              description: "Cilium operator is unavailable. Network policy enforcement and BGP may be affected!"
              current_value: "Operator unreachable"
              threshold: "100% availability required"
              dashboard_url: "https://grafana.homelab.local/d/cilium-operator/cilium-operator-metrics"
              runbook_url: "https://docs.cilium.io/en/stable/operations/troubleshooting/"

          - alert: CiliumEndpointNotReady
            expr: cilium_endpoint_state{endpoint_state="not-ready"} > 0
            for: 5m
            labels:
              severity: warning
              priority: P3
              tier: "1"
              component: network
            annotations:
              summary: "{{ $value }} Cilium endpoints not ready"
              description: |
                Cilium has {{ $value }} endpoints in not-ready state. Networking issues detected for some pods.
              current_value: "{{ $value }} endpoints"
              threshold: "0 not-ready endpoints"
              dashboard_url: "https://grafana.homelab.local/d/cilium-agent/cilium-agent-metrics"
              runbook_url: "https://docs.cilium.io/en/stable/operations/troubleshooting/"

          - alert: CiliumConntrackTableFull
            expr: (cilium_datapath_conntrack_gc_entries / cilium_datapath_conntrack_gc_size_current) > 0.9
            for: 10m
            labels:
              severity: warning
              priority: P3
              tier: "1"
              component: network
            annotations:
              summary: "Cilium conntrack table > 90% full on {{ $labels.instance }}"
              description: |
                Conntrack table on {{ $labels.instance }} is {{ $value | humanizePercentage }} full. Connection tracking may drop!
              current_value: "{{ $value | humanizePercentage }}"
              threshold: "90%"
              dashboard_url: "https://grafana.homelab.local/d/cilium-agent/cilium-agent-metrics"
              runbook_url: "https://docs.cilium.io/en/stable/operations/performance/"

          - alert: HighNetworkErrorRate
            expr: rate(node_network_receive_errs_total[5m]) > 10
            for: 10m
            labels:
              severity: warning
              priority: P3
              tier: "1"
              component: network
            annotations:
              summary: "High network error rate on {{ $labels.instance }} interface {{ $labels.device }}"
              description: |
                Network interface {{ $labels.device }} on {{ $labels.instance }} is seeing {{ $value }} receive errors/sec. Hardware or driver issue.
              current_value: "{{ $value }} errors/sec"
              threshold: "10 errors/sec"
              dashboard_url: "https://grafana.homelab.local/d/talos-node-exporter-full/node-exporter-full"
              runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs"

      - name: tier1.pods.critical
        interval: 30s
        rules:
          # ====== POD FAILURES (P2/P3) ======
          - alert: PodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
            for: 10m
            labels:
              severity: critical
              priority: P2
              tier: "1"
              component: workload
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
              description: |
                Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container }} is restarting {{ $value | humanize }} times/sec. Application failure!
              current_value: "{{ $value | humanize }} restarts/sec"
              threshold: "0 restarts"
              dashboard_url: "https://grafana.homelab.local/d/k8s-views-pods/kubernetes-views-pods"
              runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping"

          - alert: PodNotReady
            expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0
            for: 15m
            labels:
              severity: warning
              priority: P3
              tier: "1"
              component: workload
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not ready for 15m"
              description: |
                Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in {{ $labels.phase }} state for 15 minutes. Investigate pod events.
              current_value: "{{ $labels.phase }}"
              threshold: "Running state required"
              dashboard_url: "https://grafana.homelab.local/d/k8s-views-pods/kubernetes-views-pods"
              runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready"

          - alert: PodMemoryOOMKilled
            expr: increase(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[5m]) > 0
            for: 1m
            labels:
              severity: critical
              priority: P2
              tier: "1"
              component: workload
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container }} OOM killed"
              description: |
                Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} was OOM killed. Increase memory limits!
              current_value: "OOM killed"
              threshold: "0 OOM kills"
              dashboard_url: "https://grafana.homelab.local/d/k8s-views-pods/kubernetes-views-pods"
              runbook_url: "https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"

          - alert: PodHighCPUThrottling
            expr: rate(container_cpu_cfs_throttled_seconds_total{container!=""}[5m]) > 0.5
            for: 10m
            labels:
              severity: warning
              priority: P3
              tier: "1"
              component: workload
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container }} heavily throttled"
              description: |
                Container {{ $labels.container }} is being throttled {{ $value | humanizePercentage }} of the time. Increase CPU limits!
              current_value: "{{ $value | humanizePercentage }}"
              threshold: "50% throttling"
              dashboard_url: "https://grafana.homelab.local/d/k8s-views-pods/kubernetes-views-pods"
              runbook_url: "https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"

          # ====== PERSISTENT VOLUMES (P2) ======
          - alert: PersistentVolumeFillingUp
            expr: (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes) * 100 < 15
            for: 10m
            labels:
              severity: critical
              priority: P2
              tier: "1"
              component: storage
            annotations:
              summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} < 15% free"
              description: |
                PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} has only {{ $value | humanize }}% free space. Risk of application failures!
              current_value: "{{ $value | humanize }}%"
              threshold: "15% free"
              dashboard_url: "https://grafana.homelab.local/d/k8s-views-global/kubernetes-views-global"
              runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup"

          - alert: PersistentVolumeInodesFillingUp
            expr: (kubelet_volume_stats_inodes_free / kubelet_volume_stats_inodes) * 100 < 10
            for: 10m
            labels:
              severity: critical
              priority: P2
              tier: "1"
              component: storage
            annotations:
              summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} inodes < 10% free"
              description: |
                PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} has only {{ $value | humanize }}% inodes free. Risk of 'No space' errors!
              current_value: "{{ $value | humanize }}%"
              threshold: "10% free"
              dashboard_url: "https://grafana.homelab.local/d/k8s-views-global/kubernetes-views-global"
              runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefilesfillingup"
