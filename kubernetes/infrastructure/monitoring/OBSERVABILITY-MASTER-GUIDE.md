# Observability Master Guide - Talos Homelab Production Setup

## ğŸ“– Table of Contents

1. [Executive Summary](#executive-summary)
2. [Was ist Grafana?](#was-ist-grafana)
3. [Grafana Operator vs Helm Chart](#grafana-operator-vs-helm-chart)
4. [Was ist Prometheus?](#was-ist-prometheus)
5. [Prometheus Operator Magie](#prometheus-operator-magie)
6. [Was ist Loki?](#was-ist-loki)
7. [Was ist Tempo?](#was-ist-tempo)
8. [Was ist Thanos?](#was-ist-thanos)
9. [Unser Production Setup](#unser-production-setup)
10. [Complete Architecture](#complete-architecture)
11. [ServiceMonitor â†’ Dashboard (No Data Fix!)](#servicemonitor--dashboard-no-data-fix)
12. [Grafana Dashboards Deep Dive](#grafana-dashboards-deep-dive)
13. [Prometheus Metrics Pipeline](#prometheus-metrics-pipeline)
14. [Loki Log Pipeline](#loki-log-pipeline)
15. [Tempo Trace Pipeline](#tempo-trace-pipeline)
16. [Thanos Long-term Storage](#thanos-long-term-storage)
17. [Best Practices](#best-practices)
18. [Daily Operations](#daily-operations)
19. [Troubleshooting](#troubleshooting)
20. [Performance Optimization](#performance-optimization)
21. [Backup & Disaster Recovery](#backup--disaster-recovery)
22. [Quick Reference](#quick-reference)

---

## Executive Summary

### TL;DR - Was haben wir gebaut?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ENTERPRISE OBSERVABILITY STACK (100% IaC)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Grafana Operator - 68 Enterprise Dashboards (CRDs!)         â”‚
â”‚ âœ… Prometheus Operator - Auto-Discovery via ServiceMonitors    â”‚
â”‚ âœ… Loki - Log Aggregation (LogQL queries)                      â”‚
â”‚ âœ… Tempo - Distributed Tracing (OTLP + Jaeger)                 â”‚
â”‚ âœ… Jaeger - Trace Frontend (Tempo Backend)                     â”‚
â”‚ âœ… Thanos - Unlimited Metrics Storage (Ceph S3)                â”‚
â”‚ âœ… Alertmanager + Robusta AI - Dual Alerting                   â”‚
â”‚ âœ… 100% GitOps (ArgoCD synced)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¯ Was macht das Stack so gut?

**IKEA-Analogie:** Du kaufst ein Regal (Grafana Operator), alle Schrauben sind dabei (CRDs), und es baut sich selbst zusammen (GitOps)! ğŸ› ï¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Alte Methode (Helm Chart):                    â”‚
â”‚ 1. helm install grafana                       â”‚
â”‚ 2. Dashboard manuell via UI importieren       â”‚
â”‚ 3. Bei Cluster-Neustart: Weg! ğŸ’¥              â”‚
â”‚ 4. Backup? Manuell! ğŸ˜°                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Neue Methode (Grafana Operator + CRDs):       â”‚
â”‚ 1. kubectl apply -f dashboard.yaml            â”‚
â”‚ 2. Fertig! âœ…                                 â”‚
â”‚ 3. Bei Cluster-Neustart: Auto-restored! ğŸ‰    â”‚
â”‚ 4. Backup? Git commit! ğŸš€                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Metrics

| Metric | Value |
|--------|-------|
| **Grafana Dashboards** | 68 (als CRDs) |
| **Prometheus Targets** | 150+ |
| **Metrics Collected** | 500,000+ time-series |
| **Loki Log Streams** | 50+ |
| **Tempo Traces** | Production-ready (OTLP + Jaeger) |
| **Tempo Retention** | 30 days (Ceph S3) |
| **Prometheus Retention** | 30 days (local) |
| **Thanos Retention** | Unlimited (Ceph S3) |
| **Alert Rules** | 100+ (Tier 0-5) |
| **Query Performance** | <100ms avg |
| **Total Storage** | 500 GB (Prometheus + Loki + Tempo) |

### Tech Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMPLETE OBSERVABILITY STACK - THE THREE PILLARS               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

App Pods (N8N, Kafka, PostgreSQL, etc.)
    â†“
    â”œâ”€ STDOUT/STDERR Logs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Promtail â”€â”€> Loki (Logs)
    â”œâ”€ /metrics Endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Prometheus (Metrics)
    â””â”€ OTLP Traces â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Tempo (Traces)
                                           â†“
                                       Thanos (S3 Storage)
                                           â†“
                                       Grafana (68 Dashboards)
                                           â”œâ”€ Explore Logs (Loki)
                                           â”œâ”€ Explore Metrics (Prometheus)
                                           â””â”€ Explore Traces (Tempo)
                                                   â†“
                                           Jaeger UI (Traces Frontend)
                                                   â†“
                                               Browser (User)
```

---

## Was ist Grafana?

### Definition (IKEA-Style)

**Grafana** = Dein **Fernseher** fÃ¼r Kubernetes ğŸ“º

- Zeigt **Metriken** (Prometheus) = Live TV ğŸ“Š
- Zeigt **Logs** (Loki) = Untertitel ğŸ“
- Zeigt **Traces** (Jaeger) = Behind-the-Scenes ğŸ¬
- Macht **Alerts** (Alertmanager) = Notfall-SMS ğŸš¨

### Use Cases

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WAS KANN GRAFANA?                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Dashboards  â†’ Charts, Graphs, Tables                        â”‚
â”‚ 2. Alerts      â†’ Slack, Email, PagerDuty                       â”‚
â”‚ 3. Datasources â†’ Prometheus, Loki, Elasticsearch               â”‚
â”‚ 4. Folders     â†’ Dashboard Organization                        â”‚
â”‚ 5. Teams       â†’ Multi-Tenancy (wenn du viele User hast)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Concepts

#### 1. **Dashboard** - Deine Monitoring-Seite

Ein Dashboard = Eine Webseite mit Charts

**Beispiel:**
```
Dashboard: "Kubernetes Cluster Overview"
â”œâ”€ Panel 1: CPU Usage (Graph)
â”œâ”€ Panel 2: Memory Usage (Graph)
â”œâ”€ Panel 3: Pod Count (Stat)
â””â”€ Panel 4: Disk Usage (Gauge)
```

#### 2. **Datasource** - Woher kommen die Daten?

Datasource = Datenquelle (Prometheus, Loki, etc.)

**Unsere Datasources:**
```
â”œâ”€ Prometheus (metrics)        â†’ http://prometheus:9090
â”œâ”€ Loki (logs)                 â†’ http://loki:3100
â”œâ”€ Alertmanager (alerts)       â†’ http://alertmanager:9093
â””â”€ Jaeger (traces)             â†’ http://jaeger:16686
```

#### 3. **Panel** - Ein einzelner Chart

Panel = Ein Graph/Table/Stat auf dem Dashboard

**Panel Types:**
```
â”œâ”€ Graph      â†’ Line chart (CPU over time)
â”œâ”€ Stat       â†’ Single number (Pod count: 42)
â”œâ”€ Table      â†’ Tabelle (Pod list)
â”œâ”€ Gauge      â†’ Speedometer (Disk 75%)
â”œâ”€ Heatmap    â†’ Latency distribution
â””â”€ Logs       â†’ Log viewer (Loki)
```

#### 4. **Folder** - Dashboard Organization

Folder = Ordner (wie in Windows Explorer)

**Unsere Folder:**
```
Grafana UI
â”œâ”€ ArgoCD
â”œâ”€ Ceph Storage
â”œâ”€ Cert-Manager
â”œâ”€ Cilium
â”œâ”€ Elasticsearch
â”œâ”€ GPU & ML
â”œâ”€ Istio
â”œâ”€ Kafka
â”œâ”€ Kubernetes
â”œâ”€ Loki
â”œâ”€ OpenTelemetry
â”œâ”€ PostgreSQL
â”œâ”€ Prometheus
â”œâ”€ Redis
â”œâ”€ Security
â”œâ”€ SLO & Reliability
â”œâ”€ Tier 0 Executive
â””â”€ Velero
```

---

## Grafana Operator vs Helm Chart

### âš”ï¸ Der groÃŸe Vergleich

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HELM CHART (Old Way)                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Install Helm chart
  helm install grafana grafana/grafana

Step 2: Port-forward to UI
  kubectl port-forward svc/grafana 3000:3000

Step 3: Login to UI (manual)
  http://localhost:3000
  Username: admin
  Password: (from secret)

Step 4: Import dashboard (manual!)
  - Click "Dashboards" â†’ "Import"
  - Paste JSON
  - Click "Import"
  - Repeat 68 times... ğŸ˜±

Step 5: Configure datasource (manual!)
  - Click "Configuration" â†’ "Data Sources"
  - Add Prometheus
  - Set URL: http://prometheus:9090
  - Click "Save & Test"

âŒ Problems:
  - Manual UI work
  - Not in Git
  - Lost on cluster restart
  - No GitOps
  - No validation
  - No versioning
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GRAFANA OPERATOR (New Way) âœ¨                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Install Grafana Operator
  kubectl apply -k kubernetes/infrastructure/monitoring/grafana-operator/

Step 2: Create Grafana CRD
  kubectl apply -f grafana.yaml

Step 3: Create Dashboard CRD
  kubectl apply -f dashboard.yaml

Step 4: Fertig! âœ…

âœ… Benefits:
  - Everything in Git
  - Auto-applied on push
  - Validated by Kubernetes
  - Versioned via Git
  - Type-safe (CRD schema)
  - GitOps-ready
```

### ğŸ¯ Warum Grafana Operator besser ist

**IKEA-Analogie:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Helm Chart = IKEA-Regal ohne Anleitung ğŸ“¦                     â”‚
â”‚ - Du musst jede Schraube selbst einsetzen                     â”‚
â”‚ - Wenn du vergisst wo, ist es kaputt                          â”‚
â”‚ - Bei Umzug: Alles neu aufbauen                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana Operator = IKEA-Regal mit Auto-Aufbau ğŸ¤–              â”‚
â”‚ - Du gibst Plan (YAML), Operator baut auf                     â”‚
â”‚ - Wenn kaputt: Operator repariert automatisch                 â”‚
â”‚ - Bei Umzug: Operator baut automatisch neu auf                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Konkrete Vorteile

#### 1. **Declarative Configuration (YAML = Plan)**

**Helm (Imperative):**
```bash
# Du musst sagen WIE
helm install grafana grafana/grafana --set admin.password=secret
# â†’ Operator weiÃŸ nicht was du willst, fÃ¼hrt nur Befehl aus
```

**Operator (Declarative):**
```yaml
# Du sagst WAS du willst
apiVersion: grafana.integreatly.org/v1beta1
kind: Grafana
metadata:
  name: grafana
spec:
  config:
    security:
      admin_password: secret
# â†’ Operator versteht dein Ziel, baut es selbst
```

#### 2. **Self-Healing (Auto-Reparatur)**

**Helm:**
```
Wenn Dashboard gelÃ¶scht wird:
  â†’ Weg, fÃ¼r immer ğŸ’¥
  â†’ Du musst manuell re-importieren
```

**Operator:**
```
Wenn Dashboard gelÃ¶scht wird:
  â†’ Operator sieht: "Hey, dashboard.yaml sagt Dashboard soll da sein!"
  â†’ Operator erstellt Dashboard neu âœ…
  â†’ Auto-Healing! ğŸ‰
```

#### 3. **Type Safety (Kubernetes Validation)**

**Helm:**
```yaml
# values.yaml
dashbord: "my-dash"  # Typo! Aber Helm sagt nichts ğŸ˜±
```

**Operator:**
```yaml
# dashboard.yaml
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDashboard
metadata:
  name: my-dash
spec:
  folder: "ArgoCD"
  foldr: "Oops"  # âŒ Kubernetes sagt: "Field 'foldr' unknown!" âœ…
```

#### 4. **GitOps (ArgoCD Auto-Sync)**

**Helm:**
```
git commit â†’ git push â†’ kubectl apply manuell
```

**Operator:**
```
git commit â†’ git push â†’ ArgoCD sieht Ã„nderung â†’ Auto-Apply! ğŸš€
```

#### 5. **Backup = Git Commit!**

**Helm:**
```bash
# Backup? Manuell...
kubectl get configmap grafana-dashboards -o yaml > backup.yaml
# Restore? Auch manuell...
kubectl apply -f backup.yaml
```

**Operator:**
```bash
# Backup = Git!
git commit -m "Add dashboard"
git push

# Restore = Git!
git checkout old-commit
kubectl apply -f dashboard.yaml
```

### ğŸ“Š Vergleichstabelle

| Feature | Helm Chart | Grafana Operator |
|---------|------------|------------------|
| **Dashboard Import** | Manual UI | `kubectl apply -f` |
| **Configuration** | values.yaml | GrafanaDashboard CRD |
| **GitOps** | âš ï¸ Schwer | âœ… Native |
| **Type Safety** | âŒ Nein | âœ… Ja (CRD Schema) |
| **Self-Healing** | âŒ Nein | âœ… Ja |
| **Backup** | Manual export | Git commit |
| **Validation** | âš ï¸ Helm lint | âœ… Kubernetes API |
| **Versioning** | Helm release | Git history |
| **Multi-Dashboard** | 68x manual | 68x `kubectl apply` |

**Winner:** Grafana Operator ğŸ†

---

## Was ist Prometheus?

### Definition (IKEA-Style)

**Prometheus** = Dein **StromzÃ¤hler** fÃ¼r Kubernetes âš¡

- Sammelt **Metrics** (CPU, RAM, Requests)
- Speichert in **Time-Series Database** (TSDB)
- Macht **Alerts** (wenn CPU > 80%)
- Hat **PromQL** (Abfragesprache wie SQL)

### Use Cases

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WAS ÃœBERWACHT PROMETHEUS?                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. System Metrics    â†’ CPU, RAM, Disk (Node Exporter)          â”‚
â”‚ 2. Kubernetes Metricsâ†’ Pods, Deployments (kube-state-metrics)  â”‚
â”‚ 3. App Metrics       â†’ Requests, Latency (deine App)           â”‚
â”‚ 4. Custom Metrics    â†’ Business KPIs (z.B. Sales/hour)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Concepts

#### 1. **Metric** - Eine Zahl die sich Ã¤ndert

```
Metric: node_cpu_seconds_total
â”œâ”€ Type: Counter (immer steigend)
â”œâ”€ Value: 123456 (CPU seconds)
â””â”€ Labels: {cpu="0", mode="idle"}
```

**Metric Types:**
```
Counter   â†’ Immer steigend (Requests total: 1000, 1001, 1002...)
Gauge     â†’ Rauf und runter (CPU: 50%, 80%, 30%...)
Histogram â†’ Verteilung (Latency: 50ms, 100ms, 200ms...)
Summary   â†’ Quantile (p50, p95, p99)
```

#### 2. **Label** - Kategorien

Labels = Tags/Filter fÃ¼r Metrics

**Beispiel:**
```
http_requests_total{method="GET", status="200"} = 1000
http_requests_total{method="POST", status="500"} = 5

â†’ Mit Labels kannst du filtern:
  - Alle GET requests
  - Alle 500 errors
  - POST requests mit 200 OK
```

#### 3. **Scrape** - Daten sammeln

Prometheus **scraped** (sammelt) Metrics von Apps

```
Step 1: App hat /metrics endpoint
  curl http://my-app:8080/metrics
  # HELP http_requests_total Total HTTP requests
  # TYPE http_requests_total counter
  http_requests_total{method="GET"} 1000

Step 2: Prometheus scraped alle 15 Sekunden
  Prometheus â†’ http://my-app:8080/metrics â†’ Speichert in TSDB

Step 3: Du queried mit PromQL
  rate(http_requests_total[5m])
  â†’ Zeigt Requests/sec Ã¼ber letzte 5 Minuten
```

#### 4. **PromQL** - Query Language

PromQL = SQL fÃ¼r Time-Series

**Beispiele:**
```promql
# CPU Usage (%)
100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory Usage (GB)
node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes

# HTTP Error Rate (%)
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100

# Top 5 Pods by CPU
topk(5, rate(container_cpu_usage_seconds_total[5m]))
```

---

## Prometheus Operator Magie

### ğŸª„ Warum Prometheus Operator so geil ist

**Vanilla Prometheus (Old Way):**
```yaml
# prometheus.yml (Manual config)
scrape_configs:
  - job_name: 'my-app'
    static_configs:
      - targets: ['my-app:8080']
```

**Problem:** Du musst **JEDE App manuell** eintragen! ğŸ˜±

**Prometheus Operator (New Way):**
```yaml
# ServiceMonitor CRD (Auto-Discovery!)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app
spec:
  selector:
    matchLabels:
      app: my-app
  endpoints:
  - port: metrics
```

**Magie:** Prometheus **findet automatisch** alle Services mit Label `app: my-app`! ğŸ‰

### ServiceMonitor = Auto-Discovery

**IKEA-Analogie:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vanilla Prometheus = Du musst jede Schraube einzeln zÃ¤hlen    â”‚
â”‚ - Neue App? â†’ Manuell in prometheus.yml eintragen             â”‚
â”‚ - App deleted? â†’ Manuell aus prometheus.yml entfernen         â”‚
â”‚ - Neue Replica? â†’ Manuell alle IPs eintragen                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus Operator = IKEA zÃ¤hlt automatisch                  â”‚
â”‚ - Neue App? â†’ ServiceMonitor sagt "Scrape alles mit Label X"  â”‚
â”‚ - App deleted? â†’ Prometheus sieht das selbst                  â”‚
â”‚ - Neue Replica? â†’ Prometheus findet sie automatisch via K8s   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Konkrete Beispiele

#### Beispiel 1: N8N Metrics scrapen

**Ohne Operator:**
```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'n8n-prod'
    static_configs:
      - targets:
        - n8n-prod-0.n8n-prod.n8n-prod.svc.cluster.local:5678
        - n8n-prod-1.n8n-prod.n8n-prod.svc.cluster.local:5678
        # Oh nein, neue Replica? Manuell hinzufÃ¼gen... ğŸ˜°
```

**Mit Operator:**
```yaml
# servicemonitor-n8n.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: n8n-prod
  namespace: monitoring
spec:
  namespaceSelector:
    matchNames:
    - n8n-prod
  selector:
    matchLabels:
      app: n8n
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
```

**Fertig!** Prometheus scraped **ALLE** N8N Pods automatisch! ğŸš€

#### Beispiel 2: Kafka Metrics scrapen

**ServiceMonitor:**
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kafka
  namespace: monitoring
spec:
  namespaceSelector:
    matchNames:
    - kafka-demo-dev
  selector:
    matchLabels:
      strimzi.io/kind: Kafka
  endpoints:
  - port: tcp-prometheus
    interval: 30s
```

**Prometheus findet automatisch:**
- Alle Kafka Brokers
- Alle Kafka Producers
- Alle Kafka Consumers
- Alle Zookeeper Nodes (falls verwendet)

### ServiceMonitor CRD Struktur

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app                      # Name des ServiceMonitors
  namespace: monitoring              # Namespace (meist 'monitoring')
  labels:
    app: my-app                      # Optional: Labels fÃ¼r Organization
spec:
  # WELCHE Services scrapen?
  namespaceSelector:
    matchNames:
    - my-app-namespace               # Nur Services in diesem Namespace

  # WELCHE Labels haben die Services?
  selector:
    matchLabels:
      app: my-app                    # Service muss Label "app: my-app" haben

  # WO ist der /metrics endpoint?
  endpoints:
  - port: metrics                    # Port name (aus Service)
    path: /metrics                   # URL path
    interval: 30s                    # Scrape interval
    scheme: http                     # http oder https

    # Optional: Relabeling
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_name]
      targetLabel: pod
```

### PodMonitor (Alternative)

Manchmal haben Pods **keinen Service** (z.B. DaemonSets)

**PodMonitor = Scrape direkt von Pods**

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  namespaceSelector:
    matchNames:
    - monitoring
  selector:
    matchLabels:
      app: node-exporter
  podMetricsEndpoints:
  - port: metrics
    interval: 30s
```

---

## Was ist Loki?

### Definition (IKEA-Style)

**Loki** = **Grep** fÃ¼r Kubernetes Logs ğŸ”

- Sammelt **Logs** (STDOUT/STDERR von Pods)
- Speichert **komprimiert** (10x weniger als Elasticsearch!)
- Queried mit **LogQL** (wie PromQL fÃ¼r Logs)
- Integration mit **Grafana** (Logs + Metrics zusammen!)

### Loki vs Elasticsearch

**Frage:** Warum haben wir BEIDE (Loki + Elasticsearch)?

**Antwort:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LOKI = Schnelle Suche fÃ¼r Entwickler ğŸï¸                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Schnell (komprimiert)                                        â”‚
â”‚ âœ… GÃ¼nstig (wenig RAM/Disk)                                     â”‚
â”‚ âœ… Grafana-Integration (Logs + Metrics in einem Dashboard!)    â”‚
â”‚ âŒ Keine Full-Text Search (nur Label-basiert)                  â”‚
â”‚ âŒ Keine komplexen Queries                                     â”‚
â”‚                                                                 â”‚
â”‚ Use Case: "Zeig mir N8N Errors der letzten 5 Minuten"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ELASTICSEARCH = Deep Search fÃ¼r Compliance/Audit ğŸ•µï¸            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Full-Text Search (Google-like)                              â”‚
â”‚ âœ… Komplexe Aggregationen (z.B. "Top 10 Error Messages")       â”‚
â”‚ âœ… Compliance (30-day retention mit ILM)                        â”‚
â”‚ âœ… Kibana UI (professionelle Data Views)                       â”‚
â”‚ âŒ Teuer (viel RAM/Disk)                                       â”‚
â”‚ âŒ Langsamer als Loki                                          â”‚
â”‚                                                                 â”‚
â”‚ Use Case: "Zeig mir alle GDPR-relevanten User-Logins 2024"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fazit:** Beide nutzen! Loki fÃ¼r **schnelles Debugging**, Elasticsearch fÃ¼r **Compliance/Audit**! ğŸ¯

### Core Concepts

#### 1. **Stream** - Log-Kategorie

Stream = Logs mit gleichen Labels

**Beispiel:**
```
Stream: {namespace="n8n-prod", app="n8n", level="error"}
â”œâ”€ Log 1: "Database connection failed"
â”œâ”€ Log 2: "API timeout after 30s"
â””â”€ Log 3: "Workflow execution failed"
```

#### 2. **Label** - Filter

Labels = Tags fÃ¼r Streams (wie in Prometheus!)

**Best Practice:** Wenige Labels (Low Cardinality)

```
âœ… GOOD (Low Cardinality):
{namespace="n8n-prod", level="error"}
â†’ Nur wenige Streams (n8n-prod + error = 1 Stream)

âŒ BAD (High Cardinality):
{namespace="n8n-prod", user_id="123", request_id="abc", trace_id="xyz"}
â†’ Millionen Streams (jeder Request = 1 Stream) â†’ OOM! ğŸ’¥
```

#### 3. **LogQL** - Query Language

LogQL = PromQL fÃ¼r Logs

**Beispiele:**
```logql
# Alle N8N Error Logs
{namespace="n8n-prod", level="error"}

# N8N Logs mit "database" im Text
{namespace="n8n-prod"} |= "database"

# N8N Errors grouped by pod
count_over_time({namespace="n8n-prod", level="error"}[5m]) by (pod)

# Top 5 Error Messages
topk(5, count_over_time({level="error"}[1h]))
```

---

## Was ist Tempo?

### Definition (IKEA-Style)

**Tempo** = **GPS-Tracker** fÃ¼r deine Requests ğŸ›°ï¸

- Tracet **Request-Flows** (von Frontend bis Database)
- Speichert in **S3** (Ceph Object Storage)
- Queried mit **TraceQL** (Filter fÃ¼r Traces)
- Integration mit **Loki & Prometheus** (Trace â†’ Logs â†’ Metrics!)

### Warum Distributed Tracing?

**Problem ohne Tracing:**
```
User meldet: "N8N Workflow ist langsam!" ğŸ¢

Du schaust in:
â”œâ”€ Grafana Metrics â†’ CPU normal, RAM normal
â”œâ”€ Loki Logs â†’ "Workflow executed" ... aber wo ist das Problem?
â””â”€ Keine Ahnung wo die Zeit verloren geht! ğŸ˜°
```

**LÃ¶sung mit Tempo:**
```
User meldet: "N8N Workflow ist langsam!" ğŸ¢

Du schaust in Tempo Trace:
â”œâ”€ Span 1: HTTP Request â†’ N8N API (10ms) âœ…
â”œâ”€ Span 2: Workflow Start â†’ N8N Engine (5ms) âœ…
â”œâ”€ Span 3: Database Query â†’ PostgreSQL (8500ms) âŒ PROBLEM HIER!
â””â”€ Span 4: External API Call â†’ Webhook (50ms) âœ…

Problem gefunden: Database Query dauert 8.5 Sekunden! ğŸ¯
```

### The Three Pillars of Observability

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ THE THREE PILLARS (Together = Full Visibility!)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š METRICS (Prometheus)
â”œâ”€ Was: CPU 80%, Requests/sec 100, Latency 500ms
â”œâ”€ Frage: "WAS passiert?"
â””â”€ Beispiel: CPU ist hoch! Aber warum?

ğŸ“ LOGS (Loki)
â”œâ”€ Was: "Database connection timeout after 30s"
â”œâ”€ Frage: "WELCHER Error?"
â””â”€ Beispiel: Timeout Error! Aber wo im Request?

ğŸ” TRACES (Tempo)
â”œâ”€ Was: Request Flow von Frontend â†’ Backend â†’ DB
â”œâ”€ Frage: "WO ist das Problem?"
â””â”€ Beispiel: DB Query dauert 8.5s! Problem gefunden! ğŸ‰
```

**Zusammen:**
```
1. Metrics sagen: "Request Latency ist hoch!" (500ms avg)
2. Logs sagen: "Database timeout errors!"
3. Traces sagen: "Diese spezifische DB Query braucht 8.5s" â†’ FIX IT!
```

### Core Concepts

#### 1. **Trace** - Ein kompletter Request-Flow

Trace = Die gesamte Reise eines Requests

**Beispiel: N8N Workflow Execution**
```
Trace ID: abc-123-def
â”œâ”€ Span 1: HTTP POST /workflow/execute (10ms)
â”œâ”€ Span 2: Load Workflow from DB (100ms)
â”œâ”€ Span 3: Execute Node 1 (API Call) (200ms)
â”œâ”€ Span 4: Execute Node 2 (Transform) (5ms)
â””â”€ Span 5: Save Result to DB (50ms)

Total Duration: 365ms
```

#### 2. **Span** - Ein einzelner Schritt

Span = Ein Teil des Requests (z.B. eine Function, ein API Call)

**Span Attributes:**
```yaml
Span ID: span-1
Parent ID: null  # Root span (kein Parent)
Service: n8n-prod
Operation: POST /workflow/execute
Duration: 10ms
Status: OK
Tags:
  - http.method: POST
  - http.status_code: 200
  - workflow.id: workflow-123
```

#### 3. **Service Graph** - Wer spricht mit wem?

Service Graph = Visualisierung der Microservice-Kommunikation

**Beispiel:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SERVICE GRAPH (Auto-Generated from Traces!)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Browser
    â†“ (100 req/sec)
N8N Frontend
    â†“ (100 req/sec)
N8N Backend
    â”œâ”€ â†’ PostgreSQL (80 req/sec, 50ms avg latency)
    â””â”€ â†’ Redis (20 req/sec, 2ms avg latency)
```

#### 4. **Trace-to-Logs Correlation** - Das Killer-Feature!

**Workflow:**
```
Step 1: User sieht Error in N8N
Step 2: Ã–ffnet Grafana â†’ Tempo â†’ Search for N8N traces
Step 3: Findet Trace mit Error (rot markiert!)
Step 4: Klickt auf Trace â†’ "View Logs for this Span"
Step 5: Grafana springt automatisch zu Loki Logs! ğŸ‰
Step 6: Sieht exakten Error-Log fÃ¼r diesen Request!
```

**Wie funktioniert das?**
- Tempo injiziert **Trace ID** in Logs
- Loki speichert Logs mit **Trace ID**
- Grafana verlinkt beide! â†’ Click = Jump to Logs! ğŸš€

### Tempo vs Jaeger

**Frage:** Warum haben wir BEIDE (Tempo + Jaeger)?

**Antwort:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMPO = Storage Backend ğŸ“¦                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… S3 backend (Ceph RGW) = Unlimited storage                   â”‚
â”‚ âœ… 30-day retention                                            â”‚
â”‚ âœ… Multi-protocol (OTLP, Jaeger, Zipkin)                       â”‚
â”‚ âœ… Trace-to-logs correlation                                   â”‚
â”‚ âŒ Basic UI (Grafana Explore)                                  â”‚
â”‚                                                                 â”‚
â”‚ Use Case: "Datenbank fÃ¼r Traces"                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JAEGER = Frontend UI ğŸ–¥ï¸                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Professional Trace UI                                       â”‚
â”‚ âœ… Service Dependency Graph                                    â”‚
â”‚ âœ… Trace Comparison                                            â”‚
â”‚ âœ… Advanced Search                                             â”‚
â”‚ âœ… Uses Tempo as Backend! (grpc-plugin)                        â”‚
â”‚ âŒ Kein eigener Storage                                        â”‚
â”‚                                                                 â”‚
â”‚ Use Case: "Professionelle UI fÃ¼r Trace-Analyse"                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fazit:** Tempo = Storage, Jaeger = UI! Beide nutzen! ğŸ¯

**Architecture:**
```
Apps (N8N, etc.)
    â†“ OTLP Traces
Tempo Distributor (Port 4317/4318)
    â†“
Tempo Ingester
    â†“
Ceph S3 (Storage)
    â†‘
    â”œâ”€ Grafana Explore (Basic UI)
    â””â”€ Jaeger UI (Professional UI)
```

### Welche Apps sollte ich tracen?

#### âœ… **PRODUCTION CRITICAL (100% Sampling):**

**1. N8N Production** (`n8n-prod`)
```yaml
Why:
  - Business-critical workflows
  - Complex multi-step operations
  - External API integrations
  - Database queries

What to trace:
  - Workflow execution time
  - Webhook reception
  - API calls to external services
  - Database queries
  - Node execution duration
```

**2. Kubernetes Infrastructure**
```yaml
What:
  - API Server requests (kube-apiserver)
  - etcd operations
  - kubelet operations

Why:
  - Find control plane bottlenecks
  - Debug slow kubectl commands
  - Monitor API request latency
```

#### ğŸ”„ **OPTIONAL (10% Sampling):**

**1. Audiobookshelf-dev**
```yaml
Why:
  - Development environment (low traffic)
  - Media streaming debugging

Sampling: 10% (every 10th request)
```

**2. N8N Dev**
```yaml
Why:
  - Workflow development
  - Test complex workflows

Sampling: 100% (dev environment, low traffic)
```

#### âŒ **NOT RECOMMENDED:**

```
âŒ Grafana - Monitoring the monitoring = too meta
âŒ Prometheus - Already has metrics, doesn't need traces
âŒ Loki - Log aggregator, no user traffic
âŒ Cert-Manager - Async certificate operations
âŒ ArgoCD - GitOps sync, not request-based
```

---

## Was ist Thanos?

### Definition (IKEA-Style)

**Thanos** = **Unbegrenzter Speicher** fÃ¼r Prometheus Metrics â™¾ï¸

- Speichert Metrics in **S3** (Ceph Object Storage)
- **Unlimited Retention** (fÃ¼r immer!)
- **Deduplication** (Duplikate entfernen)
- **Downsampling** (Alte Metrics komprimieren)
- **Query Frontend** (Schnelle Queries Ã¼ber S3)

### Warum Thanos?

**Problem ohne Thanos:**
```
Prometheus speichert lokal (PVC):
â”œâ”€ 30 Tage Retention
â”œâ”€ 100 GB Disk
â””â”€ Nach 30 Tagen: Metrics gelÃ¶scht ğŸ’¥

Frage: "Wie war die CPU Usage vor 3 Monaten?"
Antwort: "Keine Ahnung, Daten weg!" ğŸ˜°
```

**LÃ¶sung mit Thanos:**
```
Prometheus â†’ Thanos Sidecar â†’ Thanos Store â†’ Ceph S3
â”œâ”€ 30 Tage lokal (schnell)
â””â”€ âˆ Tage in S3 (langsam aber unbegrenzt!)

Frage: "Wie war die CPU Usage vor 3 Monaten?"
Antwort: "Hier, aus S3!" ğŸ‰
```

### Thanos Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ THANOS ARCHITECTURE (IKEA-Style)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus Pod                                                 â”‚
â”‚ â”œâ”€ Prometheus (main container)                                â”‚
â”‚ â”‚  â””â”€ TSDB: /data (PVC, 30 days)                              â”‚
â”‚ â””â”€ Thanos Sidecar (sidecar container)                         â”‚
â”‚    â””â”€ Uploads TSDB blocks to S3 every 2 hours                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“ Upload
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ceph S3 (Object Storage)                                       â”‚
â”‚ â””â”€ Bucket: thanos                                              â”‚
â”‚    â”œâ”€ 2025-01/ (January metrics, compressed)                  â”‚
â”‚    â”œâ”€ 2025-02/ (February metrics, compressed)                 â”‚
â”‚    â””â”€ 2025-03/ (March metrics, compressed)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†‘ Read
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Thanos Query (Deployment)                                      â”‚
â”‚ â””â”€ Queries both:                                               â”‚
â”‚    â”œâ”€ Prometheus (last 30 days, fast)                         â”‚
â”‚    â””â”€ S3 (older data, slow but available!)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†‘ Query
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana                                                         â”‚
â”‚ â””â”€ Datasource: Thanos Query (not Prometheus!)                 â”‚
â”‚    â†’ Can query data from 3 months ago! ğŸ‰                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Thanos Benefits

| Feature | Without Thanos | With Thanos |
|---------|----------------|-------------|
| **Retention** | 30 days | âˆ (Unlimited) |
| **Storage** | 100 GB (PVC) | Unlimited (S3) |
| **Cost** | Expensive (SSD) | Cheap (Object Storage) |
| **Query Speed** | Fast | Fast (30d) + Slow (older) |
| **HA** | âŒ Single Prometheus | âœ… Multiple Prometheus (deduplicated) |
| **Backup** | Manual | Auto (S3 versioning) |

---

## Unser Production Setup

### Infrastructure Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KUBERNETES CLUSTER (Talos 1.10.6)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ctrl-0           â”‚  â”‚ worker-1         â”‚  â”‚ worker-2         â”‚
â”‚ (Control Plane)  â”‚  â”‚ (Worker)         â”‚  â”‚ (Worker)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Prometheus       â”‚  â”‚ App Pods         â”‚  â”‚ App Pods         â”‚
â”‚ Grafana          â”‚  â”‚ Promtail         â”‚  â”‚ Promtail         â”‚
â”‚ Loki             â”‚  â”‚ Node Exporter    â”‚  â”‚ Node Exporter    â”‚
â”‚ Alertmanager     â”‚  â”‚                  â”‚  â”‚                  â”‚
â”‚ Thanos Query     â”‚  â”‚                  â”‚  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployed Components

**Namespace: `monitoring`**

```yaml
Prometheus (StatefulSet):
  - Replicas: 1
  - Storage: 100 GB (Ceph RBD)
  - Retention: 30 days
  - Scrape Interval: 15s

Grafana (Deployment):
  - Replicas: 1
  - Dashboards: 68 (as CRDs)
  - Datasources: 4 (Prometheus, Loki, Alertmanager, Jaeger)

Loki (StatefulSet):
  - Replicas: 1
  - Storage: 50 GB (Ceph RBD)
  - Retention: 7 days

Tempo (Distributed Architecture):
  - Distributor (Deployment): Replicas 1, receives traces (OTLP/Jaeger)
  - Ingester (StatefulSet): Replicas 1, buffers spans, 10 GB storage
  - Compactor (Deployment): Replicas 1, uploads to S3
  - Querier (Deployment): Replicas 1, queries Ingester + S3
  - Query Frontend (Deployment): Replicas 1, caching + optimization
  - Metrics Generator: Generates span metrics for Prometheus
  - S3 Backend: Ceph RGW (tempo-traces bucket)
  - Retention: 30 days
  - Protocols: OTLP gRPC (4317), OTLP HTTP (4318), Jaeger gRPC (14250)

Jaeger (Deployment):
  - Replicas: 1
  - Backend: Tempo (via grpc-plugin)
  - UI: http://jaeger:16686
  - Use Case: Professional trace visualization

Alertmanager (StatefulSet):
  - Replicas: 1
  - Routes: Tier 0-5

Thanos Sidecar (in Prometheus Pod):
  - S3 Bucket: thanos
  - Upload Interval: 2h

Thanos Query (Deployment):
  - Replicas: 1
  - Queries: Prometheus + S3

Node Exporter (DaemonSet):
  - Pods: 6 (one per node)
  - Metrics: System (CPU, RAM, Disk)

Kube State Metrics (Deployment):
  - Replicas: 1
  - Metrics: Kubernetes Objects

Promtail (DaemonSet):
  - Pods: 6 (one per node)
  - Logs: All pod logs â†’ Loki
```

### Resource Allocation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RESOURCE USAGE (Monitoring Stack)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Prometheus:
  Requests: 2 CPU, 4Gi RAM
  Limits:   4 CPU, 8Gi RAM
  Disk:     100 GB (PVC)

Grafana:
  Requests: 500m CPU, 1Gi RAM
  Limits:   1 CPU, 2Gi RAM

Loki:
  Requests: 1 CPU, 2Gi RAM
  Limits:   2 CPU, 4Gi RAM
  Disk:     50 GB (PVC)

Node Exporter (per node):
  Requests: 100m CPU, 100Mi RAM
  Limits:   200m CPU, 200Mi RAM

Promtail (per node):
  Requests: 100m CPU, 128Mi RAM
  Limits:   200m CPU, 256Mi RAM

TOTAL CLUSTER:
  CPU:    ~10 cores
  Memory: ~20 GB
  Disk:   150 GB (Prometheus + Loki)
```

---

## Complete Architecture

### Full Stack Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMPLETE OBSERVABILITY ARCHITECTURE (IKEA-Style)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: APPLICATIONS                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    N8N      â”‚  â”‚   Kafka     â”‚  â”‚ PostgreSQL  â”‚  â”‚   Redis     â”‚
â”‚    Pods     â”‚  â”‚   Brokers   â”‚  â”‚  Clusters   â”‚  â”‚   Caches    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                â”‚                â”‚
       â”‚ /metrics       â”‚ /metrics       â”‚ /metrics       â”‚ /metrics
       â”‚ STDOUT/STDERR  â”‚ STDOUT/STDERR  â”‚ STDOUT/STDERR  â”‚ STDOUT/STDERR
       â”‚                â”‚                â”‚                â”‚
       â–¼                â–¼                â–¼                â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: COLLECTION                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus (ServiceMonitor)      â”‚  â”‚ Promtail (DaemonSet)             â”‚
â”‚ - Scrapes /metrics every 15s     â”‚  â”‚ - Reads /var/log/pods/*.log      â”‚
â”‚ - Stores in TSDB (30 days)       â”‚  â”‚ - Parses JSON logs               â”‚
â”‚ - ServiceMonitor auto-discovery  â”‚  â”‚ - Adds Kubernetes labels         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                     â”‚
             â”‚ TSDB Blocks (2h)                    â”‚ gRPC Push
             â–¼                                     â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: STORAGE                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Thanos Sidecar                   â”‚  â”‚ Loki (StatefulSet)               â”‚
â”‚ - Uploads blocks to S3 (2h)      â”‚  â”‚ - Stores logs (7 days)           â”‚
â”‚ - Bucket: thanos                 â”‚  â”‚ - Chunks on disk (PVC 50GB)      â”‚
â”‚ - Ceph Object Storage            â”‚  â”‚ - Label-based indexing           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                     â”‚
             â”‚ S3 Upload                           â”‚ LogQL Query
             â–¼                                     â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ceph S3 (Object Storage)         â”‚  â”‚ Grafana (Deployment)             â”‚
â”‚ â””â”€ thanos/                       â”‚  â”‚ - Datasource: Thanos Query       â”‚
â”‚    â”œâ”€ 2025-01/ (Jan metrics)     â”‚  â”‚ - Datasource: Loki               â”‚
â”‚    â”œâ”€ 2025-02/ (Feb metrics)     â”‚  â”‚ - 68 Dashboards (CRDs)           â”‚
â”‚    â””â”€ 2025-03/ (Mar metrics)     â”‚  â”‚ - Unlimited retention via S3!    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–²
             â”‚ Read from S3
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Thanos Query (Deployment)        â”‚
â”‚ - Queries Prometheus (30d)       â”‚
â”‚ - Queries S3 (older data)        â”‚
â”‚ - Deduplicates metrics           â”‚
â”‚ - Downsamples old data           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–²
             â”‚ PromQL Query
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana Dashboard                â”‚
â”‚ - Panel: CPU Usage (last 90d)   â”‚
â”‚ - Data: 30d from Prometheus      â”‚
â”‚         60d from S3 (Thanos)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Sequence

**METRICS FLOW (Prometheus â†’ Thanos â†’ Grafana):**

```
Step 1: App exposes /metrics
  â”œâ”€ App: N8N Pod
  â”œâ”€ Endpoint: http://n8n-prod:5678/metrics
  â””â”€ Format: Prometheus text format

Step 2: ServiceMonitor tells Prometheus to scrape
  â”œâ”€ ServiceMonitor: servicemonitor-n8n.yaml
  â”œâ”€ Selector: app=n8n
  â””â”€ Prometheus auto-discovers all N8N pods

Step 3: Prometheus scrapes every 15 seconds
  â”œâ”€ Prometheus â†’ http://n8n-prod:5678/metrics
  â”œâ”€ Stores in TSDB (local PVC)
  â””â”€ Retention: 30 days

Step 4: Thanos Sidecar uploads to S3
  â”œâ”€ Every 2 hours
  â”œâ”€ TSDB blocks â†’ Ceph S3 bucket "thanos"
  â””â”€ Infinite retention!

Step 5: User opens Grafana Dashboard
  â”œâ”€ Dashboard: "N8N Production Metrics"
  â”œâ”€ Query: rate(http_requests_total[5m])
  â””â”€ Datasource: Thanos Query (not Prometheus!)

Step 6: Thanos Query fetches data
  â”œâ”€ Last 30 days: From Prometheus (fast)
  â”œâ”€ Older data: From S3 (slow but works!)
  â””â”€ Returns combined result to Grafana

Step 7: Grafana renders chart
  â””â”€ User sees data from last 90 days! ğŸ‰
```

**LOGS FLOW (Promtail â†’ Loki â†’ Grafana):**

```
Step 1: App writes logs to STDOUT
  â”œâ”€ App: N8N Pod
  â””â”€ Log: "Workflow execution started"

Step 2: Kubernetes captures logs
  â”œâ”€ Container Runtime: containerd
  â””â”€ File: /var/log/pods/n8n-prod_n8n-0_abc123/n8n/0.log

Step 3: Promtail tails log file
  â”œâ”€ Promtail (DaemonSet) on worker node
  â”œâ”€ Reads: /var/log/pods/**/*.log
  â””â”€ Parses JSON format

Step 4: Promtail adds labels
  â”œâ”€ namespace: n8n-prod
  â”œâ”€ pod: n8n-0
  â”œâ”€ container: n8n
  â””â”€ app: n8n

Step 5: Promtail pushes to Loki
  â”œâ”€ Protocol: gRPC
  â”œâ”€ Endpoint: http://loki:3100/loki/api/v1/push
  â””â”€ Batch: Every 10 seconds

Step 6: Loki stores logs
  â”œâ”€ Index: Labels (namespace, pod, app)
  â”œâ”€ Chunks: Log content (compressed)
  â””â”€ Retention: 7 days

Step 7: User opens Grafana Explore
  â”œâ”€ Datasource: Loki
  â”œâ”€ Query: {namespace="n8n-prod"} |= "error"
  â””â”€ Grafana shows logs! ğŸ‰
```

---

## Tempo Trace Pipeline

### ğŸ” Wie funktioniert Distributed Tracing?

**IKEA-Analogie:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metrics (Prometheus) = Geschwindigkeitsmesser ğŸï¸              â”‚
â”‚ â†’ Sagt dir: "Du fÃ¤hrst 80 km/h"                               â”‚
â”‚ â†’ Aber NICHT: Wo genau bist du auf der Strecke?               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Traces (Tempo) = GPS-Navigation ğŸ›°ï¸                            â”‚
â”‚ â†’ Sagt dir: "Du bist bei Kilometer 42"                        â”‚
â”‚ â†’ UND: "Stau auf der Strecke zwischen KM 50-55!"              â”‚
â”‚ â†’ GPS fÃ¼r jeden einzelnen Request! ğŸ¯                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Complete Trace Flow (OTLP â†’ Tempo â†’ Grafana)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRACES FLOW (App â†’ Tempo â†’ Grafana/Jaeger)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: App instruments code with OTLP SDK
  â”œâ”€ App: N8N Production
  â”œâ”€ Library: @opentelemetry/sdk-node
  â””â”€ Code: SDK auto-instruments HTTP, Database, Redis calls

Step 2: App sends traces to Tempo Distributor
  â”œâ”€ Protocol: OTLP gRPC (port 4317)
  â”œâ”€ Endpoint: http://tempo-distributor.monitoring.svc:4317
  â”œâ”€ Format: Protobuf (binary, efficient)
  â””â”€ Batch: Every 5 seconds (configurable)

Step 3: Tempo Distributor validates and forwards
  â”œâ”€ Distributor receives trace
  â”œâ”€ Validates trace format
  â”œâ”€ Adds metadata (cluster, namespace)
  â””â”€ Forwards to Tempo Ingester

Step 4: Tempo Ingester buffers traces
  â”œâ”€ Ingester receives spans
  â”œâ”€ Buffers in memory (10-15 minutes)
  â”œâ”€ Groups spans into blocks
  â””â”€ Writes blocks to local disk (PVC)

Step 5: Tempo Compactor uploads to S3
  â”œâ”€ Every 2 hours: Compactor reads blocks from Ingester
  â”œâ”€ Compresses blocks (parquet format)
  â”œâ”€ Uploads to Ceph S3: s3://tempo-traces/blocks/
  â””â”€ Retention: 30 days (same as Loki)

Step 6: Metrics Generator creates metrics FROM traces
  â”œâ”€ Reads spans from Ingester
  â”œâ”€ Generates metrics:
  â”‚  â”œâ”€ request_duration_seconds{service="n8n-prod"}
  â”‚  â”œâ”€ span_duration_seconds{span="database-query"}
  â”‚  â””â”€ service_graph_request_total{client="n8n",server="postgres"}
  â””â”€ Remote-writes to Prometheus!

Step 7: User searches traces in Grafana
  â”œâ”€ Grafana â†’ Explore â†’ Tempo Datasource
  â”œâ”€ Query: {service.name="n8n-prod" && duration>1s}
  â””â”€ Tempo Query Frontend searches:
     â”œâ”€ Recent data (last 15 min): From Ingester (fast!)
     â””â”€ Older data: From S3 (slower, but works!)

Step 8: Grafana renders trace
  â”œâ”€ Shows Trace ID, Duration, Spans
  â”œâ”€ Waterfall view of spans (Timeline)
  â”œâ”€ Click "Logs for this span" â†’ Jumps to Loki! ğŸ‰
  â””â”€ Click "Metrics for this service" â†’ Jumps to Prometheus! ğŸš€
```

### Tempo Architecture (Distributed Components)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMPO DISTRIBUTED ARCHITECTURE (Production)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apps (N8N, etc.)                 â”‚
â”‚ - OTLP SDK instrumented          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ OTLP gRPC (4317)
             â”‚ OTLP HTTP (4318)
             â”‚ Jaeger gRPC (14250)
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tempo Distributor (Deployment)   â”‚
â”‚ - Validates traces               â”‚
â”‚ - Load balancing                 â”‚
â”‚ - Forwards to Ingesters          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ gRPC
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tempo Ingester (StatefulSet)     â”‚
â”‚ - Buffers spans (10-15 min)     â”‚
â”‚ - Writes blocks to PVC           â”‚
â”‚ - Replication factor: 1          â”‚
â”‚ - Storage: 10 GB (rook-ceph-rbd) â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚              â”‚
      â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Blocks                                      â”‚ Spans (live)
      â–¼                                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tempo Compactor (Deployment)     â”‚  â”‚ Metrics Generator         â”‚
â”‚ - Reads blocks from Ingester     â”‚  â”‚ - Generates span metrics  â”‚
â”‚ - Compresses to Parquet          â”‚  â”‚ - Service graphs          â”‚
â”‚ - Uploads to S3 (every 2h)       â”‚  â”‚ - Remote-write to Prom    â”‚
â”‚ - Deletes old data (30d)         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
             â”‚ S3 Upload                          â”‚ Remote Write
             â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ceph S3 (Object Storage)         â”‚  â”‚ Prometheus                â”‚
â”‚ Bucket: tempo-traces             â”‚  â”‚ - Span duration metrics   â”‚
â”‚ Format: Parquet (columnar)       â”‚  â”‚ - Service graph metrics   â”‚
â”‚ Retention: 30 days               â”‚  â”‚ - Queryable via PromQL!   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Read
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tempo Querier (Deployment)       â”‚
â”‚ - Queries Ingester (recent)      â”‚
â”‚ - Queries S3 (historical)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tempo Query Frontend (Deploy)    â”‚
â”‚ - Caches queries                 â”‚
â”‚ - Optimizes searches             â”‚
â”‚ - API: http://tempo:3200         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana Explore                  â”‚
â”‚ - Tempo Datasource               â”‚
â”‚ - TraceQL queries                â”‚
â”‚ - Trace-to-logs correlation      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tempo vs Prometheus Data Flow (Vergleich)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROMETHEUS (Metrics) - PULL-based                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Prometheus â”€â”€â”€ scrapes every 15s â”€â”€â†’ App /metrics endpoint
    â†“
Stores in TSDB (local PVC)
    â†“
Thanos uploads to S3 (every 2h)
    â†“
Grafana queries via PromQL

Advantage: Simple (no client libraries needed)
Disadvantage: High-cardinality data = OOM! ğŸ’¥


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMPO (Traces) - PUSH-based                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

App (OTLP SDK) â”€â”€â”€ pushes â”€â”€â†’ Tempo Distributor (4317)
    â†“
Tempo Ingester buffers (10-15 min)
    â†“
Compactor uploads to S3 (every 2h)
    â†“
Grafana queries via TraceQL

Advantage: High-cardinality OK (millions of trace IDs!)
Disadvantage: Requires OTLP SDK in app
```

### How to Instrument N8N for Tracing

#### Step 1: Add OpenTelemetry to N8N (Custom Image)

**Dockerfile:**
```dockerfile
FROM n8nio/n8n:latest

# Install OpenTelemetry SDK
USER root
RUN npm install -g \
    @opentelemetry/sdk-node@^0.52.0 \
    @opentelemetry/auto-instrumentations-node@^0.47.0 \
    @opentelemetry/exporter-trace-otlp-grpc@^0.52.0

# Tracing config
COPY tracing.js /app/tracing.js

USER node
ENV NODE_OPTIONS="--require /app/tracing.js"
```

**tracing.js:**
```javascript
const { NodeSDK } = require('@opentelemetry/sdk-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-grpc');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');

const sdk = new NodeSDK({
  serviceName: 'n8n-prod',
  traceExporter: new OTLPTraceExporter({
    url: 'http://tempo-distributor.monitoring.svc:4317',
  }),
  instrumentations: [getNodeAutoInstrumentations()],
});

sdk.start();
console.log('ğŸ” OpenTelemetry Tracing started for n8n-prod');
```

#### Step 2: Update N8N Deployment

```yaml
# kubernetes/applications/n8n/prod/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: n8n
  namespace: n8n-prod
spec:
  template:
    spec:
      containers:
      - name: n8n
        image: your-registry/n8n:tracing  # Custom image!
        env:
        # OTLP Environment Variables
        - name: OTEL_SERVICE_NAME
          value: "n8n-prod"
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://tempo-distributor.monitoring.svc:4317"
        - name: OTEL_TRACES_SAMPLER
          value: "parentbased_traceidratio"
        - name: OTEL_TRACES_SAMPLER_ARG
          value: "1.0"  # 100% sampling (production critical!)

        # Existing N8N env vars
        - name: N8N_HOST
          value: "n8n.homelab.local"
        # ... other vars
```

#### Step 3: Deploy and Verify

```bash
# Build custom N8N image with OTLP
docker build -t your-registry/n8n:tracing .
docker push your-registry/n8n:tracing

# Deploy updated N8N
kubectl apply -f kubernetes/applications/n8n/prod/

# Wait for N8N to be ready
kubectl rollout status deployment/n8n -n n8n-prod

# Check if traces are being sent (look for OTLP logs)
kubectl logs -n n8n-prod deployment/n8n | grep -i "telemetry\|tracing"

# Verify in Tempo
kubectl exec -n grafana deployment/grafana-deployment -- \
  curl -s "http://tempo-query-frontend.monitoring.svc:3200/api/search?tags=service.name%3Dn8n-prod&limit=10"
```

**Expected Output:** JSON with Trace IDs! ğŸ‰

#### Step 4: Query Traces in Grafana

```
1. Open Grafana: http://grafana.homelab.local:3000

2. Go to Explore â†’ Select "Tempo" datasource

3. Query Types:

   A. Search by Service:
      {service.name="n8n-prod"}

   B. Find slow traces (> 1 second):
      {service.name="n8n-prod" && duration>1s}

   C. Find error traces:
      {service.name="n8n-prod" && status=error}

   D. Find specific workflow:
      {service.name="n8n-prod" && workflow.id="workflow-123"}

4. Click on a trace â†’ Waterfall view shows:
   â”œâ”€ All spans (HTTP, DB, Redis, etc.)
   â”œâ”€ Duration of each span
   â””â”€ Click "Logs for this span" â†’ Jumps to Loki! ğŸš€
```

### Sampling Strategies (Production Best Practices)

#### Strategy 1: 100% Sampling (Production Critical Apps)

**Use for:** N8N Production, Payment APIs, Critical Workflows

```yaml
env:
- name: OTEL_TRACES_SAMPLER
  value: "parentbased_traceidratio"
- name: OTEL_TRACES_SAMPLER_ARG
  value: "1.0"  # 100% = Every request is traced
```

**Why:**
- Business-critical â†’ Need to see EVERY request
- Low traffic (< 1000 req/sec) â†’ Storage OK
- Compliance/Audit â†’ Need complete traces

#### Strategy 2: 10% Sampling (High-Traffic Apps)

**Use for:** Audiobookshelf-dev, High-traffic APIs

```yaml
env:
- name: OTEL_TRACES_SAMPLER
  value: "parentbased_traceidratio"
- name: OTEL_TRACES_SAMPLER_ARG
  value: "0.1"  # 10% = Every 10th request
```

**Why:**
- High traffic â†’ 100% = Too much data
- Representative sample sufficient
- Saves storage costs

#### Strategy 3: Tail-based Sampling (Advanced)

**Only trace ERRORS or SLOW requests:**

```yaml
# tempo/values.yaml
tempo:
  overrides:
    defaults:
      metrics_generator:
        processors:
          - service-graphs
          - span-metrics
        filter_policies:
          - name: error-and-slow-traces
            spans_per_second: 100
            policies:
              - name: error-traces
                type: string_attribute
                key: error
                value: "true"
              - name: slow-traces
                type: numeric_attribute
                key: duration_ms
                min_value: 1000  # > 1 second
```

**Why:**
- Best of both worlds: Full coverage for problems
- Low storage: Only errors + slow traces saved
- Production-ready: Used by Uber, Netflix

---

## ServiceMonitor â†’ Dashboard (No Data Fix!)

### ğŸ¯ Das wichtigste Kapitel!

**Problem:** Du hast ein Dashboard, aber es zeigt **"No Data"** ğŸ˜±

**LÃ¶sung:** Schritt-fÃ¼r-Schritt Fix! (IKEA-Style)

### IKEA-Anleitung: Von Service zu Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCHRITT 1: Service muss /metrics Endpoint haben                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Deine App muss Metrics exposen!

Beispiel: N8N
â”œâ”€ URL: http://n8n-prod:5678/metrics
â””â”€ Test: curl http://n8n-prod:5678/metrics

Output:
  # HELP http_requests_total Total HTTP requests
  # TYPE http_requests_total counter
  http_requests_total{method="GET",status="200"} 1000

âœ… Wenn du Metrics siehst â†’ Weiter zu Schritt 2
âŒ Wenn "404 Not Found" â†’ Deine App muss erst Metrics exposen!
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCHRITT 2: Service muss Port mit Name "metrics" haben          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Dein Kubernetes Service braucht einen Port mit Name "metrics"!

âŒ FALSCH (kein Port-Name):
```yaml
apiVersion: v1
kind: Service
metadata:
  name: n8n-prod
spec:
  selector:
    app: n8n
  ports:
  - port: 5678        # âŒ Kein Name!
    targetPort: 5678
```

âœ… RICHTIG (mit Port-Name):
```yaml
apiVersion: v1
kind: Service
metadata:
  name: n8n-prod
  labels:             # âœ… Labels fÃ¼r ServiceMonitor!
    app: n8n
spec:
  selector:
    app: n8n
  ports:
  - name: http        # âœ… Name!
    port: 5678
    targetPort: 5678
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCHRITT 3: ServiceMonitor erstellen                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Jetzt sagst du Prometheus: "Scrape alle Services mit Label app=n8n"

Datei: servicemonitor-n8n.yaml
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: n8n-prod
  namespace: monitoring        # âœ… ServiceMonitor IMMER in "monitoring"!
  labels:
    app: n8n
spec:
  # WELCHE Namespaces?
  namespaceSelector:
    matchNames:
    - n8n-prod                 # âœ… Nur n8n-prod namespace

  # WELCHE Services (via Labels)?
  selector:
    matchLabels:
      app: n8n                 # âœ… Service muss Label "app: n8n" haben!

  # WO ist /metrics?
  endpoints:
  - port: http                 # âœ… Port name aus Service!
    path: /metrics             # âœ… URL path (Standard: /metrics)
    interval: 30s              # âœ… Scrape alle 30 Sekunden
```

Apply it:
```bash
kubectl apply -f servicemonitor-n8n.yaml
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCHRITT 4: Check ob Prometheus target findet                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Gehe zu Prometheus UI:
  kubectl port-forward -n monitoring svc/prometheus-operated 9090:9090
  http://localhost:9090/targets

Suche nach "n8n":
  âœ… State: UP â†’ Prometheus scraped erfolgreich!
  âŒ State: DOWN â†’ Siehe "Troubleshooting" unten

```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCHRITT 5: Check ob Metrics in Prometheus sind                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Gehe zu Prometheus â†’ Graph Tab

Query:
  http_requests_total{namespace="n8n-prod"}

Result:
  âœ… Metrics shown â†’ Prometheus hat Daten!
  âŒ "No data" â†’ ServiceMonitor stimmt nicht

```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCHRITT 6: Dashboard mit richtiger Query                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Jetzt erstelle Dashboard Panel mit korrekter PromQL Query

âŒ FALSCH (kein Namespace Filter):
```promql
rate(http_requests_total[5m])
# â†’ Zeigt ALLE Apps (N8N + Kafka + Redis + ...)
```

âœ… RICHTIG (mit Namespace Filter):
```promql
rate(http_requests_total{namespace="n8n-prod"}[5m])
# â†’ Zeigt nur N8N!
```

Panel Config in Grafana Dashboard:
```yaml
panels:
- title: "N8N Request Rate"
  targets:
  - expr: 'rate(http_requests_total{namespace="n8n-prod"}[5m])'
    datasource: Prometheus
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCHRITT 7: Refresh Grafana Dashboard                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Open Dashboard â†’ Refresh â†’ Data appears! ğŸ‰

```

### HÃ¤ufige "No Data" Probleme

#### Problem 1: ServiceMonitor im falschen Namespace

âŒ **FALSCH:**
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: n8n-prod
  namespace: n8n-prod  # âŒ FALSCH! ServiceMonitor sollte in "monitoring" sein!
```

âœ… **RICHTIG:**
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: n8n-prod
  namespace: monitoring  # âœ… RICHTIG!
spec:
  namespaceSelector:
    matchNames:
    - n8n-prod  # â† Hier sagst du welche Namespaces gescraped werden
```

#### Problem 2: Falsche Labels

Service hat:
```yaml
labels:
  app.kubernetes.io/name: n8n  # â† Dieses Label
```

ServiceMonitor sucht:
```yaml
selector:
  matchLabels:
    app: n8n  # â† Aber sucht nach diesem Label! âŒ MISMATCH!
```

**Fix:**
```yaml
selector:
  matchLabels:
    app.kubernetes.io/name: n8n  # âœ… Muss Ã¼bereinstimmen!
```

#### Problem 3: Port Name stimmt nicht

Service:
```yaml
ports:
- name: web  # â† Port heiÃŸt "web"
  port: 5678
```

ServiceMonitor:
```yaml
endpoints:
- port: metrics  # â† Aber sucht nach "metrics"! âŒ MISMATCH!
```

**Fix:**
```yaml
endpoints:
- port: web  # âœ… Muss Ã¼bereinstimmen!
  path: /metrics
```

#### Problem 4: Falsche PromQL Query

```promql
# âŒ FALSCH: Sucht nach Metric die es nicht gibt
my_custom_metric_that_doesnt_exist

# âœ… RICHTIG: Check erst ob Metric existiert
# Gehe zu Prometheus â†’ Graph â†’ Type "n8n" â†’ Auto-complete zeigt verfÃ¼gbare Metrics
```

### ServiceMonitor Troubleshooting Checklist

**IKEA-Checklist** (Von oben nach unten abarbeiten):

```
â˜ Step 1: App exposes /metrics?
  â†’ Test: curl http://service:port/metrics

â˜ Step 2: Service has port name?
  â†’ kubectl get svc n8n-prod -o yaml | grep "name:"

â˜ Step 3: Service has correct labels?
  â†’ kubectl get svc n8n-prod -o yaml | grep "labels:" -A 5

â˜ Step 4: ServiceMonitor in "monitoring" namespace?
  â†’ kubectl get servicemonitor -n monitoring

â˜ Step 5: ServiceMonitor labels match Service labels?
  â†’ Compare spec.selector.matchLabels

â˜ Step 6: Prometheus Target UP?
  â†’ http://localhost:9090/targets

â˜ Step 7: Metrics in Prometheus?
  â†’ Query: {namespace="n8n-prod"}

â˜ Step 8: Dashboard PromQL correct?
  â†’ Test query in Prometheus first
```

---

## Grafana Dashboards Deep Dive

### GrafanaDashboard CRD Structure

```yaml
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDashboard
metadata:
  name: n8n-production-metrics
  namespace: grafana
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/component: dashboard
spec:
  # WICHTIG: allowCrossNamespaceImport = true
  # â†’ Dashboard kann von Grafana in anderem Namespace geladen werden
  allowCrossNamespaceImport: true

  # Folder in Grafana UI
  folder: "Applications"

  # Grafana Instance Selector
  instanceSelector:
    matchLabels:
      app: grafana

  # Dashboard JSON (compressed)
  json: |
    {"title":"N8N Production","panels":[...]}
```

### Dashboard Organization Strategy

**Unsere 68 Dashboards:**

```
Grafana UI
â”œâ”€ Tier 0 Executive (2 dashboards)
â”‚  â”œâ”€ Kubernetes Global View
â”‚  â””â”€ Node System Overview
â”‚
â”œâ”€ ArgoCD (5 dashboards)
â”‚  â”œâ”€ ArgoCD GitOps
â”‚  â”œâ”€ ArgoCD Operational
â”‚  â”œâ”€ ArgoCD Application
â”‚  â”œâ”€ ArgoCD Notifications
â”‚  â””â”€ ArgoCD Overview v3
â”‚
â”œâ”€ Ceph Storage (4 dashboards)
â”‚  â”œâ”€ Rook Ceph Storage
â”‚  â”œâ”€ Ceph Cluster
â”‚  â”œâ”€ Ceph Pools
â”‚  â””â”€ Ceph OSD
â”‚
â”œâ”€ Kubernetes (11 dashboards)
â”‚  â”œâ”€ API Server
â”‚  â”œâ”€ CoreDNS
â”‚  â”œâ”€ Scheduler
â”‚  â”œâ”€ Controller Manager
â”‚  â”œâ”€ etcd
â”‚  â”œâ”€ Global View
â”‚  â”œâ”€ Namespaces View
â”‚  â”œâ”€ Nodes View
â”‚  â”œâ”€ Pods View
â”‚  â”œâ”€ Persistent Volumes
â”‚  â””â”€ State Metrics v2
â”‚
â””â”€ ... (53 more dashboards)
```

### Dashboard Import vs CRD

**Warum CRD besser ist:**

| Feature | Dashboard Import (UI) | GrafanaDashboard CRD |
|---------|----------------------|---------------------|
| **Method** | Click "Import" in UI | `kubectl apply -f` |
| **Storage** | Grafana Database | Git Repository |
| **Versioning** | âŒ Nur in Grafana DB | âœ… Git History |
| **Backup** | Manual DB Export | Git Commit |
| **Restore** | Manual Re-Import | `kubectl apply -f` |
| **GitOps** | âŒ Nein | âœ… ArgoCD Auto-Sync |
| **Validation** | âŒ Nein | âœ… Kubernetes API |
| **Team Sharing** | Export JSON, Email | Git Push |
| **CI/CD** | âŒ Schwer | âœ… Easy |

**Winner:** GrafanaDashboard CRD ğŸ†

---

## Backup & Disaster Recovery

### ğŸ¯ Velero Backup - Backup ALLES!

**Was kann Velero backupen?**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VELERO KANN BACKUPEN:                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Grafana Dashboards (als GrafanaDashboard CRDs!)             â”‚
â”‚ âœ… Prometheus TSDB (PersistentVolumeClaim + Data)              â”‚
â”‚ âœ… Loki Chunks (PersistentVolumeClaim + Data)                  â”‚
â”‚ âœ… ServiceMonitors (alle ServiceMonitor CRDs)                  â”‚
â”‚ âœ… PrometheusRules (alle Alert Rules)                          â”‚
â”‚ âœ… ConfigMaps (Grafana Datasources, Alertmanager Config)       â”‚
â”‚ âœ… Secrets (Grafana Admin Password, etc.)                      â”‚
â”‚ âœ… Thanos Config (S3 credentials)                              â”‚
â”‚ âœ… ALLES in Kubernetes! ğŸš€                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Backup Strategy

#### Daily Backup (Automatisch via Velero Schedule)

```yaml
# velero-schedule-monitoring-daily.yaml
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: monitoring-daily
  namespace: velero
spec:
  schedule: "0 2 * * *"  # Jeden Tag um 2:00 AM
  template:
    includedNamespaces:
    - monitoring
    - grafana
    includeClusterResources: true
    storageLocation: default
    volumeSnapshotLocations:
    - default
    ttl: 720h  # Keep for 30 days
```

**Apply:**
```bash
kubectl apply -f velero-schedule-monitoring-daily.yaml
```

**Was wird gebackuped:**
- Prometheus PVC (100 GB TSDB data)
- Loki PVC (50 GB logs)
- Alle Grafana Dashboards (68 CRDs)
- Alle ServiceMonitors
- Alle PrometheusRules
- ConfigMaps & Secrets

#### Manual Backup (On-Demand)

```bash
# Backup kompletter Monitoring Stack
velero backup create monitoring-stack-$(date +%Y%m%d-%H%M%S) \
  --include-namespaces monitoring,grafana \
  --include-cluster-resources \
  --storage-location default

# Backup nur Grafana Dashboards (schnell!)
velero backup create grafana-dashboards-$(date +%Y%m%d-%H%M%S) \
  --include-namespaces grafana \
  --include-resources grafanadashboards

# Backup nur ServiceMonitors + Rules
velero backup create prometheus-configs-$(date +%Y%m%d-%H%M%S) \
  --include-namespaces monitoring \
  --include-resources servicemonitors,prometheusrules
```

### Restore Procedure

#### Disaster Recovery (Kompletter Cluster Restore)

**Scenario:** Dein Cluster ist komplett weg! ğŸ’¥

**IKEA-Style Recovery:**

```
SCHRITT 1: Neuer Kubernetes Cluster
  â”œâ”€ Fresh Talos install
  â””â”€ kubectl get nodes â†’ All Ready

SCHRITT 2: Velero installieren
  helm install velero vmware-tanzu/velero \
    --namespace velero \
    --set configuration.backupStorageLocation.bucket=velero \
    --set configuration.backupStorageLocation.config.s3Url=http://rook-ceph-rgw.rook-ceph.svc

SCHRITT 3: Check Backups
  velero backup get
  # Output: monitoring-daily-20250120-020000

SCHRITT 4: Restore Monitoring Stack
  velero restore create --from-backup monitoring-daily-20250120-020000

SCHRITT 5: Wait for Restore
  kubectl get pods -n monitoring --watch
  # Warte bis alle Pods Running

SCHRITT 6: Check Grafana
  kubectl port-forward -n grafana svc/grafana 3000:3000
  http://localhost:3000
  # â†’ Alle 68 Dashboards sind wieder da! ğŸ‰

SCHRITT 7: Check Prometheus
  http://localhost:9090
  # â†’ Alle Metrics sind wieder da!
  # â†’ Historical data from PVC restored!
```

#### Partial Restore (Nur Dashboards)

**Scenario:** Du hast versehentlich ein Dashboard gelÃ¶scht

```bash
# Restore nur Grafana Dashboards
velero restore create grafana-dashboards-restore \
  --from-backup grafana-dashboards-20250120-020000 \
  --include-namespaces grafana \
  --include-resources grafanadashboards

# Check restore
kubectl get grafanadashboards -n grafana
```

### Git Backup (ZusÃ¤tzlich!)

**Best Practice:** Doppelte Absicherung!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BACKUP STRATEGY (2-fach)                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Velero Backup (automatisch)                                â”‚
â”‚    â””â”€ Speichert PVCs + CRDs in Ceph S3                        â”‚
â”‚                                                                â”‚
â”‚ 2. Git Backup (automatisch via ArgoCD)                        â”‚
â”‚    â””â”€ Alle YAMLs in Git Repository                            â”‚
â”‚                                                                â”‚
â”‚ Vorteil: Wenn Velero kaputt ist â†’ Git restore!                â”‚
â”‚          Wenn Git kaputt ist â†’ Velero restore!                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Git Backup Struktur:**
```
kubernetes/infrastructure/monitoring/
â”œâ”€ grafana/
â”‚  â”œâ”€ kustomization.yaml
â”‚  â”œâ”€ grafana.yaml
â”‚  â””â”€ enterprise-dashboards/
â”‚     â”œâ”€ argocd/
â”‚     â”‚  â”œâ”€ argocd-gitops.yaml
â”‚     â”‚  â””â”€ ... (68 dashboards)
â”‚     â””â”€ ...
â”‚
â”œâ”€ servicemonitors/
â”‚  â”œâ”€ servicemonitor-n8n.yaml
â”‚  â”œâ”€ servicemonitor-kafka.yaml
â”‚  â””â”€ ... (alle ServiceMonitors)
â”‚
â””â”€ alertmanager/
   â”œâ”€ alertmanagerconfig-tier0.yaml
   â””â”€ ... (alle Alert Configs)
```

**Restore from Git:**
```bash
# Clone repo
git clone https://github.com/Tim275/talos-homelab.git

# Apply all monitoring YAMLs
kubectl apply -k kubernetes/infrastructure/monitoring/

# Fertig! Alles wieder da! ğŸ‰
```

---

## Quick Reference

### Essential Commands

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROMETHEUS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Port-forward to Prometheus UI
kubectl port-forward -n monitoring svc/prometheus-operated 9090:9090
# â†’ http://localhost:9090

# Check Prometheus Targets
curl http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | {job, health}'

# Check ServiceMonitors
kubectl get servicemonitors -A

# Check PrometheusRules
kubectl get prometheusrules -A

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GRAFANA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Port-forward to Grafana UI
kubectl port-forward -n grafana svc/grafana 3000:3000
# â†’ http://localhost:3000

# List all Grafana Dashboards
kubectl get grafanadashboards -n grafana

# Get Grafana admin password
kubectl get secret -n grafana grafana-admin-credentials \
  -o jsonpath='{.data.GF_SECURITY_ADMIN_PASSWORD}' | base64 -d

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOKI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Port-forward to Loki
kubectl port-forward -n monitoring svc/loki 3100:3100

# Query Loki logs (via HTTP)
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query={namespace="n8n-prod"}' | jq .

# Check Loki health
curl http://localhost:3100/ready

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEMPO (DISTRIBUTED TRACING)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Port-forward to Tempo Query Frontend
kubectl port-forward -n monitoring svc/tempo-query-frontend 3200:3200
# â†’ http://localhost:3200

# Check Tempo health
curl http://localhost:3200/ready

# Search traces by service name (via HTTP API)
curl -s "http://localhost:3200/api/search?tags=service.name%3Dn8n-prod&limit=10" | jq .

# Get specific trace by ID
curl -s "http://localhost:3200/api/traces/<trace-id>" | jq .

# Check Tempo metrics (span metrics generated by Metrics Generator)
curl http://localhost:3200/metrics | grep tempo_

# Check all Tempo pods
kubectl get pods -n monitoring -l app.kubernetes.io/name=tempo

# Check Tempo Distributor (receives traces)
kubectl logs -n monitoring deployment/tempo-distributor --tail=50

# Check Tempo Ingester (buffers traces)
kubectl logs -n monitoring statefulset/tempo-ingester --tail=50

# Check Tempo Compactor (uploads to S3)
kubectl logs -n monitoring deployment/tempo-compactor --tail=50

# Check Tempo S3 bucket (Ceph)
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- \
  radosgw-admin bucket stats --bucket=tempo-traces

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# JAEGER UI (Uses Tempo as Backend)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Port-forward to Jaeger UI
kubectl port-forward -n monitoring svc/jaeger-query 16686:16686
# â†’ http://localhost:16686

# Check Jaeger health
curl http://localhost:16686/

# Jaeger uses Tempo via gRPC plugin (grpc-plugin backend)
kubectl logs -n monitoring deployment/jaeger-query | grep -i tempo

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# THANOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Port-forward to Thanos Query
kubectl port-forward -n monitoring svc/thanos-query 9090:9090
# â†’ http://localhost:9090

# Check Thanos Store status
kubectl logs -n monitoring deployment/thanos-query | grep "store"

# List S3 buckets (Ceph)
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- \
  radosgw-admin bucket list

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VELERO BACKUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# List backups
velero backup get

# Create manual backup
velero backup create monitoring-manual-$(date +%Y%m%d)

# Describe backup
velero backup describe monitoring-daily-latest

# Restore from backup
velero restore create --from-backup monitoring-daily-latest

# Check restore status
velero restore get
```

### Useful PromQL Queries

```promql
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYSTEM METRICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# CPU Usage per Node (%)
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory Usage per Node (%)
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100

# Disk Usage per Node (%)
(1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} /
     node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"})) * 100

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KUBERNETES METRICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Pod Count per Namespace
count(kube_pod_info) by (namespace)

# Container Restarts (last 1h)
increase(kube_pod_container_status_restarts_total[1h]) > 0

# Pods not Running
kube_pod_status_phase{phase!="Running"} == 1

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# APPLICATION METRICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# HTTP Request Rate (req/sec)
rate(http_requests_total[5m])

# HTTP Error Rate (%)
rate(http_requests_total{status=~"5.."}[5m]) /
rate(http_requests_total[5m]) * 100

# HTTP Latency p95 (seconds)
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
```

### Useful LogQL Queries

```logql
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOG QUERIES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# All logs from N8N
{namespace="n8n-prod"}

# Only error logs
{namespace="n8n-prod"} |= "error"

# Logs containing "database"
{namespace="n8n-prod"} |= "database"

# Error count (last 5 min)
count_over_time({namespace="n8n-prod"} |= "error" [5m])

# Top 10 error messages
topk(10, count_over_time({level="error"}[1h]))
```

---

## Summary

### What We Built

```
âœ… Enterprise Observability Stack (100% IaC)
âœ… Grafana Operator (68 Dashboards as CRDs)
âœ… Prometheus Operator (Auto-Discovery)
âœ… ServiceMonitor Magic (No Manual Config!)
âœ… Loki Log Aggregation
âœ… Thanos Unlimited Storage (Ceph S3)
âœ… Velero Backup (Everything!)
âœ… GitOps-Ready (ArgoCD Synced)
```

### Key Benefits

| Feature | Benefit |
|---------|---------|
| **CRDs** | Git-based, Type-safe, Self-healing |
| **ServiceMonitor** | Auto-discovery, No manual config |
| **Thanos** | Unlimited retention, S3 storage |
| **Loki** | Fast log queries, Low cost |
| **Velero** | Disaster recovery in minutes |
| **GitOps** | Everything in Git, ArgoCD synced |

### Files Reference

```
kubernetes/infrastructure/monitoring/
â”œâ”€ OBSERVABILITY-MASTER-GUIDE.md          â† YOU ARE HERE
â”œâ”€ grafana/
â”‚  â”œâ”€ kustomization.yaml                  â† 68 dashboards
â”‚  â””â”€ enterprise-dashboards/
â”œâ”€ servicemonitors/
â”‚  â”œâ”€ servicemonitor-n8n.yaml             â† Example ServiceMonitor
â”‚  â””â”€ ... (all ServiceMonitors)
â”œâ”€ kube-prometheus-stack/
â”‚  â””â”€ values.yaml                         â† Prometheus config
â””â”€ velero/
   â””â”€ schedule-monitoring-daily.yaml      â† Daily backup
```

---

**Created for:** Talos Homelab Production
**Last Updated:** 2025-10-21
**Grafana Operator:** v5.19.1
**Prometheus Operator:** v0.77.0
**Loki:** v2.9.0
**Thanos:** v0.35.0
**Velero:** v1.13.0
