# üéØ Backup Priority Matrix: Was MUSS gebackuped werden?

## üìä Analyse deines Clusters

### ‚úÖ **TIER-0: KRITISCHE DATEN (MUST BACKUP!)**
**= Datenverlust = Business-Critical**

#### 1. **PostgreSQL Datenbanken** üî¥ HIGHEST PRIORITY
```yaml
n8n-prod:
  - n8n-postgres-1, n8n-postgres-2
  - Enth√§lt: Workflows, Executions, Credentials, User Accounts
  - Backup: ‚úÖ Tier-0 (alle 6h)
  - Recovery: Velero PVC Snapshot

n8n-dev:
  - n8n-postgres-1
  - Enth√§lt: Dev Workflows, Test Data
  - Backup: ‚úÖ Tier-1 (t√§glich)

keycloak:
  - keycloak-db-1
  - Enth√§lt: User Accounts, OIDC Clients, Realm Config
  - Backup: ‚úÖ Tier-0 (alle 6h)
  - Warum kritisch: Login funktioniert nicht ohne!

infisical:
  - infisical-postgres-1
  - Enth√§lt: Secrets, API Keys, Environment Variables
  - Backup: ‚úÖ Tier-0 (alle 6h)
  - Warum kritisch: Alle App-Secrets sind hier!
```

**Velero Selector:**
```yaml
labelSelector:
  matchLabels:
    cnpg.io/cluster: n8n-postgres
    cnpg.io/cluster: keycloak-db
    cnpg.io/cluster: infisical-postgres
```

---

#### 2. **Redis (mit Persistence)** üü° HIGH PRIORITY
```yaml
authelia:
  - redis-authelia-0
  - Enth√§lt: Session Data, 2FA Secrets, TOTP Seeds
  - Backup: ‚úÖ Tier-0 (alle 6h)
  - Warum kritisch: User Sessions + 2FA verloren!

n8n-prod:
  - redis-n8n-0
  - Enth√§lt: Job Queue, Workflow Execution State
  - Backup: ‚úÖ Tier-1 (t√§glich)
  - Warum kritisch: Running workflows verloren

argocd:
  - argocd-redis-ha-server-0,1,2
  - Enth√§lt: Application Sync State, Repo Cache
  - Backup: ‚ö†Ô∏è  Tier-2 (w√∂chentlich)
  - Warum weniger kritisch: ArgoCD kann re-sync from Git
```

**Velero Selector:**
```yaml
labelSelector:
  matchLabels:
    app.kubernetes.io/name: redis
  matchExpressions:
    - key: app.kubernetes.io/instance
      operator: In
      values: [authelia, n8n, argocd]
```

---

#### 3. **Elasticsearch Indices** üü† MEDIUM-HIGH PRIORITY
```yaml
elastic-system:
  - production-cluster-es-master-data-0,1,2
  - Enth√§lt: Application Logs, Audit Trails, Metrics
  - Backup: ‚úÖ Elasticsearch Snapshots ‚Üí Ceph S3
  - Snapshot Repo: ceph-s3-snapshots
  - Schedule: Daily (ILM Policy managed)

Was ist drin?
  - Vector logs (application logs)
  - Audit trails (who did what)
  - Error logs (debugging history)

Warum wichtig?
  - Compliance (DSGVO audit trail)
  - Debugging (incident investigation)
  - Aber: K√∂nnen neu generiert werden (nicht wie DB-Daten!)
```

**Backup Method:**
```bash
# Elasticsearch hat eigenes Snapshot System (nicht Velero!)
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X PUT "https://localhost:9200/_snapshot/ceph-s3-snapshots/daily-$(date +%Y%m%d)"
```

---

#### 4. **Grafana Dashboards** üü° HIGH PRIORITY
```yaml
grafana:
  - grafana-deployment-*
  - Enth√§lt: Custom Dashboards, Datasources, Alert Rules
  - Backup: ‚úÖ Tier-2 (t√§glich)
  - Warum wichtig: Dashboards neu erstellen = Stunden Arbeit!

PVC:
  - grafana-storage (SQLite DB mit Dashboard configs)
```

**Velero Selector:**
```yaml
labelSelector:
  matchLabels:
    app.kubernetes.io/name: grafana
```

---

#### 5. **LLDAP User Directory** üî¥ CRITICAL
```yaml
lldap:
  - lldap-*
  - Enth√§lt: User Accounts, Groups, LDAP Tree
  - Backup: ‚úÖ Tier-0 (alle 6h)
  - Warum kritisch: Central User Directory (SSO Basis!)

PVC:
  - lldap-data (SQLite DB mit User Data)
```

---

#### 6. **Authelia Config + Data** üî¥ CRITICAL
```yaml
authelia:
  - authelia-*
  - Enth√§lt: ACL Rules, User Sessions, 2FA Secrets
  - Backup: ‚úÖ Tier-0 (alle 6h)
  - Warum kritisch: Auth Gateway f√ºr alle Apps!
```

---

### ‚ö†Ô∏è **TIER-1: WICHTIGE DATEN (Should Backup)**
**= Datenverlust = √Ñrgerlich, aber rebuild m√∂glich**

#### 7. **Kafka Topics** üü° MEDIUM PRIORITY
```yaml
kafka:
  - my-cluster-dual-role-0,1,2
  - Enth√§lt: Persistent Topics, Messages
  - Backup: ‚ö†Ô∏è  Tier-1 (t√§glich)
  - Warum weniger kritisch: Depends on use case

Frage: Hast du wichtige Messages in Kafka?
  - Wenn nur Demo/Test ‚Üí SKIP BACKUP
  - Wenn Production Events ‚Üí TIER-0!
```

---

#### 8. **InfluxDB Metrics** üü¢ LOW PRIORITY
```yaml
influxdb:
  - influxdb-0
  - Enth√§lt: Time-Series Metrics (Historical Data)
  - Backup: ‚ö†Ô∏è  Tier-3 (w√∂chentlich)
  - Warum weniger kritisch: Metrics regenerieren sich
  - Aber: Historical Trends verloren!
```

---

#### 9. **Loki Log Storage** üü¢ LOW PRIORITY
```yaml
loki:
  - loki-0
  - Enth√§lt: Log Chunks (compressed logs)
  - Backup: ‚ö†Ô∏è  SKIP (too much data, low value)
  - Warum skip: Logs sind in Elasticsearch (besser searchable)
  - Loki = short-term buffer (7-14 Tage retention)
```

---

### ‚úÖ **TIER-2: CONFIG/STATE (Nice to Have)**
**= Verlust = Rebuild dauert 1-2 Stunden**

#### 10. **Sealed Secrets** üü° IMPORTANT
```yaml
sealed-secrets:
  - sealed-secrets-controller-*
  - Enth√§lt: Encryption Keys f√ºr Sealed Secrets
  - Backup: ‚úÖ Tier-2 (t√§glich)
  - Warum wichtig: Ohne Keys kannst du Sealed Secrets nicht decrypten!

Was backupen:
  - Secret: sealed-secrets-controller (Encryption Key)
  - Namespace: sealed-secrets
```

---

#### 11. **Cert-Manager Certificates** üü¢ LOW PRIORITY
```yaml
cert-manager:
  - cert-manager-*
  - Enth√§lt: TLS Certificates, Let's Encrypt Accounts
  - Backup: ‚ö†Ô∏è  SKIP (LetsEncrypt re-issue m√∂glich)
  - Aber: Rate Limits beachten! (5 certs/domain/week)
```

---

#### 12. **ArgoCD Applications** üü¢ LOW PRIORITY
```yaml
argocd:
  - argocd-application-controller-*
  - Enth√§lt: Application Manifests, Sync State
  - Backup: ‚ö†Ô∏è  SKIP (alles in Git!)
  - Recovery: Bootstrap ArgoCD ‚Üí Auto-sync from Git
```

---

### ‚ùå **TIER-3: STATELESS (NO BACKUP NEEDED!)**
**= Verlust = Einfach neu deployen (GitOps)**

#### Stateless Operators (SKIP BACKUP):
```yaml
‚úÖ SKIP (GitOps recovery):
  - rook-ceph-operator (aber Ceph DATA = critical!)
  - prometheus-operator
  - grafana-operator
  - elastic-operator
  - strimzi-cluster-operator
  - velero selbst
  - cilium
  - coredns
  - metrics-server
  - kube-prometheus-stack
  - loki-gateway
  - promtail
  - hubble
  - istio-operator
  - jaeger-operator

Warum SKIP?
  - Sind nur Controller (keine Daten)
  - GitOps deployt sie neu aus Manifests
  - Recovery: kubectl apply -k infrastructure/
```

---

## üéØ **ZUSAMMENFASSUNG: Was geh√∂rt in Velero Backups?**

### **Tier-0 (alle 6h):**
```yaml
Namespaces:
  - n8n-prod          # PostgreSQL + Redis (Workflows)
  - keycloak          # PostgreSQL (User Accounts)
  - infisical         # PostgreSQL (Secrets)
  - authelia          # Redis + Config (2FA, Sessions)
  - lldap             # SQLite (User Directory)

Label Selector:
  app.kubernetes.io/component: database
  cnpg.io/cluster: *
  app.kubernetes.io/name: redis (nur authelia, n8n)

Backup Method:
  - Velero CSI Volume Snapshots (Rook Ceph RBD)
  - Include: PVCs, Secrets, ConfigMaps
```

### **Tier-1 (t√§glich):**
```yaml
Namespaces:
  - n8n-dev           # Dev Environment
  - grafana           # Dashboards, Datasources
  - kafka             # (nur wenn Production Topics!)
  - influxdb          # (optional - Metrics History)

Label Selector:
  app.kubernetes.io/name: grafana
  app.kubernetes.io/name: kafka
```

### **Tier-2 (w√∂chentlich):**
```yaml
Namespaces:
  - sealed-secrets    # Encryption Keys
  - cert-manager      # (optional - Rate Limits)

Label Selector:
  app.kubernetes.io/name: sealed-secrets
```

### **SKIP (GitOps recovery):**
```yaml
Namespaces:
  - argocd            # (alles in Git)
  - rook-ceph         # (Operator, aber DATA wird via CSI gesichert!)
  - monitoring        # (Prometheus = ephemeral data)
  - kube-system       # (Kubernetes core services)
  - cilium            # (CNI, stateless)
  - velero            # (Backup Tool selbst)
```

---

## üì¶ **Aktuelle Velero Backup-Gr√∂√üen (gesch√§tzt):**

```
Tier-0 (kritisch):
  - n8n-prod:      5GB (PostgreSQL + Redis)
  - keycloak:      500MB (PostgreSQL)
  - infisical:     200MB (PostgreSQL)
  - authelia:      100MB (Redis)
  - lldap:         50MB (SQLite)
                   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Total:           5.85GB √ó 4 backups/Tag = 23.4GB/Tag

Tier-1 (wichtig):
  - grafana:       100MB (Dashboards)
  - n8n-dev:       1GB (Dev DB)
                   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Total:           1.1GB √ó 1 backup/Tag = 1.1GB/Tag

Elasticsearch Snapshots:
  - ES Indices:    10GB (komprimiert)
  - Daily:         10GB √ó 7 Tage = 70GB

Gesamt pro Woche:
  - Velero:        (23.4 + 1.1) √ó 7 = 171.5GB
  - Elasticsearch: 70GB
                   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Total:           241.5GB/Woche

Ceph S3 Bucket (100GB):
  - Mit Rotation (7d Tier-0, 30d Tier-1) = PASST! ‚úÖ
```

---

## ‚úÖ **FAZIT: Deine kritischen Backups:**

**MUST BACKUP (Datenverlust = Katastrophe):**
1. ‚úÖ n8n-prod (Workflows, Credentials) ‚Üí Tier-0
2. ‚úÖ Keycloak (User Accounts) ‚Üí Tier-0
3. ‚úÖ Infisical (Secrets) ‚Üí Tier-0
4. ‚úÖ Authelia (2FA, Sessions) ‚Üí Tier-0
5. ‚úÖ LLDAP (User Directory) ‚Üí Tier-0

**SHOULD BACKUP (√Ñrgerlich, aber rebuild m√∂glich):**
6. ‚úÖ Grafana (Dashboards) ‚Üí Tier-1
7. ‚úÖ n8n-dev (Dev Environment) ‚Üí Tier-1
8. ‚ö†Ô∏è  Kafka (nur wenn Production Events!)
9. ‚ö†Ô∏è  Elasticsearch Snapshots (Logs, Audit Trail)

**SKIP BACKUP (GitOps recovery):**
- ‚ùå ArgoCD (alles in Git)
- ‚ùå Monitoring Operators (stateless)
- ‚ùå Rook-Ceph Operator (Ceph DATA wird via CSI gesichert!)
- ‚ùå Cilium, CoreDNS (stateless)

**Speicherplatz:**
- Velero: ~25GB/Tag √ó 7 Tage = 175GB
- Elasticsearch: ~70GB (7 Tage Logs)
- Total: ~245GB/Woche
- **Ceph S3 Bucket (100GB):** MIT Rotation = PASST! ‚úÖ
