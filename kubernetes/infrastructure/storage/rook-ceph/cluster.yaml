apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
  labels:
    app.kubernetes.io/name: rook-ceph
    app.kubernetes.io/component: storage
    app.kubernetes.io/part-of: storage-platform
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.3
    allowUnsupported: false

  dataDirHostPath: /var/lib/rook

  # Google SRE practices: Conservative upgrade settings
  upgradeOSDRequiresHealthyPGs: true
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10

  # Google pattern: Odd number for consensus, minimal viable cluster
  mon:
    count: 3
    allowMultiplePerNode: false

  # Google pattern: Active-standby for management
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true  # Re-enabled after fixing overlapping CRUSH roots
      - name: balancer
        enabled: true
      - name: prometheus
        enabled: true

  # Minimal viable dashboard
  dashboard:
    enabled: true
    ssl: false

  # Essential monitoring only
  monitoring:
    enabled: true

  # Google pattern: Simple, reliable networking
  network:
    provider: host

  # Essential services only
  crashCollector:
    disable: false

  # Conservative cleanup
  cleanupPolicy:
    confirmation: ""

  # Homelab: Memory optimization - MGR needs more for dashboard/modules
  resources:
    mgr:
      limits:
        cpu: "500m"        # Increased for dashboard responsiveness
        memory: "2Gi"      # Increased - MGR loads many Python modules!
      requests:
        cpu: "100m"        # Increased for baseline performance
        memory: "1Gi"      # Increased - prevent OOMKills
    mon:
      limits:
        cpu: "300m"        # Reduced from 500m
        memory: "512Mi"    # Reduced from 1Gi - MAJOR SAVINGS!
      requests:
        cpu: "30m"         # Reduced from 50m
        memory: "128Mi"    # Reduced from 256Mi
    osd:
      limits:
        cpu: "750m"        # Moderate increase from 500m - better I/O without going overboard
        memory: "1.5Gi"    # Moderate increase from 1.2Gi - reasonable BlueStore cache
      requests:
        cpu: "150m"        # Moderate increase from 100m - better baseline
        memory: "600Mi"    # Moderate increase from 512Mi - reasonable BlueStore memory

  removeOSDsIfOutAndSafeToRemove: true

  # Optimized storage: 6 workers with dedicated 50GB block devices
  storage:
    useAllNodes: false
    useAllDevices: false  # Use specific block devices (scsi1 = /dev/sdb)
    nodes:
      - name: "worker-1"  # nipogi - 32GB RAM, stateful workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)
      - name: "worker-2"  # nipogi - 32GB RAM, stateful workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)
      - name: "worker-3"  # msa2proxmox - 20GB RAM, compute workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)
      - name: "worker-4"  # msa2proxmox - 20GB RAM, compute workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)
      - name: "worker-5"  # msa2proxmox - 20GB RAM, compute workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)
      - name: "worker-6"  # msa2proxmox - 20GB RAM, compute workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)

  # Google pattern: Simple health checks
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s

  # Google pattern: Graceful disruption management
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0

  # Google pattern: Simple, effective anti-affinity
  placement:
    all:
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
    mon:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: rook-ceph-mon
            topologyKey: kubernetes.io/hostname
    mgr:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: rook-ceph-mgr
              topologyKey: kubernetes.io/hostname
    osd:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: Exists
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: rook-ceph-osd
              topologyKey: kubernetes.io/hostname
