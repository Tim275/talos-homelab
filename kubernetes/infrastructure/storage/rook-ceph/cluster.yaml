apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
  labels:
    app.kubernetes.io/name: rook-ceph
    app.kubernetes.io/component: storage
    app.kubernetes.io/part-of: storage-platform
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  cephVersion:
    # Ceph Squid v19.2.3 - Latest stable (December 2025)
    # Ceph Tentacle v20.2.0 available but Squid is more stable for homelab
    # https://docs.ceph.com/en/latest/releases/
    image: quay.io/ceph/ceph:v19.2.3
    allowUnsupported: false

  # üîê ENTERPRISE SECURITY UPGRADES (Optional - for Production/Finance/Healthcare):
  # security:
  #   kms:  # Key Management Service integration (for encryption at rest)
  #     connectionDetails:
  #       KMS_PROVIDER: "vault"  # HashiCorp Vault integration
  #   keyRotation:
  #     enabled: true
  #     schedule: "@monthly"  # Rotate keys monthly

  dataDirHostPath: /var/lib/rook

  # ============================================================================
  # ENTERPRISE CEPH CONFIGURATION (SUSE/Red Hat Best Practices)
  # Ref: https://documentation.suse.com/sbp/storage/html/SBP-rook-ceph-kubernetes/
  # ============================================================================
  cephConfig:
    global:
      # CRITICAL: Explicit memory target (SUSE recommends minimum 4GB)
      osd_memory_target: "4294967296"     # 4GB in bytes - never go below this!
      osd_memory_target_autotune: "true"  # Auto-adjust based on available memory
      bluestore_cache_autotune: "true"    # Auto-tune BlueStore cache

      # VM-optimized heartbeat tolerances (prevents false OSD down alerts)
      osd_heartbeat_grace: "120"          # Default: 20s, increased for VM I/O latency
      osd_heartbeat_interval: "15"        # Default: 6s
      mon_osd_down_out_interval: "600"    # Wait 10 min before marking OSD out

      # VM/Proxmox BlueStore tuning (prevents SLOW_OPS warnings)
      # CRITICAL: Aggressive VM tuning for Proxmox shared storage
      osd_op_complaint_time: "120"        # Default: 30s, increased to 120s for VM I/O spikes
      osd_op_log_threshold: "50"          # Default: 5, reduce log spam
      osd_op_history_slow_op_threshold: "120" # Match complaint time
      bluestore_throttle_bytes: "134217728"          # 128MB write buffer (doubled)
      bluestore_throttle_deferred_bytes: "268435456" # 256MB deferred buffer (doubled)

      # Reduce I/O pressure from commit batching
      bluestore_default_buffered_write: "true"
      bluestore_prefer_deferred_size: "65536"  # Defer small writes

      # Client I/O priority over recovery (banking-grade: never slow down clients)
      osd_client_op_priority: "63"
      osd_recovery_op_priority: "3"
      osd_recovery_max_active: "1"
      osd_max_backfills: "1"

      # Scrubbing during low-load periods only
      osd_scrub_begin_hour: "2"           # Start scrubbing at 2 AM
      osd_scrub_end_hour: "6"             # End scrubbing at 6 AM
      osd_scrub_during_recovery: "false"  # Never scrub during recovery

  # IMPORTANT: Clean up previous data if cluster was deleted
  cleanupPolicy:
    confirmation: ""  # Set to "yes-really-destroy-data" for complete cleanup
    sanitizeDisks:
      method: quick  # quick|complete
      dataSource: zero  # zero|random
      iteration: 1

  # Google SRE practices: Conservative upgrade settings
  upgradeOSDRequiresHealthyPGs: true
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10

  # Google pattern: Odd number for consensus, minimal viable cluster
  mon:
    count: 3
    allowMultiplePerNode: false

  # Google pattern: Active-standby for management
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true  # Re-enabled after fixing overlapping CRUSH roots
      - name: balancer
        enabled: true
      - name: prometheus
        enabled: true
      - name: telemetry
        enabled: true  # Advanced cluster insights and health analytics

  # Production-grade dashboard with SSL (end-to-end encryption)
  dashboard:
    enabled: true
    ssl: false  # HTTP backend - Envoy Gateway handles TLS at edge (like ArgoCD, Grafana, etc.)

  # Essential monitoring only
  monitoring:
    enabled: true

  # Google pattern: Simple, reliable networking
  network:
    provider: host
    # üîê ENTERPRISE SECURITY - Wire Encryption enabled for Production
    connections:
      encryption:
        enabled: true  # msgr2 secure mode - encrypts all inter-daemon communication
      compression:
        enabled: false  # Disabled for security (avoid compression side-channels)

  # Essential services only
  crashCollector:
    disable: false

  # ============================================================================
  # ENTERPRISE RESOURCE CONFIGURATION (SUSE Best Practice)
  # ============================================================================
  # CRITICAL: "Never set resource LIMITS for Ceph daemons‚Äîonly REQUESTS"
  # Reason: In failure scenarios, killing daemons at limits makes things WORSE
  # Ref: https://documentation.suse.com/sbp/storage/html/SBP-rook-ceph-kubernetes/
  # ============================================================================
  resources:
    mgr:
      # NO LIMITS - Enterprise best practice
      requests:
        cpu: "250m"        # Homelab: reduced from 500m
        memory: "1.5Gi"    # Increased: SUSE min for prod is 2GB
    mon:
      # NO LIMITS - Enterprise best practice
      requests:
        cpu: "250m"        # Homelab: reduced from 500m
        memory: "2Gi"      # Increased: SUSE min for prod is 4GB - 1Gi caused quorum issues!
    osd:
      # NO LIMITS - Enterprise best practice
      # Request MUST be osd_memory_target (4GB) + overhead
      requests:
        cpu: "500m"        # Homelab: reduced from 1000m (still need some for I/O)
        memory: "4Gi"      # Homelab: reduced from 5Gi (match osd_memory_target)
    prepareosd:
      requests:
        cpu: "250m"
        memory: "256Mi"
    crashcollector:
      requests:
        cpu: "50m"
        memory: "64Mi"

  removeOSDsIfOutAndSafeToRemove: true

  # Optimized storage: 6 workers with dedicated 50GB block devices
  storage:
    useAllNodes: false
    useAllDevices: false  # Use specific block devices (scsi1 = /dev/sdb)
    # Global BlueStore tuning for VM environments
    config:
      osdsPerDevice: "1"
      # Reduce write amplification on VMs
      bluestore_min_alloc_size_hdd: "65536"
      # RocksDB tuning for reduced I/O spikes
      bluestore_rocksdb_options: "compression=kNoCompression,max_write_buffer_number=4,min_write_buffer_number_to_merge=1,recycle_log_file_num=4"
    nodes:
      - name: "worker-1"  # nipogi - 32GB RAM, stateful workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)
      - name: "worker-2"  # nipogi - 32GB RAM, stateful workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)
      - name: "worker-3"  # msa2proxmox - 20GB RAM, compute workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)
      - name: "worker-4"  # msa2proxmox - 20GB RAM, compute workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)
      - name: "worker-5"  # msa2proxmox - 20GB RAM, compute workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)
      - name: "worker-6"  # msa2proxmox - 20GB RAM, compute workloads
        config:
          storeType: bluestore
        devices:
          - name: "/dev/sdb"  # 50GB dedicated storage disk (scsi1)

  # Google pattern: Simple health checks
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s

  # Google pattern: Graceful disruption management
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0

  # Google pattern: Simple, effective anti-affinity
  placement:
    all:
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
    mon:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: rook-ceph-mon
            topologyKey: kubernetes.io/hostname
    mgr:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: rook-ceph-mgr
              topologyKey: kubernetes.io/hostname
    osd:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: Exists
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: rook-ceph-osd
              topologyKey: kubernetes.io/hostname
