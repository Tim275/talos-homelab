# üíæ Talos Homelab Disk Architecture

**Last Updated**: 2025-10-04
**Status**: Conservative sizing implemented
**Philosophy**: Incremental growth - start reasonable, scale when needed

---

## üìä Current Disk Configuration

### **Hardware Overview**

| Host | Model | CPU | RAM | Total Storage |
|------|-------|-----|-----|---------------|
| **nipogi** | NiPoGi AM21 | AMD Ryzen 9 6900HX (8C/16T @ 4.9GHz) | 32GB DDR5 | 1TB NVMe SSD |
| **msa2proxmox** | MINISFORUM MS-A2 | AMD Ryzen 9 9955HX (16C/32T @ 5.4GHz) | 96GB DDR5 | 2TB SSD |

### **VM Disk Allocation**

#### **Control Plane (ctrl-0)**
```yaml
Host: nipogi
OS Disk (scsi0):    50GB  # Talos OS + /var/lib/rook monitors
Ceph OSD (scsi1):   None  # Control plane has no Ceph OSD
```

#### **Workers 1-2 (nipogi)**
```yaml
Host: nipogi (1TB total storage)
Per Worker:
  OS Disk (scsi0):    50GB   # Talos OS + /var/lib/rook monitors
  Ceph OSD (scsi1):   200GB  # Block storage for PVCs

Total per Worker: 250GB
2 Workers Total:  500GB (leaving ~500GB free on host)
```

#### **Workers 3-6 (msa2proxmox)**
```yaml
Host: msa2proxmox (2TB total storage)
Per Worker:
  OS Disk (scsi0):    50GB   # Talos OS + /var/lib/rook monitors
  Ceph OSD (scsi1):   200GB  # Block storage for PVCs

Total per Worker: 250GB
4 Workers Total:  1TB (leaving ~1TB free on host)
```

---

## üéØ Disk Usage Breakdown

### **OS Disk (scsi0) - 50GB**

**Purpose**: Operating system, container runtime, Ceph monitor metadata

| Component | Size | Description |
|-----------|------|-------------|
| Talos OS | ~5GB | Minimal Linux OS for Kubernetes |
| Container Images | ~20-30GB | Cached container images |
| /var/lib/rook | ~5-10GB | Ceph monitor metadata (mon-a, mon-b, mon-c) |
| Logs & Temp | ~5GB | System logs, temporary files |
| **Reserve** | ~10GB | Growth buffer |

**Why 50GB?**
- ‚úÖ Fixes MON_DISK_LOW warning (was 18GB, 78% full)
- ‚úÖ Plenty of room for container image cache
- ‚úÖ Not overly aggressive (can increase later if needed)

### **Ceph OSD Disk (scsi1) - 200GB per Worker**

**Purpose**: Distributed block storage for Kubernetes PVCs

| Component | Description |
|-----------|-------------|
| **Raw Storage** | 6 workers √ó 200GB = 1.2TB total raw |
| **Replication** | 3x replication for high availability |
| **Usable Storage** | 1.2TB √∑ 3 = **~400GB usable** |

**Current Usage** (~280GB of 400GB usable):
```
Elasticsearch:     3 √ó 10GB = 30GB
Kafka:             3 √ó 5GB  = 15GB
N8N (prod+dev):    ~15GB
Audiobookshelf:    ~120GB
Prometheus:        19GB
PostgreSQL DBs:    ~20GB
InfluxDB:          20GB
Loki:              10GB
Keep:              10GB
Other Apps:        ~20GB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total Used:        ~280GB
Available:         ~120GB (30% free)
```

---

## üèóÔ∏è Rook-Ceph Architecture

### **Cluster Topology**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TALOS CLUSTER - 7 Nodes                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                           ‚îÇ
‚îÇ  Control Plane (ctrl-0):                                 ‚îÇ
‚îÇ  ‚îú‚îÄ OS Disk: 50GB                                        ‚îÇ
‚îÇ  ‚îú‚îÄ Ceph Monitor: Yes (uses /var/lib/rook on OS disk)   ‚îÇ
‚îÇ  ‚îî‚îÄ Ceph OSD: No                                         ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  Workers (6x):                                           ‚îÇ
‚îÇ  ‚îú‚îÄ OS Disk: 50GB                                        ‚îÇ
‚îÇ  ‚îú‚îÄ Ceph Monitor: Distributed (mon-a, mon-b, mon-c)     ‚îÇ
‚îÇ  ‚îî‚îÄ Ceph OSD: Yes (200GB dedicated /dev/sdb disk)       ‚îÇ
‚îÇ                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CEPH DAEMONS - Distribution                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                           ‚îÇ
‚îÇ  Monitors (3):         mon-a, mon-b, mon-c               ‚îÇ
‚îÇ  ‚îú‚îÄ Storage:           /var/lib/rook (on OS disk)        ‚îÇ
‚îÇ  ‚îî‚îÄ Anti-affinity:     1 per node (different nodes)      ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  Managers (2):         mgr-a, mgr-b                      ‚îÇ
‚îÇ  ‚îî‚îÄ Modules:           pg_autoscaler, balancer, prometheus‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  OSDs (6):             osd-0 through osd-5               ‚îÇ
‚îÇ  ‚îú‚îÄ 1 OSD per worker:  /dev/sdb (scsi1)                  ‚îÇ
‚îÇ  ‚îú‚îÄ Store Type:        BlueStore                         ‚îÇ
‚îÇ  ‚îî‚îÄ Size:              200GB each                        ‚îÇ
‚îÇ                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Storage Classes**

| Storage Class | Use Case | Replication | Performance |
|--------------|----------|-------------|-------------|
| `rook-ceph-block-enterprise` | PVCs for databases, apps | 3x | High (NVMe/SSD) |

---

## üìà Capacity Planning & Growth

### **Current Capacity (Conservative Start)**

```
OS Disks:
‚îú‚îÄ ctrl-0:     50GB   (control plane)
‚îú‚îÄ worker-1:   50GB   (nipogi)
‚îú‚îÄ worker-2:   50GB   (nipogi)
‚îú‚îÄ worker-3:   50GB   (msa2proxmox)
‚îú‚îÄ worker-4:   50GB   (msa2proxmox)
‚îú‚îÄ worker-5:   50GB   (msa2proxmox)
‚îî‚îÄ worker-6:   50GB   (msa2proxmox)
Total:         350GB

Ceph OSDs:
‚îú‚îÄ worker-1:   200GB  (nipogi)
‚îú‚îÄ worker-2:   200GB  (nipogi)
‚îú‚îÄ worker-3:   200GB  (msa2proxmox)
‚îú‚îÄ worker-4:   200GB  (msa2proxmox)
‚îú‚îÄ worker-5:   200GB  (msa2proxmox)
‚îî‚îÄ worker-6:   200GB  (msa2proxmox)
Total Raw:     1.2TB
Usable (3x):   ~400GB
```

### **Future Growth Path (When Needed)**

**Option 1: Moderate Growth**
```
OS Disk:       50GB ‚Üí 100GB   (2x increase)
Ceph OSD:      200GB ‚Üí 400GB  (2x increase)
Total Usable:  400GB ‚Üí 800GB  (2x increase)
```

**Option 2: Aggressive Growth (Use Full Hardware)**
```
nipogi workers:
  OS Disk:     50GB ‚Üí 100GB
  Ceph OSD:    200GB ‚Üí 400GB  (leaves 500GB free per worker on 1TB host)

msa2proxmox workers:
  OS Disk:     50GB ‚Üí 100GB
  Ceph OSD:    200GB ‚Üí 450GB  (leaves ~1TB free per worker on 2TB host)

Total Usable:  400GB ‚Üí ~1TB
```

### **When to Grow?**

Monitor these metrics:

**OS Disk Warning Signs:**
- `df -h /` shows >80% usage
- MON_DISK_LOW alerts return
- Container image cache eviction

**Ceph OSD Warning Signs:**
- Ceph status shows >70% full
- PVC provisioning failures
- HEALTH_WARN with "nearfull" status

---

## üîß Infrastructure as Code

### **Terraform Configuration**

Location: `tofu/talos_nodes.auto.tfvars`

```hcl
# Per-node disk sizing
"worker-1" = {
  host_node      = "nipogi"
  os_disk_size   = 50    # OS + /var/lib/rook monitors
  ceph_disk_size = 200   # Ceph OSD
}
```

### **Proxmox VM Disk Layout**

```
VM ID 1001 (worker-1):
‚îú‚îÄ scsi0: 50GB   (Talos OS disk, bootable)
‚îÇ  ‚îú‚îÄ Format:        raw
‚îÇ  ‚îú‚îÄ Cache:         writethrough
‚îÇ  ‚îú‚îÄ Discard:       on
‚îÇ  ‚îî‚îÄ Mounted as:    / (root filesystem)
‚îÇ
‚îî‚îÄ scsi1: 200GB  (Ceph OSD disk, raw block device)
   ‚îú‚îÄ Format:        raw
   ‚îú‚îÄ Cache:         none (direct I/O for Ceph)
   ‚îú‚îÄ Discard:       on
   ‚îî‚îÄ Managed by:    Rook-Ceph operator (/dev/sdb)
```

### **Rook-Ceph Configuration**

Location: `kubernetes/infrastructure/storage/rook-ceph/cluster.yaml`

```yaml
spec:
  dataDirHostPath: /var/lib/rook  # Monitor metadata on OS disk

  mon:
    count: 3                       # HA with odd number
    allowMultiplePerNode: false    # Anti-affinity
    # Uses dataDirHostPath (no PVC for bootstrap simplicity)

  storage:
    useAllNodes: false
    useAllDevices: false           # Explicit device selection
    nodes:
      - name: "worker-1"
        devices:
          - name: "/dev/sdb"       # scsi1 - 200GB Ceph OSD
```

---

## üö® Troubleshooting

### **MON_DISK_LOW Warning**

**Symptom:**
```
HEALTH_WARN: mon b is low on available space
```

**Root Cause:**
- Ceph monitors use `/var/lib/rook` on OS disk (scsi0)
- OS disk was too small (20GB ‚Üí 78% full)

**Solution:**
- ‚úÖ Increased OS disk to 50GB
- ‚úÖ Provides 30GB+ free space for monitors

### **Verifying Disk Sizes**

**Check OS disk space:**
```bash
kubectl get nodes -o custom-columns=\
NAME:.metadata.name,\
OS-DISK:.status.capacity.ephemeral-storage
```

**Check Ceph cluster capacity:**
```bash
kubectl get cephcluster -n rook-ceph rook-ceph \
  -o jsonpath='{.status.ceph.capacity}' | jq
```

**Check Ceph health:**
```bash
kubectl get cephcluster -n rook-ceph rook-ceph \
  -o jsonpath='{.status.ceph.health}'
# Expected: HEALTH_OK
```

---

## üìö Best Practices Applied

### **Ceph Official Recommendations**

‚úÖ **1 OSD per disk** - Not multiple OSDs on single disk
‚úÖ **Odd number of monitors** - 3 monitors for HA
‚úÖ **Minimum OSD size** - 1TB recommended (we: 200GB acceptable for homelab)
‚úÖ **Separate monitor storage** - Monitors on fast storage (/var/lib/rook on OS disk)
‚úÖ **BlueStore backend** - Modern Ceph storage engine
‚úÖ **3x replication** - High availability with data safety

### **Homelab Optimizations**

‚úÖ **Conservative sizing** - Start small, grow when needed
‚úÖ **Per-node flexibility** - Different hosts, different sizes (future-ready)
‚úÖ **Cost efficiency** - Don't over-allocate, leave room for host OS
‚úÖ **Easy scaling** - Disk resize is non-destructive (grow only)

---

## üéØ Migration & Resize Guide

### **How to Resize Disks (Future)**

**1. Update Terraform:**
```bash
cd tofu/
# Edit talos_nodes.auto.tfvars
# Change os_disk_size or ceph_disk_size

tofu plan   # Review changes
tofu apply  # Apply disk resize
```

**2. Resize OS Disk (Automatic):**
```bash
# Talos automatically expands root filesystem on boot
# No manual intervention needed
```

**3. Resize Ceph OSD (Requires Rebuild):**
```bash
# Ceph OSDs cannot be resized in-place
# Must destroy and recreate with larger disk

# 1. Scale down OSD (gracefully)
kubectl -n rook-ceph delete deployment rook-ceph-osd-X

# 2. Wait for Ceph to rebalance (may take time)
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status

# 3. Apply Terraform change (larger disk)
tofu apply

# 4. Rook operator will recreate OSD with new size
kubectl -n rook-ceph get pods -l app=rook-ceph-osd
```

**‚ö†Ô∏è Important**: Resize one OSD at a time, wait for HEALTH_OK between each!

---

## üìä Monitoring & Alerts

### **Key Metrics to Watch**

**Ceph Health:**
```bash
# Should always be HEALTH_OK
kubectl get cephcluster -n rook-ceph rook-ceph \
  -o jsonpath='{.status.ceph.health}'
```

**Ceph Capacity:**
```bash
kubectl get cephcluster -n rook-ceph rook-ceph \
  -o jsonpath='{.status.ceph.capacity}' | jq
```

**OS Disk Usage:**
```bash
# Check from inside pods or nodes
df -h /var/lib/rook
```

### **Prometheus Alerts**

Located in: `kubernetes/infrastructure/monitoring/kube-prometheus-stack/`

- `CephClusterErrorState` - Ceph cluster in ERROR state
- `CephMonDiskLow` - Monitor disk <15% free (previously triggered!)
- `CephOSDDiskFull` - OSD >85% full
- `CephClusterNearFull` - Cluster >75% capacity

---

## üîó References

- [Rook Ceph Documentation](https://rook.io/docs/rook/latest/)
- [Ceph Hardware Recommendations](https://docs.ceph.com/en/latest/start/hardware-recommendations/)
- [Talos Linux Storage](https://www.talos.dev/latest/talos-guides/configuration/storage/)

---

**Maintained by**: Talos Homelab Team
**Last Review**: 2025-10-04
**Next Review**: When approaching 70% capacity or adding new nodes
