cluster:
  name: talos
  id: 1

kubeProxyReplacement: true

# 2025 ENTERPRISE PERFORMANCE OPTIMIZATIONS
# =============================================
# Bypass iptables conntrack for max performance
installNoConntrackIptablesRules: true

# Native routing for best eBPF performance
routingMode: native
autoDirectNodeRoutes: true
ipv4NativeRoutingCIDR: 10.244.0.0/16  # Kubernetes pod CIDR

# Talos specific
k8sServiceHost: localhost
k8sServicePort: 7445
securityContext:
  capabilities:
    ciliumAgent:
      - CHOWN
      - KILL
      - NET_ADMIN
      - NET_RAW
      - IPC_LOCK
      - SYS_ADMIN
      - SYS_RESOURCE
      - DAC_OVERRIDE
      - FOWNER
      - SETGID
      - SETUID
    cleanCiliumState:
      - NET_ADMIN
      - SYS_ADMIN
      - SYS_RESOURCE

cgroup:
  autoMount:
    enabled: false  # TALOS REQUIRED - Talos provides mounts
  hostRoot: /sys/fs/cgroup

# https://www.talos.dev/latest/talos-guides/network/host-dns/#forwarding-kube-dns-to-host-dns
# https://docs.cilium.io/en/stable/operations/performance/tuning/#ebpf-host-routing
bpf:
  hostLegacyRouting: true
  masquerade: true  # Required for installNoConntrackIptablesRules
  lbExternalClusterIP: true  # TAILSCALE VPN: Allow external access to ClusterIP services

  # ENTERPRISE SCALE - BPF map sizing for 10K+ pods
  # https://docs.cilium.io/en/stable/operations/performance/scalability/
  bpfMapDynamicSizeRatio: 0.0025  # Scale maps with cluster size

  # Policy enforcement acceleration
  policyMapMax: 65536    # Max policy entries

  # Connection tracking optimization
  ctTcpMax: 524288       # 512K TCP connections
  ctAnyMax: 262144       # 256K other connections

  # NAT table sizing
  natMax: 524288         # 512K NAT entries
  neighMax: 524288       # 512K neighbor entries

# https://docs.cilium.io/en/stable/network/concepts/ipam/
ipam:
  mode: kubernetes
  multiPoolPreAllocation: ""

operator:
  # HIGH AVAILABILITY - 3 replicas for zero downtime
  replicas: 3

  rollOutPods: true
  prometheus:
    metricsService: true
    enabled: true
    port: 9963
    serviceMonitor:
      enabled: true
  # DISABLED: Too large for ConfigMap annotations
  dashboards:
    enabled: false

  # ENTERPRISE RESOURCES
  resources:
    limits:
      cpu: 1000m      # Increased for HA workload
      memory: 512Mi   # Increased for stability
    requests:
      cpu: 100m       # Increased baseline
      memory: 256Mi   # Increased for HA
rollOutCiliumPods: true
resources:
  limits:
    cpu: 1000m
    memory: 1Gi
  requests:
    cpu: 200m
    memory: 512Mi

k8sClientRateLimit:
  qps: 50      # Increased from 20 for Gateway API controller
  burst: 200   # Increased from 100 for Gateway API controller

l2announcements:
  enabled: true

externalIPs:
  enabled: true

loadBalancer:
  # https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#maglev-consistent-hashing
  algorithm: maglev
  enabled: true

gatewayAPI:
  enabled: true
  enableAlpn: true
  enableAppProtocol: true

# ENTERPRISE PERFORMANCE - Bandwidth Manager + BBR Congestion Control
# ========================================================================
# Bandwidth Manager: Enables Fair Queue packet scheduling + optimal sysctl settings
# BBR (Bottleneck Bandwidth and RTT): Google's congestion control algorithm
#
# PERFORMANCE IMPACT:
# - BBR: Up to 2700x higher throughput vs loss-based algorithms (CUBIC)
# - Bandwidth Manager: Reduces latency spikes, improves fairness
# - Requires: Linux kernel 5.18+ for BBR on pods
#
# https://docs.cilium.io/en/stable/network/kubernetes/bandwidth-manager/
bandwidthManager:
  enabled: true
  bbr: true  # ENABLED: BBR congestion control for improved throughput

# ENTERPRISE PERFORMANCE - BIG TCP (DISABLED due to WireGuard)
# ================================================================
# BIG TCP: Allows up to 192KB GSO/GRO packets (vs 64KB default)
#
# PERFORMANCE IMPACT:
# - Reduces CPU by 20-40% for high-throughput workloads
# - Reduces # of times network stack is traversed
#
# WARNING: INCOMPATIBLE WITH WIREGUARD ENCRYPTION!
# BIG TCP requires: No tunneling, no encryption, kernel 6.3+, supported NICs (mlx4/mlx5/ice)
#
# CURRENT: Disabled (auto-disabled by Cilium due to WireGuard encryption)
# TRADE-OFF: Security (WireGuard) > Performance (BIG TCP)
#
# To enable BIG TCP: Set encryption.enabled=false
# https://docs.cilium.io/en/stable/operations/performance/tuning/
enableIPv4BIGTCP: false  # Auto-disabled when WireGuard is enabled
enableIPv6BIGTCP: false

# ENTERPRISE SECURITY - WireGuard Transparent Encryption
# ===========================================================
# Zero-Trust pod-to-pod encryption using WireGuard (Linux kernel native)
#
# SECURITY BENEFITS:
# - Pod-to-pod traffic encrypted at network layer (transparent to apps)
# - Compliance: GDPR, SOC2, PCI-DSS, HIPAA
# - Multi-tenant isolation (encrypted by default)
# - Production-grade: Used by Google GKE, AWS EKS
#
# PERFORMANCE IMPACT:
# - BIG TCP disabled (incompatible) -> ~20-40% CPU overhead vs BIG TCP
# - WireGuard overhead: ~5-10% (minimal, hardware-accelerated on modern CPUs)
# - BBR compensates partially for BIG TCP loss
#
# NET IMPACT: ~10-15% performance trade-off for enterprise security
#
# DECRYPTING TRAFFIC FOR DEBUGGING:
# -------------------------------------
# Option 1: Hubble (L7 visibility BEFORE encryption)
#   kubectl port-forward -n kube-system svc/hubble-ui 12000:80
#   # Hubble sees unencrypted HTTP/DNS metrics
#
# Option 2: tcpdump inside pod (BEFORE encryption)
#   kubectl exec -it <pod> -- tcpdump -i any -nn port 80
#
# Option 3: Cilium Monitor (eBPF events, pre-encryption)
#   kubectl exec -n kube-system ds/cilium -- cilium monitor --type drop --type trace
#
# Option 4: Disable encryption temporarily (NOT for production!)
#   helm upgrade cilium --set encryption.enabled=false
#
# WARNING: Wire-level tcpdump on nodes will show ENCRYPTED traffic (WireGuard ESP packets)
#
# https://docs.cilium.io/en/stable/security/network/encryption-wireguard/
encryption:
  enabled: true
  type: wireguard
  nodeEncryption: true  # Encrypt node-to-node traffic (beta feature)
  strictMode:
    enabled: true
    cidr: 10.244.0.0/16  # Pod CIDR - only encrypt pod traffic
    allowRemoteNodeIdentities: false

# ENTERPRISE NETWORKING - DNS Proxy
# =====================================
# DNS visibility + FQDN-based network policies
# Enables DNS query logging in Hubble metrics
dnsProxy:
  enabled: true
  enableDnsCompression: true

# ENTERPRISE OBSERVABILITY - L7 Proxy
# =======================================
# HTTP/gRPC protocol visibility via Envoy proxy
# Required for: httpV2 metrics, L7 NetworkPolicies, API-level observability
l7Proxy: true

envoy:
  prometheus:
    enabled: true
    port: "9964"
    serviceMonitor:
      enabled: true
  securityContext:
    capabilities:
      keepCapNetBindService: true
      envoy: [ NET_ADMIN, PERFMON, BPF ]

hubble:
  enabled: true
  metrics:
    enabled:
      - dns
      - drop
      - tcp
      - flow
      - port-distribution
      - icmp
      - "httpV2:exemplars=true;labelsContext=source_ip,source_namespace,source_workload,destination_ip,destination_namespace,destination_workload,traffic_direction;sourceContext=workload-name|reserved-identity;destinationContext=workload-name|reserved-identity"
    enableOpenMetrics: true
    port: 9965
    serviceMonitor:
      enabled: true
    # DISABLED: Too large for ConfigMap annotations
    dashboards:
      enabled: false
  relay:
    enabled: true
    rollOutPods: true
    prometheus:
      enabled: true
      port: 9966
      serviceMonitor:
        enabled: true
  ui:
    enabled: true
    rollOutPods: true

ingressController:
  enabled: true   # Cilium native ingress controller
  default: true   # Set as default IngressClass
  loadbalancerMode: shared  # Share LB IPs across Ingresses

authentication:
  enabled: false
  mutual:
    spire:
      enabled: false
      install:
        server:
          dataStorage:
            storageClass: cilium-spire-sc

prometheus:
  metricsService: true
  enabled: true
  port: 9962
  serviceMonitor:
    enabled: true
    trustCRDsExist: true

# DISABLED: Cilium dashboards cause "Too long" annotation error
# Instead use GrafanaDashboard CRDs in monitoring layer
dashboards:
  enabled: false
# DISABLED: Clustermesh not needed for single-cluster setup
# Prevents TLS certificate errors in clustermesh-apiserver
clustermesh:
  useAPIServer: false
  config:
    enabled: false
    global:
      services:
        enabled: false
    clusters: []
  apiserver:
    metrics:
      enabled: false
      port: 9962
      serviceMonitor:
        enabled: true

# Istio configuration
cni:
  exclusive: false

socketLB:
  hostNamespaceOnly: false  # Enable socket-based LB across all namespaces (fixes DNS resolution)
