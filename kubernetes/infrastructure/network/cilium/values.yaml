cluster:
  name: talos
  id: 1

kubeProxyReplacement: true

# ðŸ”¥ 2025 ENTERPRISE PERFORMANCE OPTIMIZATIONS
# =============================================
# Bypass iptables conntrack for max performance
installNoConntrackIptablesRules: true

# Native routing for best eBPF performance
routingMode: native
autoDirectNodeRoutes: true

# Talos specific
k8sServiceHost: localhost
k8sServicePort: 7445
securityContext:
  capabilities:
    ciliumAgent:
      - CHOWN
      - KILL
      - NET_ADMIN
      - NET_RAW
      - IPC_LOCK
      - SYS_ADMIN
      - SYS_RESOURCE
      - DAC_OVERRIDE
      - FOWNER
      - SETGID
      - SETUID
    cleanCiliumState:
      - NET_ADMIN
      - SYS_ADMIN
      - SYS_RESOURCE

cgroup:
  autoMount:
    enabled: false  # TALOS REQUIRED - Talos provides mounts
  hostRoot: /sys/fs/cgroup

# https://www.talos.dev/latest/talos-guides/network/host-dns/#forwarding-kube-dns-to-host-dns
# https://docs.cilium.io/en/stable/operations/performance/tuning/#ebpf-host-routing
bpf:
  hostLegacyRouting: true

  # ðŸ”¥ ENTERPRISE SCALE - BPF map sizing for 10K+ pods
  # https://docs.cilium.io/en/stable/operations/performance/scalability/
  bpfMapDynamicSizeRatio: 0.0025  # Scale maps with cluster size

  # Policy enforcement acceleration
  policyMapMax: 65536    # Max policy entries

  # Connection tracking optimization
  ctTcpMax: 524288       # 512K TCP connections
  ctAnyMax: 262144       # 256K other connections

  # NAT table sizing
  natMax: 524288         # 512K NAT entries
  neighMax: 524288       # 512K neighbor entries

# https://docs.cilium.io/en/stable/network/concepts/ipam/
ipam:
  mode: kubernetes
  multiPoolPreAllocation: ""

operator:
  # ðŸ”¥ HIGH AVAILABILITY - 3 replicas for zero downtime
  replicas: 3

  rollOutPods: true
  prometheus:
    metricsService: true
    enabled: true
    port: 9963
    serviceMonitor:
      enabled: true
  # ðŸš¨ DISABLED: Too large for ConfigMap annotations
  dashboards:
    enabled: false

  # ðŸš€ ENTERPRISE RESOURCES
  resources:
    limits:
      cpu: 1000m      # Increased for HA workload
      memory: 512Mi   # Increased for stability
    requests:
      cpu: 100m       # Increased baseline
      memory: 256Mi   # Increased for HA
rollOutCiliumPods: true
resources:
  limits:
    cpu: 1000m
    memory: 1Gi
  requests:
    cpu: 200m
    memory: 512Mi

k8sClientRateLimit:
  qps: 20
  burst: 100

l2announcements:
  enabled: true

externalIPs:
  enabled: true

loadBalancer:
  # https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#maglev-consistent-hashing
  algorithm: maglev

gatewayAPI:
  enabled: true
  enableAlpn: true
  enableAppProtocol: true

envoy:
  prometheus:
    enabled: true
    port: "9964"
    serviceMonitor:
      enabled: true
  securityContext:
    capabilities:
      keepCapNetBindService: true
      envoy: [ NET_ADMIN, PERFMON, BPF ]

hubble:
  enabled: true
  metrics:
    enabled:
      - dns
      - drop
      - tcp
      - flow
      - port-distribution
      - icmp
      - "httpV2:exemplars=true;labelsContext=source_ip,source_namespace,source_workload,destination_ip,destination_namespace,destination_workload,traffic_direction;sourceContext=workload-name|reserved-identity;destinationContext=workload-name|reserved-identity"
    enableOpenMetrics: true
    port: 9965
    serviceMonitor:
      enabled: true
    # ðŸš¨ DISABLED: Too large for ConfigMap annotations
    dashboards:
      enabled: false
  relay:
    enabled: true
    rollOutPods: true
    prometheus:
      enabled: true
      port: 9966
      serviceMonitor:
        enabled: true
  ui:
    enabled: true
    rollOutPods: true

ingressController:
  enabled: false

authentication:
  enabled: false
  mutual:
    spire:
      enabled: false
      install:
        server:
          dataStorage:
            storageClass: cilium-spire-sc

prometheus:
  metricsService: true
  enabled: true
  port: 9962
  serviceMonitor:
    enabled: true
    trustCRDsExist: true

# ðŸš¨ DISABLED: Cilium dashboards cause "Too long" annotation error
# Instead use GrafanaDashboard CRDs in monitoring layer
dashboards:
  enabled: false
clustermesh:
  useAPIServer: true
  config:
    enabled: true
    global:
      services:
        enabled: true
    clusters: []
  apiserver:
    metrics:
      enabled: true
      port: 9962
      serviceMonitor:
        enabled: true

# Istio configuration
cni:
  exclusive: false

socketLB:
  hostNamespaceOnly: true
