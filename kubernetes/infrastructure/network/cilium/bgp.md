# BGP Migration Guide - Cilium L2 to BGP

This document explains the migration from **L2 Announcements (ARP)** to **BGP peering** for LoadBalancer IP advertisement.

---

## ğŸ”´ Current Setup - L2 Announcements (ARP)

**Architecture:**
```
LoadBalancer Service â†’ Cilium L2 Announcements â†’ ARP Broadcast â†’ UniFi Router learns MAC
```

**Current Files:**
- `announce.yaml` - CiliumL2AnnouncementPolicy (enables ARP announcements)
- `ip-pool.yaml` - CiliumLoadBalancerIPPool (192.168.68.150-170)
- `values.yaml` - `l2announcements.enabled: true`

**How it works:**
- Cilium announces LoadBalancer IPs via **Layer 2 ARP** protocol
- UniFi router learns: "IP 192.168.68.150 is at MAC address XY"
- Similar to MetalLB L2 mode

**Limitations:**
- âŒ No true load balancing (only failover to single node)
- âŒ Client source IP lost (SNAT applied)
- âŒ Single point of failure per service
- âŒ ARP broadcast traffic in network
- âŒ Not scalable for multi-cluster setups

---

## ğŸŸ¢ Target Setup - BGP Peering

**Architecture:**
```
LoadBalancer Service â†’ Cilium BGP â†’ eBGP Peering â†’ UniFi Router learns Routes â†’ ECMP Load Balancing
```

**New Files Required:**
- `bgp-cluster-config.yaml` - BGP configuration (ASN, Router ID)
- `bgp-peer-config.yaml` - UniFi router as BGP neighbor
- `bgp-advertisement.yaml` - What to advertise (Service, Pod IPs)
- `bgp-password-secret.yaml` - Optional BGP authentication
- `ip-pool.yaml` - **KEEP** (existing IP pool, no changes)
- `values.yaml` - **UPDATE** (`bgp.enabled: true`)

**How it works:**
- Cilium speaks **BGP protocol** with UniFi router (eBGP peering)
- UniFi router learns routes: "192.168.68.150 reachable via Node1, Node2, Node3"
- Router uses **ECMP** (Equal-Cost Multi-Path) for true load balancing
- Traffic distributed across all 3 nodes

**Advantages:**
- âœ… True load balancing via ECMP (3 nodes)
- âœ… Client source IP preserved (`externalTrafficPolicy: Local`)
- âœ… No single point of failure
- âœ… Reduced broadcast traffic
- âœ… Industry standard (AWS, GCP, Azure use BGP)
- âœ… Multi-cluster ready
- âœ… Enterprise skill building

---

## ğŸ’¡ Practical Explanation - What Actually Happens?

### ğŸ”´ AKTUELL (L2 ARP) - Current Behavior

**Example:** You access `https://n8n.timourhomelab.org`

```
1. Browser asks: "Where is 192.168.68.150?"

2. Cilium responds via ARP:
   "192.168.68.150 is at MAC address aa:bb:cc (Node 1)"

3. UniFi Router remembers:
   192.168.68.150 â†’ MAC aa:bb:cc â†’ Node 1 ONLY

4. ALL requests go to Node 1
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Client  â”‚â”€â”€â”€â”€â”€â”€â”
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                    â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ UniFi Router         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼ 100% Traffic
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Node 1 (overloaded) â”‚  â† ALL requests here!
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Node 2 (idle)    â”‚  â† Does nothing!
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Node 3 (idle)    â”‚  â† Does nothing!
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Problems:**
- âŒ Node 1 does EVERYTHING (overloaded)
- âŒ Node 2 + 3 do NOTHING (wasted resources)
- âŒ If Node 1 crashes â†’ Service down (~5-10 seconds until Cilium failover to Node 2)
- âŒ Client IP is replaced by NAT â†’ Logs show only Node IP

---

### ğŸŸ¢ NEU (BGP) - New Behavior

**Example:** You access `https://n8n.timourhomelab.org`

```
1. Browser asks: "Where is 192.168.68.150?"

2. Cilium speaks BGP with UniFi:
   Node 1: "192.168.68.150 reachable via ME"
   Node 2: "192.168.68.150 reachable via ME"
   Node 3: "192.168.68.150 reachable via ME"

3. UniFi Router learns 3 equal-cost routes:
   192.168.68.150 â†’ via Node 1 (33%)
   192.168.68.150 â†’ via Node 2 (33%)
   192.168.68.150 â†’ via Node 3 (33%)

4. Traffic is DISTRIBUTED (ECMP Load Balancing)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Client  â”‚â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â”‚      â”‚
                    â–¼      â–¼      â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ UniFi Router (ECMP)          â”‚
   â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
      â”‚ 33%       â”‚ 33%       â”‚ 33%
      â–¼           â–¼           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Node 1 â”‚ â”‚ Node 2 â”‚ â”‚ Node 3 â”‚  â† ALL working!
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Advantages:**
- âœ… Traffic evenly distributed (33% / 33% / 33%)
- âœ… All nodes working â†’ Better performance
- âœ… If Node 1 crashes â†’ Router immediately uses Node 2+3 (0 downtime)
- âœ… Client IP preserved â†’ You see real IPs in logs

---

## ğŸ¯ Concrete Examples - What You'll Notice

### **Example 1: Heavy Load (many requests)**

**L2 ARP (OLD):**
```
10 clients access N8N
â†’ ALL 10 requests go to Node 1
â†’ Node 1: 100% CPU
â†’ Node 2: 0% CPU (idle)
â†’ Node 3: 0% CPU (idle)
â†’ Response Time: 500ms (Node 1 overloaded)
```

**BGP (NEW):**
```
10 clients access N8N
â†’ 3-4 requests to Node 1 (33%)
â†’ 3-4 requests to Node 2 (33%)
â†’ 3-4 requests to Node 3 (33%)
â†’ All nodes: 33% CPU (evenly distributed)
â†’ Response Time: 100ms (load distributed)
```

---

### **Example 2: Node Failure**

**L2 ARP (OLD):**
```
Node 1 crashes (running N8N service)
â†’ 5-10 seconds downtime (Cilium failover)
â†’ Cilium announces new ARP for Node 2
â†’ Service back online
```

**BGP (NEW):**
```
Node 1 crashes
â†’ BGP session to Node 1 breaks
â†’ UniFi Router IMMEDIATELY uses Node 2+3
â†’ 0 seconds downtime
â†’ Load now: 50% Node 2, 50% Node 3
```

---

### **Example 3: Client IP Logging**

**L2 ARP (OLD):**
```
Your laptop: 192.168.68.50
â†’ Request to N8N
â†’ N8N sees in logs: 192.168.68.10 (Node IP!)
â†’ You CANNOT see which client it was
```

**BGP (NEW):**
```
Your laptop: 192.168.68.50
â†’ Request to N8N
â†’ N8N sees in logs: 192.168.68.50 (real client IP!)
â†’ You see EXACTLY which client it was
```

---

## ğŸ“Š Performance Comparison (Real World)

| Scenario | L2 ARP | BGP |
|----------|--------|-----|
| **100 Requests/sec** | Node 1: 100 req/s<br>Node 2: 0<br>Node 3: 0 | Node 1: 33 req/s<br>Node 2: 33 req/s<br>Node 3: 33 req/s |
| **Node 1 crashes** | 5-10s Downtime | 0s Downtime |
| **CPU Load** | Node 1: 80%<br>Node 2: 5%<br>Node 3: 5% | Node 1: 30%<br>Node 2: 30%<br>Node 3: 30% |
| **Client IP visible?** | âŒ No (NAT) | âœ… Yes |

---

## ğŸ’¡ Why This Matters for Your Homelab

**1. Better Resource Utilization:**
- You have 3 nodes with 16 cores each
- L2: Only 1 node working (32 cores idle = wasted)
- BGP: All 3 nodes working (all 48 cores utilized)

**2. Higher Availability:**
- L2: Node crash = service interruption
- BGP: Node crash = no interruption

**3. Better Logs for Security/Debugging:**
- L2: You only see node IPs (useless)
- BGP: You see real client IPs (important for security)

**4. Enterprise Skills:**
- L2: Homelab solution (not in production)
- BGP: Real cloud practice (AWS, GCP, Azure use BGP)

---

## âš ï¸ Safe Migration - Zero Downtime Strategy

### **CRITICAL: Will Cilium Crash During Migration?**

**Answer:** No, Cilium itself will NOT crash - but your **services could become unreachable** if you migrate incorrectly!

---

### **âŒ DANGER - What Can Go Wrong?**

**Risky Migration Approach:**

```
Step 1: Disable L2 in values.yaml
   l2announcements:
     enabled: false  âŒ

Step 2: Enable BGP
   bgp:
     enabled: true

Step 3: ArgoCD sync

âŒ PROBLEM: If BGP doesn't work â†’ ALL Services DOWN!
âŒ Router has no routes â†’ Services unreachable
âŒ Downtime until you re-enable L2
```

---

### **âœ… SAFE Migration - Parallel Operation (Zero Downtime)**

The key is to run **L2 and BGP in parallel** during testing, then cut over once BGP is proven.

---

#### **Phase 1: Parallel Operation (L2 + BGP simultaneously)**

**Step 1 - Add BGP WITHOUT disabling L2:**

```yaml
# kubernetes/infrastructure/network/cilium/values.yaml

# âœ… L2 STAYS ACTIVE
l2announcements:
  enabled: true  # â† DO NOT disable!

# âœ… BGP ENABLED IN PARALLEL
bgp:
  enabled: true
  announce:
    loadbalancerIP: true
```

**Step 2 - Deploy BGP CRDs:**

```yaml
# kubernetes/infrastructure/network/cilium/kustomization.yaml
resources:
  # âœ… L2 STAYS ACTIVE
  - announce.yaml
  - ip-pool.yaml

  # âœ… BGP ENABLED IN PARALLEL
  - bgp-cluster-config.yaml
  - bgp-peer-config.yaml
  - bgp-advertisement.yaml
  - bgp-password-secret.yaml
```

**Result:**
- âœ… L2 announcements continue working (existing services OK)
- âœ… BGP peering is established (new capability)
- âœ… **Both systems run in parallel** â†’ Zero Risk

---

#### **Phase 2: Test BGP with New Service**

Create a test service to verify BGP works:

```yaml
# test-bgp-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: whoami-bgp-test
  namespace: default
  labels:
    bgp.cilium.io/ip-pool: default              # â† BGP IP Pool
    bgp.cilium.io/advertise-service: default    # â† BGP Advertisement
spec:
  type: LoadBalancer
  externalTrafficPolicy: Local  # â† Client IP preservation
  selector:
    app: whoami
  ports:
  - port: 80
```

**Verify BGP Works:**

```bash
# 1. Check BGP peering
kubectl exec -n kube-system ds/cilium -- cilium bgp peers
# Expected: Session State = established

# 2. Check BGP routes
kubectl exec -n kube-system ds/cilium -- cilium bgp routes advertised ipv4 unicast
# Expected: Service IP advertised from all 3 nodes

# 3. Test service reachable
curl http://172.20.10.X  # BGP IP
# Expected: HTTP 200, RemoteAddr shows real client IP
```

**If this works:**
- âœ… BGP works correctly
- âœ… ECMP Load Balancing active
- âœ… Client IP preservation working

---

#### **Phase 3: Disable L2 (only after successful tests)**

**ONLY when BGP is 100% working:**

```yaml
# kubernetes/infrastructure/network/cilium/values.yaml
l2announcements:
  enabled: false  # âœ… NOW safe to disable

bgp:
  enabled: true
```

```yaml
# kubernetes/infrastructure/network/cilium/kustomization.yaml
resources:
  # âŒ L2 disabled
  # - announce.yaml

  # âœ… BGP stays active
  - bgp-cluster-config.yaml
  - bgp-peer-config.yaml
  - bgp-advertisement.yaml
  - bgp-password-secret.yaml
  - ip-pool.yaml  # â† STAYS (reused by BGP)
```

---

### **ğŸš¨ Rollback Plan - If BGP Fails**

**Immediately revert to L2:**

```yaml
# values.yaml
l2announcements:
  enabled: true  # âœ… RE-ENABLE

bgp:
  enabled: false  # âŒ DISABLE
```

```yaml
# kustomization.yaml
resources:
  - announce.yaml  # âœ… RE-ENABLE
  # - bgp-*.yaml   # âŒ COMMENT OUT
```

```bash
# ArgoCD sync
argocd app sync infrastructure-cilium

# Services reachable again within 10-20 seconds
```

---

### **ğŸ“Š Migration Timeline - Zero Downtime**

```
Day 1 - Preparation
â”œâ”€ BGP YAML files created âœ…
â”œâ”€ UniFi Router BGP configured
â””â”€ L2 announcements: ACTIVE

Day 2 - Parallel Operation
â”œâ”€ BGP enabled (values.yaml: bgp.enabled: true)
â”œâ”€ L2 announcements: STILL ACTIVE
â”œâ”€ BGP peering: established
â””â”€ Test Service deployed

Day 3 - Verification
â”œâ”€ BGP routes advertised âœ…
â”œâ”€ ECMP Load Balancing working âœ…
â”œâ”€ Client IP preservation âœ…
â””â”€ All tests passed

Day 4 - Cutover
â”œâ”€ L2 announcements disabled
â”œâ”€ BGP takes over completely
â””â”€ Monitoring: All OK âœ…
```

---

### **âš ï¸ Important Points**

**1. Cilium Will NOT Crash:**
- BGP is just a configuration change
- Cilium pods stay running
- CNI continues to function

**2. Services Could Become Unreachable If:**
- âŒ You disable L2 BEFORE BGP works
- âŒ UniFi Router BGP is misconfigured
- âŒ ASN numbers don't match
- âŒ IP addresses are wrong

**3. Safe Migration Means:**
- âœ… Run L2 + BGP in parallel
- âœ… Verify BGP with test service
- âœ… Only then disable L2

---

### **ğŸ¯ Migration Checklist**

```
â–¡ BGP YAML files created (bgp-*.yaml)
â–¡ UniFi Router BGP configured (ASN 65001, Peers at 192.168.68.10-12)
â–¡ Cilium values.yaml: bgp.enabled: true (L2 STAYS enabled: true)
â–¡ kustomization.yaml: BGP files uncommented (announce.yaml STAYS)
â–¡ ArgoCD sync infrastructure-cilium
â–¡ Verify: cilium bgp peers â†’ Session State = established
â–¡ Test Service deployed (whoami-bgp-test)
â–¡ Verify: Service reachable via BGP IP
â–¡ Verify: Client IP preservation works
â–¡ 24h Monitoring: No issues
â–¡ Disable L2 (values.yaml: l2announcements.enabled: false)
â–¡ Comment out announce.yaml in kustomization.yaml
â–¡ ArgoCD sync
â–¡ Final Verify: All services still reachable
```

---

### **TL;DR**

Cilium won't crash, but you MUST run **L2 and BGP in parallel** while testing. Only disable L2 when BGP is 100% proven. Otherwise all services go down! ğŸš¨

---

## ğŸ¯ Migration Strategy

### Phase 1: Preparation (5 minutes)

1. **Check UniFi router capabilities:**
   - UniFi Dream Machine Pro/SE supports BGP
   - UniFi Dream Router does NOT support BGP (requires UDM-Pro)
   - Check: Console Settings â†’ Gateway â†’ Advanced â†’ BGP

2. **Choose ASN numbers:**
   - Cilium Cluster: **ASN 65000** (private ASN range 64512-65534)
   - UniFi Router: **ASN 65001** (different ASN for eBGP)

3. **Plan IP addressing:**
   - LoadBalancer Pool: **192.168.68.150-170** (existing, no change)
   - BGP Peer IP: **192.168.68.1** (UniFi router)
   - Node IPs: **192.168.68.x** (existing Talos nodes)

### Phase 2: BGP Configuration (20 minutes)

1. **Create BGP YAML files:**
   ```bash
   # Already created in this directory:
   # - bgp-cluster-config.yaml
   # - bgp-peer-config.yaml
   # - bgp-advertisement.yaml
   # - bgp-password-secret.yaml (optional)
   ```

2. **Enable in kustomization.yaml:**
   ```yaml
   resources:
     # âŒ DISABLE L2:
     # - announce.yaml

     # âœ… ENABLE BGP:
     - bgp-cluster-config.yaml
     - bgp-peer-config.yaml
     - bgp-advertisement.yaml
     - bgp-password-secret.yaml  # Optional
     - ip-pool.yaml  # Keep existing
   ```

3. **Update values.yaml:**
   ```yaml
   # âŒ DISABLE L2:
   l2announcements:
     enabled: false

   # âœ… ENABLE BGP:
   bgp:
     enabled: true
     announce:
       loadbalancerIP: true
       podCIDR: false  # Only announce LoadBalancer IPs
   ```

### Phase 3: UniFi Router BGP Setup (10 minutes)

**UniFi Console â†’ Settings â†’ Gateway â†’ Advanced â†’ BGP:**

1. **Enable BGP:** ON
2. **Local ASN:** 65001
3. **Router ID:** 192.168.68.1

4. **Add BGP Neighbor (repeat for each Talos node):**
   - **Neighbor IP:** 192.168.68.10 (node1)
   - **Remote ASN:** 65000
   - **Password:** (optional, matches `bgp-password-secret.yaml`)
   - **Enable:** ON

5. **Add BGP Neighbor:** 192.168.68.11 (node2)
6. **Add BGP Neighbor:** 192.168.68.12 (node3)

### Phase 4: Deploy & Verify (10 minutes)

1. **Deploy BGP configuration:**
   ```bash
   # ArgoCD will auto-sync, or manually:
   kubectl apply -k kubernetes/infrastructure/network/cilium/
   ```

2. **Verify BGP peering:**
   ```bash
   # Check Cilium BGP status:
   kubectl exec -n kube-system ds/cilium -- cilium bgp peers

   # Expected output:
   # Node       Local AS  Peer AS  Peer Address     Session State
   # node1      65000     65001    192.168.68.1     established
   # node2      65000     65001    192.168.68.1     established
   # node3      65000     65001    192.168.68.1     established
   ```

3. **Verify routes on UniFi:**
   ```bash
   # SSH to UniFi router:
   show ip bgp summary

   # Should show 3 established sessions (one per node)
   # Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd
   # 192.168.68.10   4 65000      10      12        0    0    0 00:05:00        5
   # 192.168.68.11   4 65000       8      10        0    0    0 00:05:00        5
   # 192.168.68.12   4 65000       7       9        0    0    0 00:05:00        5
   ```

4. **Test LoadBalancer service:**
   ```bash
   # Create test service:
   kubectl create deployment nginx --image=nginx
   kubectl expose deployment nginx --type=LoadBalancer --port=80

   # Check external IP:
   kubectl get svc nginx
   # NAME    TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)        AGE
   # nginx   LoadBalancer   10.96.100.200   192.168.68.150   80:32000/TCP   1m

   # Test from external client:
   curl http://192.168.68.150
   # Should see nginx welcome page
   ```

5. **Verify ECMP load balancing:**
   ```bash
   # On UniFi router:
   show ip route 192.168.68.150

   # Should show 3 next-hops (ECMP):
   # S>  192.168.68.150/32 [20/0] via 192.168.68.10, eth0, 00:05:00
   #                              via 192.168.68.11, eth0, 00:05:00
   #                              via 192.168.68.12, eth0, 00:05:00
   ```

### Phase 5: Rollback Plan (if needed)

If BGP doesn't work, rollback to L2:

1. **Disable BGP in kustomization.yaml:**
   ```yaml
   resources:
     - announce.yaml  # âœ… RE-ENABLE
     # - bgp-*.yaml   # âŒ DISABLE
   ```

2. **Re-enable L2 in values.yaml:**
   ```yaml
   l2announcements:
     enabled: true  # âœ… RE-ENABLE
   bgp:
     enabled: false  # âŒ DISABLE
   ```

3. **Disable BGP on UniFi:**
   - Console â†’ Settings â†’ Gateway â†’ Advanced â†’ BGP â†’ OFF

4. **ArgoCD sync:**
   ```bash
   argocd app sync infrastructure-cilium
   ```

---

## ğŸ“Š Comparison: L2 vs BGP

| Feature | L2 Announcements | BGP Peering |
|---------|-----------------|-------------|
| **Protocol** | ARP (Layer 2) | BGP (Layer 3) |
| **Load Balancing** | No (failover only) | Yes (ECMP) |
| **Client Source IP** | Lost (SNAT) | Preserved |
| **Failure Mode** | Single node down = outage | Automatic failover |
| **Router Config** | None | BGP peers required |
| **YAML Files** | 2-3 files | 5-6 files |
| **Setup Time** | 10 minutes | 30-45 minutes |
| **Scalability** | Single cluster only | Multi-cluster ready |
| **Industry Standard** | No | Yes (AWS, GCP, Azure) |
| **Broadcast Traffic** | High (ARP) | Low (BGP updates) |

---

## ğŸ¤ Interview Talking Points

**"Why did you migrate from L2 to BGP?"**

> "I migrated from Cilium L2 Announcements to BGP peering for several reasons:
>
> 1. **True Load Balancing:** L2 only provides failover to a single node. BGP enables ECMP load balancing across all 3 nodes, distributing traffic evenly.
>
> 2. **Client Source IP Preservation:** With L2, the client IP gets SNAT'd and lost. BGP with `externalTrafficPolicy: Local` preserves the real client IP for logging and security policies.
>
> 3. **Enterprise Skills:** BGP is the industry standard used by AWS, GCP, Azure. Learning BGP in my homelab prepares me for real-world cloud infrastructure.
>
> 4. **Scalability:** BGP prepares my homelab for multi-cluster setups and service mesh architectures.
>
> 5. **No Single Point of Failure:** L2 pins services to one node. BGP distributes load, so a node failure doesn't cause service disruption.
>
> The setup required creating 4 new CRDs (`CiliumBGPClusterConfig`, `CiliumBGPPeerConfig`, `CiliumBGPAdvertisement`) and configuring my UniFi router with ASN 65001 to peer with my cluster (ASN 65000). The migration took about 45 minutes including testing."

---

## ğŸ“š References

- [Cilium BGP Documentation](https://docs.cilium.io/en/stable/network/bgp-control-plane/)
- [UniFi BGP Configuration](https://help.ui.com/hc/en-us/articles/115000166827-UniFi-UDM-USG-Advanced-Routing-BGP)
- [BGP ECMP Load Balancing](https://docs.cilium.io/en/stable/network/bgp-control-plane/#ecmp-load-balancing)
- [ExternalTrafficPolicy: Local](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip)

---

## ğŸ† Status

- **Current:** L2 Announcements (ARP) - Working
- **Target:** BGP Peering - Ready to migrate (YAML files created)
- **Decision:** Migrate when UniFi router BGP is confirmed available
