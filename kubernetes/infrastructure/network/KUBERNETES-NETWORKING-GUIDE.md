# ğŸŒ Kubernetes Networking Complete Guide - CNI, Services, Ingress

## ğŸ“– Was ist Kubernetes Networking?

**Kubernetes Networking** ist das System das ermÃ¶glicht dass **Pods miteinander kommunizieren**, **Services erreichbar** sind, und **externe Traffic** zu Pods routed wird.

### ğŸ¤” Das Kubernetes Networking Problem

Kubernetes muss 4 grundlegende Networking Herausforderungen lÃ¶sen:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4 Kubernetes Networking Requirements              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚ 1ï¸âƒ£ Pod-to-Pod Communication                       â”‚
â”‚    Pod A (10.244.1.5) â†’ Pod B (10.244.2.10)       â”‚
â”‚    Requirement: Direct IP connectivity (no NAT!)  â”‚
â”‚                                                    â”‚
â”‚ 2ï¸âƒ£ Pod-to-Service Communication                    â”‚
â”‚    Pod â†’ Service (ClusterIP) â†’ Backend Pods       â”‚
â”‚    Requirement: Load balancing, service discovery â”‚
â”‚                                                    â”‚
â”‚ 3ï¸âƒ£ External-to-Service Communication               â”‚
â”‚    Internet â†’ LoadBalancer/Ingress â†’ Pods        â”‚
â”‚    Requirement: Public IP, TLS termination        â”‚
â”‚                                                    â”‚
â”‚ 4ï¸âƒ£ Pod-to-External Communication                   â”‚
â”‚    Pod (10.244.1.5) â†’ api.github.com              â”‚
â”‚    Requirement: NAT, egress routing               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Kubernetes delegiert diese Aufgaben an:**
- **CNI Plugin** (Container Network Interface) â†’ LÃ¶st 1ï¸âƒ£ & 4ï¸âƒ£
- **kube-proxy** (oder CNI) â†’ LÃ¶st 2ï¸âƒ£
- **Ingress Controller** / **LoadBalancer** â†’ LÃ¶st 3ï¸âƒ£

---

## ğŸ—ï¸ Die 3 Networking Layer

### **Layer 1: CNI (Container Network Interface)**

**Was ist CNI?**

CNI ist ein **Plugin System** das Pod networking konfiguriert. Wenn Kubernetes einen Pod erstellt, ruft es das CNI Plugin mit `ADD <pod-name>`:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CNI Plugin Execution Flow                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚ 1. kubelet creates Pod namespace              â”‚
â”‚    â””â”€ Network namespace: /var/run/netns/pod1  â”‚
â”‚                                                â”‚
â”‚ 2. kubelet calls CNI plugin                   â”‚
â”‚    â””â”€ Command: ADD pod1                       â”‚
â”‚                                                â”‚
â”‚ 3. CNI plugin creates veth pair               â”‚
â”‚    â””â”€ eth0 (in Pod) â†” vethXYZ (in host)      â”‚
â”‚                                                â”‚
â”‚ 4. CNI assigns IP address                     â”‚
â”‚    â””â”€ IPAM: 10.244.1.5/24                     â”‚
â”‚                                                â”‚
â”‚ 5. CNI configures routing                     â”‚
â”‚    â””â”€ Route: 10.244.0.0/16 â†’ cilium_host      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CNI Plugin Options:**

| CNI Plugin | Type | Performance | Features |
|-----------|------|-------------|----------|
| **Cilium** | eBPF-based | â­â­â­â­â­ Best | L7 policies, WireGuard, kube-proxy replacement |
| **Calico** | iptables | â­â­â­ Good | BGP routing, Network Policies |
| **Flannel** | Overlay (VXLAN) | â­â­ OK | Simple, easy setup |
| **Weave** | Overlay | â­â­ OK | Mesh networking, encryption |

**Our Choice: Cilium**

Reasons:
- âœ… eBPF = Native kernel datapath (fastest)
- âœ… kube-proxy replacement (no extra process)
- âœ… L7 visibility (HTTP/DNS inspection)
- âœ… WireGuard encryption (Zero-Trust)
- âœ… Production-grade (Google GKE, AWS EKS)

---

### **Layer 2: Services & kube-proxy**

**Was ist ein Kubernetes Service?**

Ein **Service** ist eine **stabile IP + DNS Name** fÃ¼r eine Gruppe von Pods (die sich Ã¤ndern kÃ¶nnen).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Problem: Pods sind ephemeral                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚ Deployment: frontend (3 replicas)             â”‚
â”‚ â”œâ”€ Pod 1: 10.244.1.5  â† deleted! âŒ           â”‚
â”‚ â”œâ”€ Pod 2: 10.244.2.8  â† running âœ…            â”‚
â”‚ â””â”€ Pod 3: 10.244.3.12 â† NEW pod! (new IP)     â”‚
â”‚                                                â”‚
â”‚ âŒ Hard-code IP 10.244.1.5? â†’ Breaks!         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Solution: Service (stable IP)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚ Service: frontend (ClusterIP: 10.96.123.45)   â”‚
â”‚ â”œâ”€ Backend Pod 1: 10.244.2.8  âœ…              â”‚
â”‚ â””â”€ Backend Pod 2: 10.244.3.12 âœ…              â”‚
â”‚                                                â”‚
â”‚ âœ… Always use Service IP: 10.96.123.45        â”‚
â”‚ âœ… DNS: frontend.default.svc.cluster.local    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Service Types:**

```yaml
# 1. ClusterIP (default) - Internal only
apiVersion: v1
kind: Service
metadata:
  name: checkout
spec:
  type: ClusterIP  # Only accessible within cluster
  selector:
    app: checkout
  ports:
    - port: 5050
      targetPort: 5050

---

# 2. NodePort - Exposes on each Node's IP
apiVersion: v1
kind: Service
metadata:
  name: checkout
spec:
  type: NodePort
  selector:
    app: checkout
  ports:
    - port: 5050
      targetPort: 5050
      nodePort: 30050  # Access via <node-ip>:30050

---

# 3. LoadBalancer - Cloud LB or MetalLB/Cilium L2 announcements
apiVersion: v1
kind: Service
metadata:
  name: grafana
spec:
  type: LoadBalancer
  selector:
    app: grafana
  ports:
    - port: 80
      targetPort: 3000
  # Assigns external IP: 192.168.68.150 (from CiliumLoadBalancerIPPool)

---

# 4. ExternalName - DNS CNAME alias
apiVersion: v1
kind: Service
metadata:
  name: external-db
spec:
  type: ExternalName
  externalName: mysql.rds.amazonaws.com  # Points to external service
```

---

### **kube-proxy vs Cilium eBPF**

**Traditional kube-proxy (iptables mode):**

```
Client Pod â†’ Service IP (10.96.123.45:5050)
     â”‚
     â–¼
kube-proxy (watches Services, updates iptables)
     â”‚
     â–¼
iptables NAT rules:
â”œâ”€ -A KUBE-SVC-XYZ -m statistic --mode random --probability 0.5 -j KUBE-SEP-1
â”œâ”€ -A KUBE-SVC-XYZ -j KUBE-SEP-2
â”‚
â”œâ”€ KUBE-SEP-1: DNAT to 10.244.1.5:5050  â† Backend Pod 1
â””â”€ KUBE-SEP-2: DNAT to 10.244.2.8:5050  â† Backend Pod 2
```

**Problems:**
- âŒ **O(n) iptables lookup** - 1000 Services = 10K+ rules
- âŒ **Extra process** - kube-proxy consumes CPU/RAM
- âŒ **No connection tracking** - Session affinity issues

**Cilium eBPF (kube-proxy replacement):**

```
Client Pod â†’ Service IP (10.96.123.45:5050)
     â”‚
     â–¼
eBPF program (attached to veth interface)
     â”‚
     â–¼
eBPF map lookup (O(1)!):
â”œâ”€ Key: 10.96.123.45:5050
â””â”€ Value: [10.244.1.5:5050, 10.244.2.8:5050]  â† Backend Pods
     â”‚
     â–¼
Maglev Consistent Hashing â†’ Select backend
     â”‚
     â–¼
Direct routing to 10.244.1.5:5050 (no NAT!)
```

**Benefits:**
- âœ… **O(1) lookup** - Constant time, regardless of # of Services
- âœ… **No kube-proxy** - One less process to manage
- âœ… **Connection tracking** - Session affinity via Maglev
- âœ… **Better performance** - 30% lower latency

**Our Setup:**

```yaml
# File: infrastructure/network/cilium/values.yaml
kubeProxyReplacement: true  # âœ… No kube-proxy!

loadBalancer:
  algorithm: maglev  # Consistent hashing for sticky sessions
```

---

### **Layer 3: Ingress & Gateway API**

**Was ist Ingress?**

**Ingress** ermÃ¶glicht **HTTP/HTTPS routing** von **auÃŸen** zu **internen Services**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Without Ingress (NodePort or LoadBalancer)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚ âŒ Problem: Need unique IP per Service!       â”‚
â”‚                                                â”‚
â”‚ Service 1: grafana       â†’ 192.168.68.151:80  â”‚
â”‚ Service 2: hubble-ui     â†’ 192.168.68.152:80  â”‚
â”‚ Service 3: argocd-server â†’ 192.168.68.153:80  â”‚
â”‚                                                â”‚
â”‚ â†’ Waste of IPs! (Limited pool: .150-.170)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ With Ingress (Shared LoadBalancer)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚ âœ… 1 Shared IP: 192.168.68.150                â”‚
â”‚                                                â”‚
â”‚ grafana.timourhomelab.org      â†’ grafana:80   â”‚
â”‚ hubble.timourhomelab.org       â†’ hubble-ui:80 â”‚
â”‚ argocd.timourhomelab.org       â†’ argocd:443   â”‚
â”‚                                                â”‚
â”‚ â†’ HTTP Host-based routing!                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ingress Example:**

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana
  namespace: monitoring
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod  # Auto TLS cert
spec:
  ingressClassName: cilium  # Use Cilium Ingress Controller
  tls:
    - hosts:
        - grafana.timourhomelab.org
      secretName: grafana-tls  # cert-manager creates this Secret
  rules:
    - host: grafana.timourhomelab.org
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: grafana
                port:
                  number: 80
```

**How Ingress Works:**

```
Internet
  â”‚
  â–¼
DNS: grafana.timourhomelab.org â†’ 192.168.68.150
  â”‚
  â–¼
Cilium Ingress Controller (Envoy proxy)
  â”‚
  â”œâ”€ TLS termination (cert-manager cert)
  â”œâ”€ HTTP Host header check: "grafana.timourhomelab.org"
  â””â”€ Route to Service: grafana:80
       â”‚
       â–¼
  Service: grafana (ClusterIP: 10.96.50.10:80)
       â”‚
       â–¼
  Pod: grafana-abc123 (10.244.3.45:3000)
```

---

### **Gateway API (Modern Ingress)**

**Was ist Gateway API?**

Gateway API ist der **Nachfolger von Ingress** - mehr features, bessere multi-tenancy.

```yaml
# Gateway API Example
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: cilium-gateway
  namespace: istio-system
spec:
  gatewayClassName: cilium  # Cilium Gateway Controller
  listeners:
    - name: http
      protocol: HTTP
      port: 80
    - name: https
      protocol: HTTPS
      port: 443
      tls:
        mode: Terminate
        certificateRefs:
          - name: wildcard-tls  # cert-manager Secret

---

apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: grafana
  namespace: monitoring
spec:
  parentRefs:
    - name: cilium-gateway
      namespace: istio-system
  hostnames:
    - grafana.timourhomelab.org
  rules:
    - backendRefs:
        - name: grafana
          port: 80
```

**Gateway API vs Ingress:**

| Feature | Ingress | Gateway API |
|---------|---------|-------------|
| **Multi-tenancy** | âŒ Single namespace | âœ… Cross-namespace refs |
| **Protocol support** | HTTP/HTTPS only | HTTP, HTTPS, TCP, UDP, gRPC |
| **Traffic splitting** | âŒ Limited | âœ… Weighted routing (Canary) |
| **Header matching** | âŒ No | âœ… Yes (advanced routing) |
| **Status reporting** | âŒ Basic | âœ… Rich status conditions |

**Our Setup:**

```yaml
# File: infrastructure/network/cilium/values.yaml
gatewayAPI:
  enabled: true
  enableAlpn: true  # HTTP/2 support
  enableAppProtocol: true  # GEP-1911 (protocol detection)
```

---

## ğŸ”€ Traffic Flow Examples

### **Example 1: Pod-to-Pod (Same Node)**

```
Pod A (10.244.1.5) â†’ Pod B (10.244.1.10)  [Same Node]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node: worker-1                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚  Pod A (eth0: 10.244.1.5)             â”‚
â”‚    â”‚                                   â”‚
â”‚    â–¼                                   â”‚
â”‚  vethABC (host side)                  â”‚
â”‚    â”‚                                   â”‚
â”‚    â–¼                                   â”‚
â”‚  eBPF program (cilium_from_container) â”‚
â”‚    â”‚                                   â”‚
â”‚    â”‚ Check: Destination 10.244.1.10   â”‚
â”‚    â”‚ â†’ Same node? YES!                â”‚
â”‚    â”‚ â†’ Direct forward (no routing)    â”‚
â”‚    â”‚                                   â”‚
â”‚    â–¼                                   â”‚
â”‚  vethXYZ (Pod B's veth)               â”‚
â”‚    â”‚                                   â”‚
â”‚    â–¼                                   â”‚
â”‚  Pod B (eth0: 10.244.1.10)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Performance: ~0.5 microseconds (eBPF redirect)
```

---

### **Example 2: Pod-to-Pod (Different Nodes)**

```
Pod A (10.244.1.5, Node: worker-1) â†’ Pod B (10.244.2.10, Node: worker-2)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node: worker-1       â”‚         â”‚ Node: worker-2       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      â”‚         â”‚                      â”‚
â”‚ Pod A (10.244.1.5)  â”‚         â”‚ Pod B (10.244.2.10) â”‚
â”‚   â”‚                  â”‚         â”‚   â”‚                  â”‚
â”‚   â–¼                  â”‚         â”‚   â–¼                  â”‚
â”‚ vethABC              â”‚         â”‚ vethXYZ              â”‚
â”‚   â”‚                  â”‚         â”‚   â”‚                  â”‚
â”‚   â–¼                  â”‚         â”‚   â–¼                  â”‚
â”‚ eBPF program         â”‚         â”‚ eBPF program         â”‚
â”‚   â”‚                  â”‚         â”‚   â”‚                  â”‚
â”‚   â”‚ Dest: 10.244.2.10â”‚         â”‚   â”‚                  â”‚
â”‚   â”‚ â†’ Different node!â”‚         â”‚   â”‚                  â”‚
â”‚   â”‚                  â”‚         â”‚   â”‚                  â”‚
â”‚   â–¼                  â”‚         â”‚   â–¼                  â”‚
â”‚ eth0 (Node IP)       â”‚         â”‚ eth0 (Node IP)       â”‚
â”‚   â”‚                  â”‚         â”‚   â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚                  â”‚
â”‚       WireGuard      â”‚ Encryptedâ”‚   â”‚                  â”‚
â”‚       Tunnel         â”‚  Traffic â”‚   â”‚                  â”‚
â”‚                      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â–º   â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Steps:
1. eBPF checks: Destination on different node
2. Encapsulation: WireGuard encryption (if enabled)
3. Route: Via native routing (no VXLAN!)
4. Destination node: eBPF decrypts, forwards to Pod B

Performance: ~1-2 microseconds + network latency
```

---

### **Example 3: Pod â†’ Service â†’ Backend Pods**

```
Frontend Pod (10.244.1.5) â†’ checkout Service (10.96.123.45:5050)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Frontend Pod sends HTTP request       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Source: 10.244.1.5:52341                      â”‚
â”‚ Dest:   10.96.123.45:5050  â† Service IP!      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: eBPF program intercepts                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ eBPF map lookup:                               â”‚
â”‚ Key: 10.96.123.45:5050                        â”‚
â”‚ Value: [10.244.2.10:5050, 10.244.3.15:5050]   â”‚
â”‚         â†‘ Backend Pod 1   â†‘ Backend Pod 2     â”‚
â”‚                                                â”‚
â”‚ Maglev Hash: source IP + port â†’ Backend 1     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Direct routing to Backend Pod         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ New Dest: 10.244.2.10:5050  â† Backend Pod     â”‚
â”‚ No DNAT! (Source IP preserved: 10.244.1.5)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Backend Pod receives request          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Backend sees: Source IP = 10.244.1.5          â”‚
â”‚ â†’ Real client IP preserved! (No NAT!)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Difference vs iptables:**

```
iptables (DNAT):
Frontend (10.244.1.5) â†’ Service (10.96.123.45)
  â†’ iptables DNAT â†’ Backend sees Source: 10.96.123.45 âŒ
  â†’ Lost real client IP!

Cilium eBPF (Direct Server Return):
Frontend (10.244.1.5) â†’ Service (10.96.123.45)
  â†’ eBPF direct route â†’ Backend sees Source: 10.244.1.5 âœ…
  â†’ Real client IP preserved!
```

---

### **Example 4: Internet â†’ Ingress â†’ Pod**

```
Internet â†’ grafana.timourhomelab.org (192.168.68.150)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: DNS Resolution                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dig grafana.timourhomelab.org                 â”‚
â”‚ â†’ A record: 192.168.68.150                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: ARP/L2 Announcement                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Router sends ARP: Who has 192.168.68.150?     â”‚
â”‚ Cilium L2 Announcement replies:               â”‚
â”‚ â†’ MAC address: aa:bb:cc:dd:ee:ff (worker-1)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Traffic arrives at worker-1           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cilium Ingress Controller (Envoy)             â”‚
â”‚ â”œâ”€ TLS Termination (cert-manager cert)        â”‚
â”‚ â”œâ”€ HTTP Host header: grafana.timourhomelab.orgâ”‚
â”‚ â””â”€ Matches Ingress rule â†’ Route to grafana:80 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Service â†’ Pod routing                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Service: grafana (ClusterIP: 10.96.50.10:80)  â”‚
â”‚ eBPF map lookup â†’ Backend Pod: 10.244.3.45    â”‚
â”‚ Direct route to Pod                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š IP Address Ranges (Our Cluster)

### **Pod CIDR: 10.244.0.0/16**

```
Total IPs: 65,536 (2^16)
Per-Node allocation: /24 (256 IPs per Node)

Node allocations:
â”œâ”€ ctrl-0:   10.244.0.0/24   (256 IPs)
â”œâ”€ worker-1: 10.244.1.0/24   (256 IPs)
â”œâ”€ worker-2: 10.244.2.0/24   (256 IPs)
â”œâ”€ worker-3: 10.244.3.0/24   (256 IPs)
â”œâ”€ worker-4: 10.244.4.0/24   (256 IPs)
â”œâ”€ worker-5: 10.244.5.0/24   (256 IPs)
â””â”€ worker-6: 10.244.6.0/24   (256 IPs)

Max Pods per Node: 254 (1 IP reserved for gateway)
Total Cluster Capacity: 7 nodes Ã— 254 pods = 1,778 pods
```

**IPAM Mode**: `kubernetes` (uses Node `spec.podCIDR`)

```yaml
# File: infrastructure/network/cilium/values.yaml
ipam:
  mode: kubernetes  # Read podCIDR from Node spec
```

---

### **Service CIDR: 10.96.0.0/12**

```
Total IPs: 1,048,576 (2^20)
Range: 10.96.0.0 - 10.111.255.255

Reserved IPs:
â”œâ”€ 10.96.0.1       â†’ Kubernetes API Server (default)
â”œâ”€ 10.96.0.10      â†’ CoreDNS Service
â””â”€ 10.96.123.45    â†’ Example: checkout Service

Max Services: ~1 million (more than enough!)
```

**Service IP allocation**: Managed by Kubernetes API Server (not CNI)

---

### **LoadBalancer IP Pool: 192.168.68.150-170**

```
CiliumLoadBalancerIPPool (L2 Announcements)

Available IPs: 21 (192.168.68.150 - .170)
Used: ~8
Free: ~13

Allocated IPs:
â”œâ”€ 192.168.68.150 â†’ Cilium Ingress (shared for all Ingresses)
â”œâ”€ 192.168.68.151 â†’ Grafana LoadBalancer
â”œâ”€ 192.168.68.152 â†’ Hubble UI LoadBalancer
â””â”€ 192.168.68.153 â†’ ArgoCD Server LoadBalancer
```

**File**: `infrastructure/network/cilium/ip-pool.yaml`

```yaml
apiVersion: cilium.io/v2alpha1
kind: CiliumLoadBalancerIPPool
metadata:
  name: ip-pool
  namespace: kube-system
spec:
  blocks:
    - start: 192.168.68.150
      stop: 192.168.68.170
```

**L2 Announcements** (ARP/NDP):

```yaml
apiVersion: cilium.io/v2alpha1
kind: CiliumL2AnnouncementPolicy
metadata:
  name: default-l2-announcement-policy
  namespace: kube-system
spec:
  externalIPs: true
  loadBalancerIPs: true
```

**How it works:**

```
1. Service type=LoadBalancer created
2. Cilium allocates IP from pool: 192.168.68.151
3. Cilium sends ARP announcement: "I have 192.168.68.151!"
4. Router updates ARP table: 192.168.68.151 â†’ worker-1 MAC
5. Traffic to .151 arrives at worker-1
6. Cilium routes to Service backend Pods
```

---

## ğŸ”’ Network Policies

### **What are Network Policies?**

**Network Policies** sind **Firewall rules** fÃ¼r Pods - ermÃ¶glichen **Zero-Trust Networking**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Without Network Policies (Default: Allow All) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚ âŒ Frontend can access Database directly!     â”‚
â”‚ âŒ Any Pod can access any other Pod!          â”‚
â”‚ âŒ No isolation between namespaces!           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ With Network Policies (Zero-Trust)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚ âœ… Frontend â†’ Checkout (allowed)              â”‚
â”‚ âœ… Checkout â†’ Database (allowed)              â”‚
â”‚ âŒ Frontend â†’ Database (denied!)              â”‚
â”‚ âœ… Explicit allow-list model                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example Network Policy (L3/L4):**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-policy
  namespace: boutique-dev
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: istio-system  # Allow Istio Gateway
      ports:
        - protocol: TCP
          port: 8080
  egress:
    - to:
        - podSelector:
            matchLabels:
              app: checkout  # Frontend can call checkout
      ports:
        - protocol: TCP
          port: 5050
    - to:  # Allow DNS
        - namespaceSelector:
            matchLabels:
              name: kube-system
        - podSelector:
            matchLabels:
              k8s-app: kube-dns
      ports:
        - protocol: UDP
          port: 53
```

---

### **Cilium L7 Network Policies**

**Cilium extends Kubernetes Network Policies** mit **L7 (HTTP/DNS) filtering**:

```yaml
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: frontend-l7-policy
  namespace: boutique-dev
spec:
  endpointSelector:
    matchLabels:
      app: frontend
  egress:
    - toEndpoints:
        - matchLabels:
            app: checkout
      toPorts:
        - ports:
            - port: "5050"
              protocol: TCP
          rules:
            http:
              - method: "GET"    # âœ… Allow GET /cart
                path: "/cart"
              - method: "POST"   # âœ… Allow POST /checkout
                path: "/checkout"
      # âŒ Block: POST /admin (not in allow-list)
```

**L7 Policy Enforcement Flow:**

```
Frontend Pod â†’ checkout:5050 (HTTP POST /checkout)
      â”‚
      â–¼
eBPF checks: Is L7 policy attached? YES!
      â”‚
      â–¼
eBPF redirects to Envoy proxy (L7 inspection)
      â”‚
      â–¼
Envoy parses HTTP:
â”œâ”€ Method: POST
â”œâ”€ Path: /checkout
â””â”€ Matches L7 rule? YES! â†’ ALLOW
      â”‚
      â–¼
Forward to Backend Pod: checkout (10.244.2.10:5050)
```

**FQDN-based Policy (DNS):**

```yaml
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-github-api
spec:
  endpointSelector:
    matchLabels:
      app: frontend
  egress:
    - toFQDNs:
        - matchName: "api.github.com"  # Only GitHub API allowed
    - toFQDNs:
        - matchPattern: "*.google.com"  # Wildcard support
```

**How FQDN policies work:**

```
1. Frontend Pod: DNS query for api.github.com
2. Cilium DNS Proxy intercepts query
3. Cilium resolves: api.github.com â†’ 140.82.121.6
4. Cilium adds temporary Network Policy:
   - Allow egress to 140.82.121.6:443 (TTL-based)
5. Frontend Pod connects to 140.82.121.6:443 âœ…
6. After DNS TTL expires, rule is removed
```

---

## ğŸ” DNS in Kubernetes

### **CoreDNS (Cluster DNS)**

**CoreDNS** ist der **DNS Server** fÃ¼r Kubernetes - lÃ¶st Service Namen zu ClusterIPs auf.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DNS Resolution Flow                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚ Pod (10.244.1.5)                              â”‚
â”‚   â”‚                                            â”‚
â”‚   â”‚ DNS query: checkout.default.svc.cluster.localâ”‚
â”‚   â”‚                                            â”‚
â”‚   â–¼                                            â”‚
â”‚ /etc/resolv.conf:                             â”‚
â”‚   nameserver 10.96.0.10  â† CoreDNS Service IP â”‚
â”‚   search default.svc.cluster.local            â”‚
â”‚   â”‚                                            â”‚
â”‚   â–¼                                            â”‚
â”‚ CoreDNS Pod (10.244.0.15)                     â”‚
â”‚   â”‚                                            â”‚
â”‚   â”‚ Lookup: checkout.default.svc.cluster.localâ”‚
â”‚   â”‚ â†’ ClusterIP: 10.96.123.45                 â”‚
â”‚   â”‚                                            â”‚
â”‚   â–¼                                            â”‚
â”‚ Response: 10.96.123.45                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**DNS Naming Convention:**

```
<service-name>.<namespace>.svc.cluster.local

Examples:
â”œâ”€ checkout.default.svc.cluster.local       â†’ 10.96.123.45
â”œâ”€ grafana.monitoring.svc.cluster.local     â†’ 10.96.50.10
â””â”€ kube-dns.kube-system.svc.cluster.local   â†’ 10.96.0.10

Shortcuts (from same namespace):
â”œâ”€ checkout                 â†’ Works! (same namespace)
â”œâ”€ checkout.default         â†’ Works! (explicit namespace)
â””â”€ checkout.default.svc     â†’ Works! (full without .cluster.local)
```

**Headless Services (StatefulSet DNS):**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
spec:
  clusterIP: None  # Headless Service!
  selector:
    app: elasticsearch
  ports:
    - port: 9200

---

# DNS records for individual Pods:
elasticsearch-0.elasticsearch.observability.svc.cluster.local â†’ 10.244.2.15
elasticsearch-1.elasticsearch.observability.svc.cluster.local â†’ 10.244.3.20
elasticsearch-2.elasticsearch.observability.svc.cluster.local â†’ 10.244.4.25
```

---

## ğŸ› ï¸ Troubleshooting Common Issues

### **1. Pod Cannot Resolve Service DNS**

**Symptom:**

```bash
kubectl exec -it frontend-abc123 -- curl http://checkout:5050
# Error: Could not resolve host: checkout
```

**Diagnose:**

```bash
# 1. Check CoreDNS is running
kubectl get pods -n kube-system -l k8s-app=kube-dns

# 2. Check Pod's /etc/resolv.conf
kubectl exec -it frontend-abc123 -- cat /etc/resolv.conf
# Expected:
# nameserver 10.96.0.10
# search default.svc.cluster.local svc.cluster.local cluster.local

# 3. Test DNS query
kubectl exec -it frontend-abc123 -- nslookup checkout
# Expected: Should return 10.96.123.45

# 4. Check Service exists
kubectl get svc checkout
```

**Common Causes:**

```
âŒ CoreDNS Pods not running
   â†’ Check: kubectl get pods -n kube-system -l k8s-app=kube-dns
   â†’ Solution: Restart CoreDNS deployment

âŒ NetworkPolicy blocking DNS (port 53/UDP)
   â†’ Check: NetworkPolicy has egress rule for kube-dns
   â†’ Solution: Add DNS egress rule

âŒ Wrong Service name or namespace
   â†’ Use: <service>.<namespace>.svc.cluster.local
```

---

### **2. Service IP Not Responding**

**Symptom:**

```bash
kubectl exec -it frontend-abc123 -- curl http://10.96.123.45:5050
# Error: Connection timeout
```

**Diagnose:**

```bash
# 1. Check Service has Endpoints
kubectl get endpoints checkout
# Expected: Should show backend Pod IPs (10.244.x.y:5050)

# 2. Check backend Pods are running
kubectl get pods -l app=checkout

# 3. Check Cilium service mapping
kubectl exec -n kube-system ds/cilium -- cilium service list | grep 10.96.123.45

# 4. Test direct Pod IP connectivity
kubectl exec -it frontend-abc123 -- curl http://10.244.2.10:5050
```

**Common Causes:**

```
âŒ No backend Pods running
   â†’ Check: kubectl get pods -l app=checkout
   â†’ Solution: Scale deployment or fix CrashLoopBackOff

âŒ Service selector doesn't match Pod labels
   â†’ Check: kubectl get svc checkout -o yaml
   â†’ Solution: Fix selector to match Pod labels

âŒ eBPF service map not updated
   â†’ Check: cilium service list
   â†’ Solution: Restart Cilium agent
```

---

### **3. Ingress Not Reachable from Internet**

**Symptom:**

```bash
curl https://grafana.timourhomelab.org
# Error: Connection timeout
```

**Diagnose:**

```bash
# 1. Check Ingress has IP assigned
kubectl get ingress -n monitoring grafana
# Expected: ADDRESS should show 192.168.68.150

# 2. Check LoadBalancer IP is allocated
kubectl get svc -n kube-system cilium-ingress
# Expected: EXTERNAL-IP should show 192.168.68.150

# 3. Check L2 announcements are working
kubectl get ciliuml2announcementpolicy
# Expected: default-l2-announcement-policy should exist

# 4. Test from external machine (not in cluster)
ping 192.168.68.150
# Expected: Should respond

# 5. Check DNS resolution
dig grafana.timourhomelab.org
# Expected: Should return 192.168.68.150
```

**Common Causes:**

```
âŒ LoadBalancer IP pool exhausted
   â†’ Check: CiliumLoadBalancerIPPool has free IPs
   â†’ Solution: Expand IP pool range

âŒ L2 announcements not working
   â†’ Check: ARP table on router
   â†’ Solution: Verify CiliumL2AnnouncementPolicy

âŒ Firewall blocking port 80/443
   â†’ Check: Node firewall rules
   â†’ Solution: Allow ingress on ports 80/443

âŒ DNS not pointing to correct IP
   â†’ Check: dig <hostname>
   â†’ Solution: Update DNS A record
```

---

## ğŸ“‹ Best Practices Checklist

### âœ… **CNI (Cilium)**

- [x] eBPF-based CNI (Cilium) deployed
- [x] kube-proxy replacement enabled
- [x] Native routing mode (no VXLAN overhead)
- [x] WireGuard encryption enabled (Zero-Trust)
- [x] BPF map sizing configured for scale (10K+ pods)
- [x] Bandwidth Manager enabled (EDT + Fair Queue)
- [x] L7 visibility policy applied (HTTP/DNS inspection)

---

### âœ… **Services & Load Balancing**

- [x] ClusterIP for internal services
- [x] LoadBalancer for external services (Cilium L2 announcements)
- [x] Maglev consistent hashing (sticky sessions)
- [x] Service mesh ready (Istio integration)
- [x] IP pool allocated (192.168.68.150-170)

---

### âœ… **Ingress & Gateway API**

- [x] Cilium Ingress Controller enabled
- [x] Shared LoadBalancer mode (1 IP for multiple Ingresses)
- [x] cert-manager integration (automatic TLS certs)
- [x] Gateway API support enabled (modern ingress)
- [x] L7 HTTP routing configured

---

### âœ… **DNS**

- [x] CoreDNS deployed (kube-system namespace)
- [x] DNS caching enabled (reduced external queries)
- [x] Cilium DNS proxy enabled (FQDN policies)
- [x] DNS metrics exported to Prometheus

---

### âœ… **Network Policies**

- [x] L7 visibility policy (global HTTP/DNS inspection)
- [x] Namespace isolation policies (future: per-app policies)
- [x] FQDN-based egress policies (DNS-based filtering)
- [x] Zero-Trust model (explicit allow-list)

---

## ğŸ“š References

### **Official Documentation**

- Kubernetes Networking: https://kubernetes.io/docs/concepts/cluster-administration/networking/
- CNI Specification: https://github.com/containernetworking/cni
- Cilium Docs: https://docs.cilium.io/
- Gateway API: https://gateway-api.sigs.k8s.io/

### **Homelab Configs**

- **Cilium values.yaml**: `infrastructure/network/cilium/values.yaml`
- **IP Pool**: `infrastructure/network/cilium/ip-pool.yaml`
- **L2 Announcements**: `infrastructure/network/cilium/announce.yaml`
- **L7 Visibility**: `infrastructure/network/cilium/l7-visibility-policy.yaml`

---

**Status**: **100% Production Ready** âœ…

**Architecture**: Cilium eBPF CNI + kube-proxy replacement + WireGuard encryption + L7 policies

**Result**: Enterprise-grade Kubernetes networking with best-in-class performance and security.
