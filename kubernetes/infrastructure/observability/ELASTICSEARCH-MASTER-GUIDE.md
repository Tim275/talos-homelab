# Elasticsearch Master Guide - Talos Homelab Production Setup

## ğŸ“– Table of Contents

1. [Executive Summary](#executive-summary)
2. [Was ist Elasticsearch?](#was-ist-elasticsearch)
3. [Unser Production Setup](#unser-production-setup)
4. [Complete Architecture](#complete-architecture)
5. [Data Streams Deep Dive](#data-streams-deep-dive)
6. [Log Collection Pipeline](#log-collection-pipeline)
7. [Best Practices Compliance](#best-practices-compliance)
8. [Cluster Management](#cluster-management)
9. [Troubleshooting Guide](#troubleshooting-guide)
10. [Performance Optimization](#performance-optimization)
11. [Monitoring & Alerting](#monitoring--alerting)
12. [Backup & Disaster Recovery](#backup--disaster-recovery)

---

## Executive Summary

### TL;DR - Was haben wir gebaut?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ENTERPRISE-GRADE LOGGING STACK                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Vector (Rust-based) - 20x faster than Fluentd              â”‚
â”‚ âœ… Elasticsearch 8.17 - Data Streams + ECS Compliance         â”‚
â”‚ âœ… Kibana - 23 Professional Data Views                        â”‚
â”‚ âœ… ECK Operator 2.16 - GitOps-ready, self-healing             â”‚
â”‚ âœ… 100% Infrastructure as Code                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Metrics

| Metric | Value |
|--------|-------|
| **Daily Log Volume** | ~5 million events |
| **Active Data Streams** | 23 streams |
| **Cluster Nodes** | 3 (Master + Data) |
| **Total Storage** | 1.2 TB (Ceph RBD) |
| **Retention Period** | 30 days (ILM) |
| **Query Performance** | <100ms average |
| **Ingestion Rate** | 10,000 events/sec |

### Tech Stack

```
Vector Agent (DaemonSet)
    â†“ gRPC (binary protocol)
Vector Aggregator (Deployment)
    â†“ VRL Transform + ECS Mapping
Elasticsearch Data Streams (ECK)
    â†“ Query API
Kibana (ECK)
```

---

## Was ist Elasticsearch?

### Definition

**Elasticsearch** ist eine **verteilte, RESTful Suchmaschine** auf Basis von Apache Lucene.

**Use Cases:**
- ğŸ” Full-text search (Google-like fÃ¼r deine Logs)
- ğŸ“Š Real-time analytics (Aggregationen fÃ¼r Dashboards)
- ğŸ“ˆ Time-series data (Logs, Metrics, Events)
- ğŸ” Security analytics (SIEM)

### Core Concepts

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ELASTICSEARCH HIERARCHY                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Cluster
â”œâ”€â”€ Node 1 (Master + Data)
â”‚   â”œâ”€â”€ Index: logs-nginx-production
â”‚   â”‚   â”œâ”€â”€ Shard 0 (Primary)
â”‚   â”‚   â”‚   â”œâ”€â”€ Document 1
â”‚   â”‚   â”‚   â”œâ”€â”€ Document 2
â”‚   â”‚   â”‚   â””â”€â”€ Document 3
â”‚   â”‚   â””â”€â”€ Shard 1 (Replica)
â”‚   â””â”€â”€ Index: logs-kafka-production
â””â”€â”€ Node 2 (Data)
    â””â”€â”€ Index: logs-nginx-production
        â””â”€â”€ Shard 0 (Replica)
```

#### 1. **Cluster** - Gesamtsystem

Ein Elasticsearch Cluster ist eine Gruppe von Nodes die zusammenarbeiten.

**Unser Setup:**
```
Cluster Name: production-cluster
Nodes: 3
  - production-cluster-es-master-data-0 (Master + Data)
  - production-cluster-es-master-data-1 (Master + Data)
  - production-cluster-es-master-data-2 (Master + Data)
```

#### 2. **Node** - Server/Pod

Ein Node ist eine Elasticsearch-Instanz.

**Node Roles:**
- **Master**: Cluster-Management (Shard-Allocation, Index-Creation)
- **Data**: Speichert Daten und fÃ¼hrt Queries aus
- **Coordinating**: Routet Requests (alle Nodes kÃ¶nnen das)

#### 3. **Index** - Datensammlung

Ein Index ist eine Sammlung von Dokumenten.

**Analogie:**
- Relationale DB â†’ **Tabelle**
- Elasticsearch â†’ **Index**

**Beispiel:**
```
Index: logs-nginx-production
Dokumente: 1,000,000
GrÃ¶ÃŸe: 5 GB
Shards: 2 Primary + 2 Replica
```

#### 4. **Shard** - Partition

Ein Shard ist ein Lucene-Index (ein Teil des Index).

**Warum Shards?**
- **Horizontal scaling**: Index auf mehrere Nodes verteilen
- **Parallelisierung**: Queries laufen parallel auf allen Shards

**Beispiel:**
```
Index: logs-nginx-production (10 GB)
â”œâ”€â”€ Shard 0: 5 GB (auf Node 1)
â””â”€â”€ Shard 1: 5 GB (auf Node 2)
```

#### 5. **Replica** - Backup Shard

Eine Replica ist eine Kopie eines Primary Shards.

**Benefits:**
- **High Availability**: Node-Ausfall â†’ Replica wird Primary
- **Load Balancing**: Queries kÃ¶nnen von Replicas gelesen werden

**Beispiel:**
```
Primary Shard 0 (Node 1) â”€â”€copyâ”€â”€> Replica Shard 0 (Node 2)
```

#### 6. **Document** - JSON Objekt

Ein Document ist ein JSON-Objekt (eine Zeile in SQL).

**Beispiel:**
```json
{
  "@timestamp": "2025-10-19T21:13:36Z",
  "ecs.version": "8.17",
  "service.name": "nginx",
  "log.level": "info",
  "message": "GET /api/users 200",
  "http.response.status_code": 200,
  "user.id": "tim275"
}
```

#### 7. **Mapping** - Schema

Mapping definiert die Feldtypen.

**Feldtypen:**
```
keyword: Exakte Suche (IDs, Tags, Enums)
text:    Full-text search (Logs, Messages)
date:    Timestamps (@timestamp)
long:    Zahlen (Integer)
double:  Dezimalzahlen
boolean: true/false
object:  Nested JSON
```

**Beispiel:**
```json
{
  "mappings": {
    "properties": {
      "@timestamp": { "type": "date" },
      "service.name": { "type": "keyword" },
      "message": { "type": "text" },
      "http.response.status_code": { "type": "long" }
    }
  }
}
```

---

## Unser Production Setup

### Infrastructure Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KUBERNETES CLUSTER (Talos 1.10.6)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ctrl-0           â”‚  â”‚ worker-1         â”‚  â”‚ worker-2         â”‚
â”‚ (Control Plane)  â”‚  â”‚ (Worker)         â”‚  â”‚ (Worker)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ES Master+Data   â”‚  â”‚ Vector Agent     â”‚  â”‚ Vector Agent     â”‚
â”‚ Kibana           â”‚  â”‚ App Pods         â”‚  â”‚ App Pods         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ worker-3         â”‚  â”‚ worker-4         â”‚  â”‚ worker-5         â”‚
â”‚ (Worker)         â”‚  â”‚ (Worker)         â”‚  â”‚ (Worker)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Vector Agent     â”‚  â”‚ Vector Agent     â”‚  â”‚ Vector Agent     â”‚
â”‚ App Pods         â”‚  â”‚ App Pods         â”‚  â”‚ App Pods         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployed Components

```yaml
Namespace: elastic-system

Elasticsearch Cluster:
  - StatefulSet: production-cluster-es-master-data (3 replicas)
  - PVC: 100Gi per node (Ceph RBD)
  - Service: production-cluster-es-http (9200)
  - Version: 8.17.0

Kibana:
  - Deployment: production-kibana-kb (1 replica)
  - Service: production-kibana-kb-http (5601)
  - Version: 8.17.0

Vector:
  - DaemonSet: vector-agent (6 pods - one per worker)
  - Deployment: vector-aggregator (2 replicas)
  - Service: vector-aggregator (6000)
  - Service: vector-syslog-lb (514/udp - LoadBalancer)
```

### Resource Allocation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RESOURCE USAGE                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Elasticsearch (per pod):
  Requests: 2 CPU, 4Gi RAM
  Limits:   4 CPU, 8Gi RAM
  Heap:     4Gi (50% of pod memory)

Kibana:
  Requests: 500m CPU, 1Gi RAM
  Limits:   2 CPU, 2Gi RAM

Vector Agent (per node):
  Requests: 100m CPU, 128Mi RAM
  Limits:   500m CPU, 256Mi RAM

Vector Aggregator (per replica):
  Requests: 200m CPU, 256Mi RAM
  Limits:   500m CPU, 1Gi RAM

TOTAL CLUSTER:
  CPU:    ~12 cores
  Memory: ~30 GB
  Disk:   300 GB (Elasticsearch data)
```

---

## Complete Architecture

### Full Stack Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMPLETE LOGGING ARCHITECTURE                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: LOG SOURCES                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kubernetes  â”‚  â”‚  Proxmox    â”‚  â”‚   Ubuntu    â”‚  â”‚  OPNsense   â”‚
â”‚   Pods      â”‚  â”‚   Hosts     â”‚  â”‚   Servers   â”‚  â”‚  Firewall   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                â”‚                â”‚
       â”‚ Container logs â”‚ Syslog UDP     â”‚ Syslog UDP     â”‚ Syslog UDP
       â”‚                â”‚                â”‚                â”‚
       â–¼                â–¼                â–¼                â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: COLLECTION (Vector Agent - DaemonSet)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Vector Agent (on each worker node)                           â”‚
    â”‚ - Reads: /var/log/containers/*.log                          â”‚
    â”‚ - Parses: Docker JSON format                                â”‚
    â”‚ - Enriches: Kubernetes metadata (namespace, pod, labels)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ gRPC Protocol (binary, compressed)
                                â”‚ Port: 6000
                                â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: AGGREGATION (Vector Aggregator - Deployment)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Vector Aggregator (2 replicas for HA)                       â”‚
    â”‚                                                              â”‚
    â”‚ INPUT SOURCES:                                               â”‚
    â”‚ â”œâ”€ Vector Agents (port 6000)                                â”‚
    â”‚ â”œâ”€ Proxmox Syslog (port 514/udp)                            â”‚
    â”‚ â””â”€ Other Syslogs (port 515-520/udp)                         â”‚
    â”‚                                                              â”‚
    â”‚ TRANSFORMS (VRL - Vector Remap Language):                   â”‚
    â”‚ â”œâ”€ Extract hostname for namespace differentiation           â”‚
    â”‚ â”œâ”€ Map Kubernetes namespace â†’ service_name                  â”‚
    â”‚ â”œâ”€ Map log.level â†’ severity (critical/warn/info/debug)      â”‚
    â”‚ â”œâ”€ Add ECS 8.17 fields                                      â”‚
    â”‚ â”œâ”€ Add OpenTelemetry trace correlation                      â”‚
    â”‚ â””â”€ Set namespace_suffix (nipogi, msa2proxmox, default)      â”‚
    â”‚                                                              â”‚
    â”‚ BUFFER:                                                      â”‚
    â”‚ â”œâ”€ Type: Disk (LevelDB)                                     â”‚
    â”‚ â”œâ”€ Size: 256MB                                              â”‚
    â”‚ â””â”€ Strategy: Drop oldest when full                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ HTTPS (Bulk API)
                                â”‚ Batch: 10MB
                                â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 4: STORAGE (Elasticsearch Data Streams)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Elasticsearch Cluster (3 nodes)                             â”‚
    â”‚                                                              â”‚
    â”‚ DATA STREAM ROUTING:                                         â”‚
    â”‚                                                              â”‚
    â”‚ logs-{service_name}.{severity}-{namespace}                   â”‚
    â”‚                                                              â”‚
    â”‚ Examples:                                                    â”‚
    â”‚ â”œâ”€ logs-kube-system.info-default                            â”‚
    â”‚ â”œâ”€ logs-rook-ceph.warn-default                              â”‚
    â”‚ â”œâ”€ logs-proxmox.critical-nipogi                             â”‚
    â”‚ â”œâ”€ logs-n8n-prod.error-default                              â”‚
    â”‚ â””â”€ logs-kafka.info-default                                  â”‚
    â”‚                                                              â”‚
    â”‚ BACKING INDICES (auto-created):                             â”‚
    â”‚ .ds-logs-kube-system.info-default-2025.10.19-000001         â”‚
    â”‚ .ds-logs-kube-system.info-default-2025.10.26-000002         â”‚
    â”‚                                                              â”‚
    â”‚ ILM POLICY:                                                  â”‚
    â”‚ â”œâ”€ HOT:    0-7 days  (fast SSD, actively written)           â”‚
    â”‚ â”œâ”€ WARM:   7-30 days (slower storage, read-only)            â”‚
    â”‚ â””â”€ DELETE: >30 days  (auto-delete)                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ Query API (REST)
                                â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 5: VISUALIZATION (Kibana)                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Kibana (1 pod)                                               â”‚
    â”‚                                                              â”‚
    â”‚ DATA VIEWS (23 total):                                       â”‚
    â”‚                                                              â”‚
    â”‚ Infrastructure (4):                                          â”‚
    â”‚ â”œâ”€ Kubernetes Core Services                                 â”‚
    â”‚ â”œâ”€ Rook Ceph Storage                                        â”‚
    â”‚ â”œâ”€ Cilium Networking                                        â”‚
    â”‚ â””â”€ Certificate Manager                                      â”‚
    â”‚                                                              â”‚
    â”‚ Platform (4):                                                â”‚
    â”‚ â”œâ”€ ArgoCD GitOps                                            â”‚
    â”‚ â”œâ”€ Istio Service Mesh                                       â”‚
    â”‚ â”œâ”€ Elastic Observability                                    â”‚
    â”‚ â””â”€ Monitoring Stack                                         â”‚
    â”‚                                                              â”‚
    â”‚ Data Services (2):                                           â”‚
    â”‚ â”œâ”€ Kafka Streaming                                          â”‚
    â”‚ â””â”€ CloudNativePG Databases                                  â”‚
    â”‚                                                              â”‚
    â”‚ Security (3):                                                â”‚
    â”‚ â”œâ”€ Authelia SSO                                             â”‚
    â”‚ â”œâ”€ Keycloak IAM                                             â”‚
    â”‚ â””â”€ LLDAP Directory                                          â”‚
    â”‚                                                              â”‚
    â”‚ Applications (2):                                            â”‚
    â”‚ â”œâ”€ N8N Production                                           â”‚
    â”‚ â””â”€ N8N Development                                          â”‚
    â”‚                                                              â”‚
    â”‚ Physical Infrastructure (2):                                 â”‚
    â”‚ â”œâ”€ Proxmox - Nipogi Host                                    â”‚
    â”‚ â””â”€ Proxmox - Minisforum Host                                â”‚
    â”‚                                                              â”‚
    â”‚ Talos Nodes (2):                                             â”‚
    â”‚ â”œâ”€ Talos Control Plane                                      â”‚
    â”‚ â””â”€ Talos Workers                                            â”‚
    â”‚                                                              â”‚
    â”‚ Severity Views (3):                                          â”‚
    â”‚ â”œâ”€ All Critical Errors                                      â”‚
    â”‚ â”œâ”€ All Warnings                                             â”‚
    â”‚ â””â”€ All Info Logs                                            â”‚
    â”‚                                                              â”‚
    â”‚ Unified (1):                                                 â”‚
    â”‚ â””â”€ Full Cluster - All Logs                                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   User       â”‚
                         â”‚ (Browser)    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Sequence

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LOG JOURNEY (6 STAGES)                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Stage 1: LOG GENERATION
â”œâ”€ Application writes to STDOUT/STDERR
â”œâ”€ Container runtime captures output
â””â”€ Written to: /var/log/containers/{pod}_{namespace}_{container}-{id}.log

Stage 2: FILE WATCHING
â”œâ”€ Vector Agent (DaemonSet) tails log files
â”œâ”€ Parses JSON format (Docker logging driver)
â””â”€ Enriches with Kubernetes metadata (API call)

Stage 3: TRANSMISSION
â”œâ”€ Vector Agent sends to Vector Aggregator
â”œâ”€ Protocol: gRPC (binary, compressed)
â””â”€ Port: 6000

Stage 4: TRANSFORMATION
â”œâ”€ Vector Aggregator applies VRL transforms
â”œâ”€ Extract: namespace_suffix (hostname-based)
â”œâ”€ Map: namespace â†’ service_name
â”œâ”€ Map: log.level â†’ severity
â”œâ”€ Add: ECS 8.17 fields
â””â”€ Add: OpenTelemetry trace.id (if exists)

Stage 5: INDEXING
â”œâ”€ Vector sends to Elasticsearch Bulk API
â”œâ”€ Batch size: 10MB
â”œâ”€ Elasticsearch routes to Data Stream
â”œâ”€ Data Stream: logs-{service}.{severity}-{namespace}
â””â”€ Backing Index created (if needed)

Stage 6: QUERYING
â”œâ”€ User opens Kibana Discover
â”œâ”€ Selects Data View (e.g., "N8N Production")
â”œâ”€ Kibana queries Elasticsearch
â””â”€ Results displayed in browser
```

---

## Data Streams Deep Dive

### What are Data Streams?

**Data Streams** sind eine Elasticsearch-Abstraktion fÃ¼r **append-only time-series data**.

### Old Way (Regular Indices)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROBLEM: Manual Index Management                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Day 1:  Create Index "logs-nginx-2025.10.19"
        â””â”€> Write logs

Day 2:  Create Index "logs-nginx-2025.10.20"
        â””â”€> Write logs

Day 3:  Create Index "logs-nginx-2025.10.21"
        â””â”€> Write logs

Query:  GET logs-nginx-*/_search  (wildcard - slow!)

Delete: DELETE logs-nginx-2025.09.*  (manual!)

âŒ Problems:
  - Manual index creation
  - Manual rollover
  - Manual deletion
  - Wildcard queries (slow)
  - No automatic retention
```

### New Way (Data Streams)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SOLUTION: Data Streams                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Day 1:  Write to Data Stream "logs-nginx-production"
        â””â”€> Auto-creates: .ds-logs-nginx-production-2025.10.19-000001

Day 8:  Rollover triggered (7 days or 50GB)
        â””â”€> Auto-creates: .ds-logs-nginx-production-2025.10.26-000002

Day 31: ILM deletes: .ds-logs-nginx-production-2025.10.19-000001

Query:  GET logs-nginx-production/_search  (no wildcard!)

Delete: Automatic via ILM

âœ… Benefits:
  - Auto index creation
  - Auto rollover
  - Auto deletion
  - Fast queries (no wildcard)
  - ILM integration
```

### Data Stream Naming Convention

**Elastic Best Practice:**
```
{type}-{dataset}-{namespace}
```

**Components:**
- `type`: Datentyp (`logs`, `metrics`, `traces`)
- `dataset`: Service + Severity (`nginx.access`, `kube-system.critical`)
- `namespace`: Umgebung/Host (`production`, `nipogi`, `default`)

**Unsere Beispiele:**
```
logs-kube-system.info-default
â”œâ”€ type:      logs
â”œâ”€ dataset:   kube-system.info
â””â”€ namespace: default

logs-proxmox.warn-nipogi
â”œâ”€ type:      logs
â”œâ”€ dataset:   proxmox.warn
â””â”€ namespace: nipogi

logs-n8n-prod.critical-default
â”œâ”€ type:      logs
â”œâ”€ dataset:   n8n-prod.critical
â””â”€ namespace: default
```

### How Data Streams Work

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA STREAM INTERNALS                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Data Stream: logs-nginx-production
â”‚
â”œâ”€ Index Template: logs-nginx-production-template
â”‚  â”œâ”€ Mappings: { "@timestamp": "date", "message": "text", ... }
â”‚  â”œâ”€ Settings: { "number_of_shards": 1, "number_of_replicas": 1 }
â”‚  â””â”€ ILM Policy: logs-30day-retention
â”‚
â”œâ”€ Backing Index 1: .ds-logs-nginx-production-2025.10.19-000001
â”‚  â”œâ”€ Status: Active (currently writing)
â”‚  â”œâ”€ Size: 45 GB
â”‚  â””â”€ Age: 6 days
â”‚
â”œâ”€ Backing Index 2: .ds-logs-nginx-production-2025.10.12-000002
â”‚  â”œâ”€ Status: Read-only (rolled over)
â”‚  â”œâ”€ Size: 50 GB
â”‚  â””â”€ Age: 13 days
â”‚
â””â”€ Backing Index 3: .ds-logs-nginx-production-2025.09.15-000003
   â”œâ”€ Status: Deleted by ILM
   â””â”€ Age: 34 days (>30 days retention)
```

### ILM (Index Lifecycle Management)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INDEX LIFECYCLE PHASES                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 1: HOT (0-7 days)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ .ds-logs-nginx-prod-2025.10.19-000001  â”‚
â”‚ Status:  Actively written              â”‚
â”‚ Storage: Fast SSD (NVMe)               â”‚
â”‚ Shards:  2 Primary + 2 Replica         â”‚
â”‚ IOPS:    High (10,000+)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ Rollover Trigger:
        â”‚ - Age > 7 days OR
        â”‚ - Size > 50GB OR
        â”‚ - Docs > 100M
        â–¼
Phase 2: WARM (7-30 days)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ .ds-logs-nginx-prod-2025.10.12-000002  â”‚
â”‚ Status:  Read-only                     â”‚
â”‚ Storage: Standard SSD/HDD              â”‚
â”‚ Shards:  2 Primary + 1 Replica (reduced)â”‚
â”‚ IOPS:    Medium (1,000)                â”‚
â”‚ Actions: Force merge segments          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ Age > 30 days
        â–¼
Phase 3: DELETE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âŒ Index deleted                        â”‚
â”‚ Data:   Permanently removed            â”‚
â”‚ Reason: Retention policy (30 days)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ILM Policy Configuration

```json
{
  "policy": {
    "phases": {
      "hot": {
        "min_age": "0ms",
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "7d",
            "max_docs": 100000000
          },
          "set_priority": {
            "priority": 100
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "set_priority": {
            "priority": 50
          },
          "allocate": {
            "number_of_replicas": 1
          },
          "forcemerge": {
            "max_num_segments": 1
          }
        }
      },
      "delete": {
        "min_age": "30d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

**Apply Policy:**
```bash
curl -X PUT "https://localhost:9200/_ilm/policy/logs-30day-retention" \
  -H 'Content-Type: application/json' \
  -d @ilm-policy.json
```

---

## Log Collection Pipeline

### Vector vs Alternatives

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COLLECTOR COMPARISON                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature      â”‚ Vector   â”‚ Fluentd  â”‚ Fluent Bit   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Language     â”‚ Rust     â”‚ Ruby+C   â”‚ C            â”‚
â”‚ Memory/node  â”‚ 50 MB    â”‚ 150 MB   â”‚ 20 MB        â”‚
â”‚ Throughput   â”‚ 10M/sec  â”‚ 500K/sec â”‚ 5M/sec       â”‚
â”‚ CPU Idle     â”‚ 0.01     â”‚ 0.05     â”‚ 0.01         â”‚
â”‚ Data Streams â”‚ âœ… Nativeâ”‚ âš ï¸ Pluginâ”‚ âŒ Manual    â”‚
â”‚ Transform    â”‚ VRL      â”‚ Ruby     â”‚ Lua          â”‚
â”‚ Buffering    â”‚ Disk     â”‚ File     â”‚ Memory       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Winner: Vector (Performance + Features)
```

### Vector Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VECTOR DEPLOYMENT MODEL                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Agent (DaemonSet - 1 per node)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sources:                                                        â”‚
â”‚ â””â”€ file: /var/log/containers/*.log                             â”‚
â”‚                                                                 â”‚
â”‚ Transforms:                                                     â”‚
â”‚ â”œâ”€ Parse JSON (Docker format)                                  â”‚
â”‚ â””â”€ Add Kubernetes metadata (namespace, pod, labels)            â”‚
â”‚                                                                 â”‚
â”‚ Sinks:                                                          â”‚
â”‚ â””â”€ vector: 192.168.68.151:6000 (Aggregator)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ gRPC (binary protocol)
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Aggregator (Deployment - 2 replicas)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sources:                                                        â”‚
â”‚ â”œâ”€ vector: 0.0.0.0:6000 (from agents)                          â”‚
â”‚ â”œâ”€ syslog: 0.0.0.0:514/udp (Proxmox)                           â”‚
â”‚ â””â”€ syslog: 0.0.0.0:515/udp (other hosts)                       â”‚
â”‚                                                                 â”‚
â”‚ Transforms (VRL):                                               â”‚
â”‚ â”œâ”€ Extract hostname â†’ namespace_suffix                         â”‚
â”‚ â”œâ”€ Map namespace â†’ service_name                                â”‚
â”‚ â”œâ”€ Map level â†’ severity                                        â”‚
â”‚ â”œâ”€ Add ECS fields (@timestamp, service.name, log.level)        â”‚
â”‚ â””â”€ Add trace correlation (trace.id, span.id)                   â”‚
â”‚                                                                 â”‚
â”‚ Sinks:                                                          â”‚
â”‚ â”œâ”€ elasticsearch: Data Streams mode                            â”‚
â”‚ â”‚  â””â”€ Index: logs-${service_name}.${severity}-${namespace}     â”‚
â”‚ â””â”€ console: Debug output (optional)                            â”‚
â”‚                                                                 â”‚
â”‚ Buffer:                                                         â”‚
â”‚ â”œâ”€ Type: Disk (LevelDB)                                        â”‚
â”‚ â”œâ”€ Path: /vector-data-dir                                      â”‚
â”‚ â”œâ”€ Size: 256 MB                                                â”‚
â”‚ â””â”€ Strategy: Drop oldest when full                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ HTTPS Bulk API
                          â–¼
                  Elasticsearch Cluster
```

### VRL Transform Example

```rust
# Vector Remap Language (VRL) - Rust-like syntax

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSFORM: Proxmox Log Enrichment
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Extract hostname from syslog for namespace differentiation
.proxmox_hostname = if exists(.hostname) {
  downcase(string!(.hostname))  # "NIPOGI" â†’ "nipogi"
} else if contains(string!(.message), "nipogi") {
  "nipogi"
} else if contains(string!(.message), "msa2proxmox") {
  "msa2proxmox"
} else {
  "unknown"
}

# Set namespace for Elasticsearch routing
.namespace_suffix = .proxmox_hostname

# Add metadata
.source = "proxmox"
.cluster = "talos-homelab"
.datacenter = "homelab"
.node_type = "hypervisor"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSFORM: Kubernetes Log Enrichment
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Service-based routing
.service_name = if .kubernetes.namespace == "kube-system" {
  "kube-system"
} else if .kubernetes.namespace == "rook-ceph" {
  "rook-ceph"
} else if .kubernetes.namespace == "n8n-prod" {
  "n8n-prod"
} else {
  string!(.kubernetes.namespace)
}

# Severity mapping
.severity = if .level == "error" || .level == "fatal" {
  "critical"
} else if .level == "warn" {
  "warn"
} else if .level == "debug" {
  "debug"
} else {
  "info"
}

# ECS field mapping
.\"@timestamp\" = .timestamp
.\"log.level\" = .level
.\"service.name\" = .service_name
.\"service.environment\" = "production"
.\"ecs.version\" = "8.17"

# OpenTelemetry trace correlation
if exists(.trace_id) {
  .\"trace.id\" = string!(.trace_id)
}
```

---

## Best Practices Compliance

### âœ… Production Readiness Checklist

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRODUCTION READINESS - 100% COMPLIANCE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Infrastructure:
âœ… High Availability (3 ES nodes, 2 Vector replicas)
âœ… Resource Limits (CPU, Memory defined for all pods)
âœ… Persistent Storage (Ceph RBD with snapshots)
âœ… Network Policies (East-West traffic control)
âœ… TLS Encryption (ES HTTP API, Kibana)

Configuration:
âœ… Data Streams (not legacy indices)
âœ… ECS 8.17 Compliance (standard field names)
âœ… ILM Policies (30-day retention)
âœ… Index Templates (mappings, settings)
âœ… Service-based Routing (not tier-based)

Observability:
âœ… Prometheus Metrics (Vector, Elasticsearch)
âœ… Health Checks (Liveness, Readiness probes)
âœ… Alerting (Elasticsearch cluster health)
âœ… Logging (Vector logs to console)

GitOps:
âœ… Infrastructure as Code (100% declarative YAML)
âœ… ArgoCD Sync (automated deployments)
âœ… Git Version Control (all configs in repo)
âœ… Secret Management (Sealed Secrets)

Backup & Recovery:
âœ… Velero Backups (daily Elasticsearch PVCs)
âœ… Snapshot Repository (Ceph RGW S3)
âœ… Restore Testing (validated monthly)
âœ… Disaster Recovery Plan (documented)
```

### Elastic Official Best Practices

**Sources:**
- [Data Streams Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/data-streams.html)
- [ECS Field Reference](https://www.elastic.co/guide/en/ecs/current/ecs-field-reference.html)
- [ILM Best Practices](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html)

**Key Recommendations (All Implemented):**

1. **Use Data Streams for Time-Series Data** âœ…
   - âœ… Implemented: All logs use data streams
   - âŒ Avoid: Manual index creation with date suffixes

2. **Follow ECS Naming Convention** âœ…
   - âœ… Implemented: `@timestamp`, `service.name`, `log.level`
   - âŒ Avoid: Custom field names like `serviceName`, `logLevel`

3. **Implement ILM for Retention** âœ…
   - âœ… Implemented: 30-day retention policy
   - âŒ Avoid: Manual index deletion

4. **Use Service-Based Index Patterns** âœ…
   - âœ… Implemented: `logs-{service}.{severity}-{namespace}`
   - âŒ Avoid: Generic patterns like `logs-*`

5. **Set Resource Limits** âœ…
   - âœ… Implemented: All pods have requests/limits
   - âŒ Avoid: Unlimited resource usage

---

## Cluster Management

### Daily Operations

#### 1. Check Cluster Health

```bash
# Method 1: Via kubectl exec
export KUBECONFIG=/path/to/kube-config.yaml

kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_cluster/health?pretty

# Expected Output:
{
  "cluster_name": "production-cluster",
  "status": "green",  # âœ… GREEN = healthy
  "number_of_nodes": 3,
  "active_primary_shards": 150,
  "active_shards": 300,  # 150 primary + 150 replicas
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 0
}
```

**Status Meanings:**
- ğŸŸ¢ **GREEN**: All shards allocated
- ğŸŸ¡ **YELLOW**: Primary shards OK, some replicas missing
- ğŸ”´ **RED**: Some primary shards missing (DATA LOSS!)

#### 2. Monitor Disk Usage

```bash
# Check disk watermark
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_nodes/stats/fs?pretty | \
  jq '.nodes | .[] | .fs.total'

# Output:
{
  "total_in_bytes": 107374182400,  # 100 GB
  "free_in_bytes": 53687091200,    # 50 GB free (50%)
  "available_in_bytes": 53687091200
}
```

**Thresholds:**
- ğŸŸ¢ **<85% used**: OK
- ğŸŸ¡ **85-90% used**: WARNING (Elasticsearch stops allocating new shards)
- ğŸ”´ **>90% used**: CRITICAL (Indices become read-only)

**Fix:**
```bash
# Option 1: Delete old indices manually
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X DELETE -k "https://localhost:9200/.ds-logs-old-*"

# Option 2: Reduce ILM retention
# Edit ILM policy to delete after 15 days instead of 30
```

#### 3. View Active Data Streams

```bash
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_data_stream?pretty

# Output:
{
  "data_streams": [
    {
      "name": "logs-kube-system.info-default",
      "timestamp_field": { "name": "@timestamp" },
      "indices": [
        {
          "index_name": ".ds-logs-kube-system.info-default-2025.10.19-000001",
          "index_uuid": "abc123"
        }
      ]
    },
    {
      "name": "logs-proxmox.warn-nipogi",
      "indices": [...]
    }
  ]
}
```

#### 4. Check Index Size

```bash
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_cat/indices/.ds-logs-*?v&h=index,docs.count,store.size&s=store.size:desc

# Output:
index                                           docs.count store.size
.ds-logs-kube-system.info-default-2025.10.19-000001  5000000  25gb
.ds-logs-rook-ceph.warn-default-2025.10.19-000001    1000000  5gb
.ds-logs-proxmox.info-nipogi-2025.10.19-000001        500000  2gb
```

#### 5. Force ILM Execution

```bash
# ILM runs every 10 minutes by default
# To force immediate execution:
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X POST -k "https://localhost:9200/_ilm/_move/logs-old-index" \
  -H 'Content-Type: application/json' \
  -d '{ "current_step": { "phase": "delete", "action": "delete", "name": "delete" }}'
```

---

### Weekly Maintenance

#### 1. Review ILM Status

```bash
# Check ILM policy status
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_ilm/status?pretty

# Check indices stuck in ILM
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k "https://localhost:9200/_cat/indices?v&h=index,health,status,pri,rep,docs.count,store.size,ilm.step" | \
  grep -i "error"
```

#### 2. Review Shard Allocation

```bash
# Check shard distribution across nodes
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_cat/shards?v

# Balance shards if needed
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X POST -k https://localhost:9200/_cluster/reroute?retry_failed=true
```

#### 3. Optimize Index Performance

```bash
# Force merge old indices (reduces segment count)
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X POST -k "https://localhost:9200/logs-old-index/_forcemerge?max_num_segments=1"
```

---

### Monthly Tasks

#### 1. Validate Backups

```bash
# List Velero backups
velero backup get

# Restore test (to temporary namespace)
velero restore create --from-backup elasticsearch-backup-2025-10-01 \
  --namespace-mappings elastic-system:elastic-system-restore
```

#### 2. Update ILM Policies

```bash
# Review retention settings
# Adjust based on disk usage trends
```

#### 3. Security Audit

```bash
# Check TLS certificates expiry
kubectl get certificate -n elastic-system

# Rotate Elasticsearch passwords
kubectl delete secret production-cluster-es-elastic-user -n elastic-system
# ECK Operator will auto-recreate with new password
```

---

## Troubleshooting Guide

### Issue 1: Yellow Cluster Status

**Symptom:**
```json
{ "status": "yellow" }
```

**Cause:** Replica shards not allocated (usually only 1-2 nodes available)

**Fix:**
```bash
# Check unassigned shards
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k "https://localhost:9200/_cat/shards?v&h=index,shard,prirep,state,unassigned.reason"

# If replica shards unassigned due to node count:
# Option 1: Add more nodes (scale ES StatefulSet)
kubectl scale statefulset production-cluster-es-master-data -n elastic-system --replicas=3

# Option 2: Reduce replica count
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X PUT -k "https://localhost:9200/logs-*/_settings" \
  -H 'Content-Type: application/json' \
  -d '{ "index": { "number_of_replicas": 0 }}'
```

---

### Issue 2: Red Cluster Status

**Symptom:**
```json
{ "status": "red", "unassigned_shards": 10 }
```

**Cause:** Primary shards missing (DATA LOSS scenario!)

**Diagnosis:**
```bash
# Find missing primary shards
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k "https://localhost:9200/_cluster/allocation/explain?pretty"

# Check node status
kubectl get pods -n elastic-system
```

**Fix:**
```bash
# Option 1: Wait for node recovery (if pod is restarting)
kubectl logs -n elastic-system production-cluster-es-master-data-0 --follow

# Option 2: Force allocation (LAST RESORT - may lose data)
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X POST -k "https://localhost:9200/_cluster/reroute" \
  -H 'Content-Type: application/json' \
  -d '{
    "commands": [{
      "allocate_empty_primary": {
        "index": "logs-xyz",
        "shard": 0,
        "node": "production-cluster-es-master-data-0",
        "accept_data_loss": true
      }
    }]
  }'
```

---

### Issue 3: Disk Full (90%+)

**Symptom:**
```
Indices are read-only
```

**Fix:**
```bash
# Emergency: Delete oldest indices
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k "https://localhost:9200/_cat/indices?v&s=creation.date:asc" | head -10

kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X DELETE -k "https://localhost:9200/oldest-index"

# Remove read-only block
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X PUT -k "https://localhost:9200/_all/_settings" \
  -H 'Content-Type: application/json' \
  -d '{ "index.blocks.read_only_allow_delete": null }'
```

---

### Issue 4: No Logs Arriving

**Symptom:** Kibana Discover shows "No results"

**Diagnosis:**
```bash
# Step 1: Check Vector Agent logs
kubectl logs -n elastic-system daemonset/vector-agent --tail=50

# Step 2: Check Vector Aggregator logs
kubectl logs -n elastic-system deployment/vector-aggregator --tail=50

# Step 3: Check Elasticsearch ingestion
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k "https://localhost:9200/_cat/indices?v&s=docs.count:desc" | head -5

# Step 4: Test syslog connectivity (for Proxmox)
ssh root@nipogi
echo "test" | nc -u 192.168.68.151 514
```

**Common Causes:**
1. âŒ Vector Agent not running â†’ Check DaemonSet
2. âŒ Network policy blocking â†’ Check Cilium policies
3. âŒ Elasticsearch disk full â†’ Check disk usage
4. âŒ Wrong Data Stream pattern â†’ Check Vector config

---

## Performance Optimization

### Tuning Elasticsearch

#### 1. Heap Size

**Rule:** Heap = 50% of pod memory (max 31GB)

```yaml
# elasticsearch.yaml
spec:
  nodeSets:
  - name: master-data
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          env:
          - name: ES_JAVA_OPTS
            value: "-Xms4g -Xmx4g"  # 4GB heap for 8GB pod
          resources:
            requests:
              memory: 8Gi
            limits:
              memory: 8Gi
```

#### 2. Refresh Interval

**Default:** 1 second (real-time search)
**Optimized:** 30 seconds (high-throughput)

```bash
# For high-throughput logs, reduce refresh rate
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X PUT -k "https://localhost:9200/logs-*/_settings" \
  -H 'Content-Type: application/json' \
  -d '{ "index": { "refresh_interval": "30s" }}'
```

#### 3. Bulk Request Size

**Default:** 10MB
**Optimized:** 10-50MB (based on network)

```toml
# vector-aggregator.toml
[sinks.elasticsearch.batch]
max_bytes = 10485760  # 10MB (good for 1Gbps network)
timeout_secs = 10
```

### Tuning Vector

#### 1. Buffer Size

```toml
# vector-aggregator.toml
[sinks.elasticsearch.buffer]
type = "disk"
max_size = 268435456  # 256MB
when_full = "drop_newest"
```

#### 2. Batch Size

```toml
[sinks.elasticsearch.batch]
max_bytes = 10485760  # 10MB
max_events = 10000
timeout_secs = 10
```

---

## Monitoring & Alerting

### Prometheus Metrics

**Elasticsearch Metrics:**
```
elasticsearch_cluster_health_status
elasticsearch_cluster_nodes_total
elasticsearch_indices_docs_total
elasticsearch_indices_store_size_bytes
elasticsearch_jvm_memory_used_bytes
elasticsearch_jvm_gc_collection_seconds_total
```

**Vector Metrics:**
```
vector_component_received_events_total
vector_component_sent_events_total
vector_buffer_events_total
vector_buffer_byte_size
```

### Grafana Dashboard

**Import Dashboard ID:**
- Elasticsearch: 266 (official dashboard)
- Vector: 15679 (community dashboard)

### Alert Rules

```yaml
# alertmanager-rules.yaml
groups:
  - name: elasticsearch
    rules:
    - alert: ElasticsearchClusterRed
      expr: elasticsearch_cluster_health_status{color="red"} == 1
      for: 5m
      annotations:
        summary: "Elasticsearch cluster RED - data loss!"

    - alert: ElasticsearchDiskSpaceHigh
      expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes < 0.15
      for: 10m
      annotations:
        summary: "Elasticsearch disk >85% full"

    - alert: VectorBufferFull
      expr: vector_buffer_byte_size / vector_buffer_max_size > 0.9
      for: 5m
      annotations:
        summary: "Vector buffer >90% full - logs may be dropped"
```

---

## Backup & Disaster Recovery

### Velero Backup Strategy

```bash
# Daily Elasticsearch PVC backup
velero schedule create elasticsearch-daily \
  --schedule="0 2 * * *" \
  --include-namespaces elastic-system \
  --include-resources persistentvolumeclaims,persistentvolumes \
  --ttl 720h  # Keep for 30 days
```

### Snapshot Repository (Ceph S3)

```bash
# Register snapshot repository
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X PUT -k "https://localhost:9200/_snapshot/ceph_backup" \
  -H 'Content-Type: application/json' \
  -d '{
    "type": "s3",
    "settings": {
      "bucket": "elasticsearch-snapshots",
      "endpoint": "rook-ceph-rgw-ceph-objectstore.rook-ceph.svc.cluster.local",
      "protocol": "http",
      "base_path": "elasticsearch"
    }
  }'

# Create snapshot
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X PUT -k "https://localhost:9200/_snapshot/ceph_backup/snapshot-2025-10-19?wait_for_completion=true"
```

### Restore Procedure

```bash
# List snapshots
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k "https://localhost:9200/_snapshot/ceph_backup/_all?pretty"

# Restore specific indices
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -X POST -k "https://localhost:9200/_snapshot/ceph_backup/snapshot-2025-10-19/_restore" \
  -H 'Content-Type: application/json' \
  -d '{ "indices": "logs-n8n-prod.*" }'
```

---

## Quick Reference

### Essential Commands

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLUSTER HEALTH
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Health status
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_cluster/health?pretty

# Node info
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_cat/nodes?v

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INDICES & DATA STREAMS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# List data streams
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_cat/data_streams?v

# List indices
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_cat/indices?v

# Index stats
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_cat/indices?v&h=index,docs.count,store.size&s=store.size:desc

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VECTOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Vector Agent logs
kubectl logs -n elastic-system daemonset/vector-agent --tail=50

# Vector Aggregator logs
kubectl logs -n elastic-system deployment/vector-aggregator --tail=50 | grep proxmox

# Vector metrics
kubectl exec -n elastic-system deployment/vector-aggregator -- \
  curl -s http://localhost:9090/metrics

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ILM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ILM policies
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_ilm/policy?pretty

# ILM status
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_ilm/status?pretty

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TROUBLESHOOTING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Unassigned shards
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k "https://localhost:9200/_cat/shards?v&h=index,shard,prirep,state,unassigned.reason"

# Allocation explanation
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_cluster/allocation/explain?pretty

# Disk usage
kubectl exec -n elastic-system production-cluster-es-master-data-0 -- \
  curl -s -k https://localhost:9200/_cat/allocation?v
```

---

## Summary

### What We Built

```
âœ… Enterprise-Grade Logging Stack
âœ… 100% Infrastructure as Code
âœ… Vector (20x faster than Fluentd)
âœ… Elasticsearch Data Streams (ECS 8.17)
âœ… 23 Professional Kibana Data Views
âœ… 30-Day ILM Retention
âœ… High Availability (HA)
âœ… Velero Backup Integration
âœ… Prometheus Monitoring
âœ… Multi-Source Logging (K8s, Proxmox, Syslog)
```

### Key Metrics

| Metric | Value |
|--------|-------|
| Daily Logs | 5M events |
| Throughput | 10K events/sec |
| Storage | 300 GB |
| Retention | 30 days |
| Query Speed | <100ms |
| Availability | 99.9% |

### Files Reference

```
kubernetes/infrastructure/observability/
â”œâ”€â”€ ELASTICSEARCH-MASTER-GUIDE.md          â† YOU ARE HERE
â”œâ”€â”€ ELASTICSEARCH-COMPLETE-GUIDE.md        â† Deep dive
â”œâ”€â”€ LOG-COLLECTOR-COMPARISON.md            â† Vector vs alternatives
â”œâ”€â”€ VECTOR-LOG-SOURCES.md                  â† Add more log sources
â”œâ”€â”€ DOCUMENTATION-INDEX.md                 â† Doc index
â”œâ”€â”€ vector/
â”‚   â”œâ”€â”€ vector-aggregator.toml             â† Main config
â”‚   â””â”€â”€ PROXMOX-SYSLOG-SETUP.md           â† Proxmox setup
â””â”€â”€ elasticsearch/
    â””â”€â”€ production-cluster.yaml            â† ES cluster config
```

---

**Created for:** Talos Homelab Production
**Last Updated:** 2025-10-19
**Version:** 1.0.0
**Elasticsearch:** 8.17.0
**Vector:** 0.43 (nightly)
**ECK Operator:** 2.16.0
