# Vector Aggregator Configuration - Central log processor (replaces Fluentd)
# Runs as Deployment with 2 replicas

data_dir = "/vector-data-dir"

[api]
enabled = true
address = "0.0.0.0:8686"

# Receive from Vector Agents
[sources.vector_agents]
type = "vector"
address = "0.0.0.0:6000"
version = "2"

# Proxmox syslog input
[sources.proxmox_syslog]
type = "syslog"
address = "0.0.0.0:5140"
mode = "udp"

# Direct Talos etcd logs from control plane
[sources.talos_etcd_logs]
type = "exec"
mode = "scheduled"
command = ["talosctl", "-n", "192.168.68.101", "logs", "etcd", "--tail"]
scheduled.exec_interval_secs = 30

# Direct Talos kubelet logs from workers (sample from worker-1)
[sources.talos_kubelet_logs]
type = "exec"
mode = "scheduled"
command = ["talosctl", "-n", "192.168.68.103", "logs", "kubelet", "--tail"]
scheduled.exec_interval_secs = 30

# Process Proxmox logs and add metadata
[transforms.process_proxmox_logs]
type = "remap"
inputs = ["proxmox_syslog"]
source = '''
.source = "proxmox"
.cluster = "talos-homelab"
.datacenter = "homelab"
.node_type = "hypervisor"
'''

# Process Talos system logs from direct talosctl access
[transforms.process_talos_system_logs]
type = "remap"
inputs = ["talos_etcd_logs", "talos_kubelet_logs"]
source = '''
.source = "talos-direct"
.cluster = "talos-homelab"
.environment = "production"

# Detect log type and assign proper index
if contains(string!(.message), "etcd") || exists(.component) && .component == "etcd" {
  .service = "etcd"
  .node_role = "control-plane"
  .index_name = "etcd-control-plane-direct"
} else if contains(string!(.message), "kubelet") {
  .service = "kubelet"
  .node_role = "worker"
  .index_name = "kubelet-worker-direct"
} else {
  .service = "talos-system"
  .node_role = "unknown"
  .index_name = "talos-system-direct"
}

# Extract node IP from message context
if exists(.node_ip) {
  .node_ip = .node_ip
} else {
  .node_ip = "192.168.68.101"  # Default to control plane
}
'''

# Parse and enrich all logs - ENTERPRISE SERVICE-BASED
[transforms.enrich_logs]
type = "remap"
inputs = ["vector_agents", "process_proxmox_logs", "process_talos_system_logs"]
source = '''
# Add timestamp if missing
if !exists(.timestamp) {
  .timestamp = now()
}

# Standardize log level
if exists(.level) {
  .level = downcase(string!(.level))
} else if contains(string!(.message), "ERROR") || contains(string!(.message), "error") {
  .level = "error"
} else if contains(string!(.message), "WARN") || contains(string!(.message), "warn") {
  .level = "warn"
} else {
  .level = "info"
}

# ═══════════════════════════════════════════════════════════════
# ENTERPRISE TIER-0: Service-Based Index Routing
# ═══════════════════════════════════════════════════════════════
.namespace = .kubernetes.namespace

# Service-based routing
.service = if .namespace == "kube-system" {
  "kube-system"
} else if .namespace == "rook-ceph" {
  "rook-ceph"
} else if .namespace == "argocd" {
  "argocd"
} else if .namespace == "cert-manager" {
  "cert-manager"
} else if .namespace == "istio-system" {
  "istio"
} else if .namespace == "monitoring" {
  "monitoring"
} else if .namespace == "n8n-prod" {
  "n8n-prod"
} else if .namespace == "n8n-dev" {
  "n8n-dev"
} else if .namespace == "kafka" {
  "kafka"
} else if .namespace == "elastic-system" {
  "elastic-system"
} else if .namespace == "cnpg-system" {
  "cloudnative-pg"
} else if contains(string!(.namespace), "boutique") {
  "boutique-" + string!(.namespace)
} else {
  string!(.namespace)
}

# ═══════════════════════════════════════════════════════════════
# Map Severity (critical/warn/info/debug)
# ═══════════════════════════════════════════════════════════════
.severity = if .level == "error" || .level == "fatal" || .level == "critical" {
  "critical"
} else if .level == "warn" || .level == "warning" {
  "warn"
} else if .level == "debug" || .level == "trace" {
  "debug"
} else {
  "info"
}

# Add cluster metadata
.cluster = "talos-homelab"
.environment = "production"
'''

# Sample logs to reduce volume (optional)
[transforms.sample_logs]
type = "sample"
inputs = ["enrich_logs"]
rate = 100  # Keep all logs (set lower to sample, e.g. 10 = 10%)
key_field = ".kubernetes.pod"  # Sample per pod

# Elasticsearch sink - ENTERPRISE INDEX PATTERN
[sinks.elasticsearch]
type = "elasticsearch"
inputs = ["sample_logs"]
endpoints = ["https://production-cluster-es-http.elastic-system.svc.cluster.local:9200"]
mode = "bulk"
bulk.index = "logs-{{ .service }}-{{ .severity }}-%Y.%m"  # ← SERVICE-BASED + MONTHLY ROLLOVER!
api_version = "v8"

# Authentication
[sinks.elasticsearch.auth]
strategy = "basic"
user = "elastic"
password = "${ELASTICSEARCH_PASSWORD}"  # Set via secret

# TLS configuration
[sinks.elasticsearch.tls]
verify_certificate = false  # For self-signed certs
verify_hostname = false

# Buffer configuration
[sinks.elasticsearch.buffer]
type = "disk"
max_size = 268435488  # 256MB buffer
when_full = "drop_newest"

[sinks.elasticsearch.batch]
max_bytes = 10485760  # 10MB batches
timeout_secs = 10

[sinks.elasticsearch.request]
timeout_secs = 60
retry_attempts = 3

# Console output for debugging (optional)
[sinks.console]
type = "console"
inputs = ["sample_logs"]
encoding.codec = "json"
target = "stdout"
