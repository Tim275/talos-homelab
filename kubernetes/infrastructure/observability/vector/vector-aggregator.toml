# Vector Aggregator Configuration - Central log processor (replaces Fluentd)
# Runs as Deployment with 2 replicas

data_dir = "/vector-data-dir"

[api]
enabled = true
address = "0.0.0.0:8686"

# Receive from Vector Agents
[sources.vector_agents]
type = "vector"
address = "0.0.0.0:6000"
version = "2"

# Proxmox syslog input
[sources.proxmox_syslog]
type = "syslog"
address = "0.0.0.0:5140"
mode = "udp"

# Direct Talos etcd logs from control plane
[sources.talos_etcd_logs]
type = "exec"
mode = "scheduled"
command = ["talosctl", "-n", "192.168.68.101", "logs", "etcd", "--tail"]
scheduled.exec_interval_secs = 30

# Direct Talos kubelet logs from workers (sample from worker-1)
[sources.talos_kubelet_logs]
type = "exec"
mode = "scheduled"
command = ["talosctl", "-n", "192.168.68.103", "logs", "kubelet", "--tail"]
scheduled.exec_interval_secs = 30

# Process Proxmox logs and add metadata
[transforms.process_proxmox_logs]
type = "remap"
inputs = ["proxmox_syslog"]
source = '''
.source = "proxmox"
.cluster = "talos-homelab"
.datacenter = "homelab"
.node_type = "hypervisor"

# ═══════════════════════════════════════════════════════════════
# ELASTIC BEST PRACTICE: Namespace differentiation by hostname
# ═══════════════════════════════════════════════════════════════
# Extract hostname from syslog metadata to differentiate hosts
.proxmox_hostname = if exists(.hostname) {
  downcase(string!(.hostname))
} else if contains(string!(.message), "nipogi") {
  "nipogi"
} else if contains(string!(.message), "minisforum") {
  "minisforum"
} else {
  "unknown"
}

# Set namespace for data stream routing (Elastic Standard)
.namespace_suffix = .proxmox_hostname
'''

# Process Talos system logs from direct talosctl access
[transforms.process_talos_system_logs]
type = "remap"
inputs = ["talos_etcd_logs", "talos_kubelet_logs"]
source = '''
.source = "talos-direct"
.cluster = "talos-homelab"
.environment = "production"

# ═══════════════════════════════════════════════════════════════
# ELASTIC BEST PRACTICE: Node role separation (control-plane vs workers)
# ═══════════════════════════════════════════════════════════════
# Detect node role based on IP address
if exists(.node_ip) {
  .node_ip = .node_ip
} else {
  .node_ip = "192.168.68.101"  # Default to control plane
}

# Determine node role from IP (control-plane: .101, workers: .103-.108)
.node_role = if .node_ip == "192.168.68.101" {
  "control-plane"
} else if match(string!(.node_ip), r'^192\.168\.68\.10[3-8]$') {
  "worker"
} else {
  "unknown"
}

# Detect log type and assign service name
if contains(string!(.message), "etcd") || exists(.component) && .component == "etcd" {
  .service_name = "etcd"
} else if contains(string!(.message), "kubelet") {
  .service_name = "kubelet"
} else {
  .service_name = "talos-system"
}

# Set namespace for data stream routing (node role differentiation)
.namespace_suffix = .node_role
'''

# Parse and enrich all logs - ENTERPRISE SERVICE-BASED
[transforms.enrich_logs]
type = "remap"
inputs = ["vector_agents", "process_proxmox_logs", "process_talos_system_logs"]
source = '''
# Add timestamp if missing
if !exists(.timestamp) {
  .timestamp = now()
}

# Standardize log level
if exists(.level) {
  .level = downcase(string!(.level))
} else if contains(string!(.message), "ERROR") || contains(string!(.message), "error") {
  .level = "error"
} else if contains(string!(.message), "WARN") || contains(string!(.message), "warn") {
  .level = "warn"
} else {
  .level = "info"
}

# ═══════════════════════════════════════════════════════════════
# ENTERPRISE TIER-0: Service-Based Index Routing
# ═══════════════════════════════════════════════════════════════
.namespace = .kubernetes.namespace

# Service-based routing (use service_name for routing, not service which is ECS nested object)
.service_name = if .namespace == "kube-system" {
  "kube-system"
} else if .namespace == "rook-ceph" {
  "rook-ceph"
} else if .namespace == "argocd" {
  "argocd"
} else if .namespace == "cert-manager" {
  "cert-manager"
} else if .namespace == "istio-system" {
  "istio"
} else if .namespace == "monitoring" {
  "monitoring"
} else if .namespace == "n8n-prod" {
  "n8n-prod"
} else if .namespace == "n8n-dev" {
  "n8n-dev"
} else if .namespace == "kafka" {
  "kafka"
} else if .namespace == "elastic-system" {
  "elastic-system"
} else if .namespace == "cnpg-system" {
  "cloudnative-pg"
} else if contains(string!(.namespace), "boutique") {
  "boutique-" + string!(.namespace)
} else {
  string!(.namespace)
}

# ═══════════════════════════════════════════════════════════════
# Map Severity (critical/warn/info/debug)
# ═══════════════════════════════════════════════════════════════
.severity = if .level == "error" || .level == "fatal" || .level == "critical" {
  "critical"
} else if .level == "warn" || .level == "warning" {
  "warn"
} else if .level == "debug" || .level == "trace" {
  "debug"
} else {
  "info"
}

# ═══════════════════════════════════════════════════════════════
# ENTERPRISE: ECS-Compliant Field Mappings
# ═══════════════════════════════════════════════════════════════

# ECS version
."ecs.version" = "8.17"

# Rename @timestamp for ECS compliance
."@timestamp" = .timestamp

# ECS log.level (normalized)
."log.level" = .level

# ECS service.name (primary identifier)
."service.name" = .service_name

# ECS service.environment
."service.environment" = "production"

# Add cluster metadata (custom fields)
.cluster = "talos-homelab"
.datacenter = "homelab"
.environment = "production"

# ═══════════════════════════════════════════════════════════════
# ENTERPRISE: OpenTelemetry Tracing Support
# ═══════════════════════════════════════════════════════════════

# Extract trace.id from headers/metadata (if exists)
if exists(.trace_id) {
  ."trace.id" = string!(.trace_id)
} else if exists(.traceId) {
  ."trace.id" = string!(.traceId)
} else if exists(.headers."X-Trace-Id") {
  ."trace.id" = string!(.headers."X-Trace-Id")
}

# Extract transaction.id (if exists)
if exists(.transaction_id) {
  ."transaction.id" = string!(.transaction_id)
} else if exists(.transactionId) {
  ."transaction.id" = string!(.transactionId)
}

# Extract span.id (if exists)
if exists(.span_id) {
  ."span.id" = string!(.span_id)
} else if exists(.spanId) {
  ."span.id" = string!(.spanId)
}

# ═══════════════════════════════════════════════════════════════
# ENTERPRISE: Structured Logging Support
# ═══════════════════════════════════════════════════════════════

# If log is JSON, preserve structured fields
if exists(.workflow_id) {
  ."workflow.id" = string!(.workflow_id)
}

if exists(.user_id) {
  ."user.id" = string!(.user_id)
}

if exists(.error) {
  ."error.message" = string!(.error)
}
'''

# Sample logs to reduce volume (optional)
[transforms.sample_logs]
type = "sample"
inputs = ["enrich_logs"]
rate = 100  # Keep all logs (set lower to sample, e.g. 10 = 10%)
key_field = ".kubernetes.pod"  # Sample per pod

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# ELASTICSEARCH SINK - ENTERPRISE DATA STREAMS + ECS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[sinks.elasticsearch]
type = "elasticsearch"
inputs = ["sample_logs"]
endpoints = ["https://production-cluster-es-http.elastic-system.svc.cluster.local:9200"]
mode = "data_stream"  # ← DATA STREAMS! (automatic rollover)
data_stream.type = "logs"
data_stream.dataset = "{{ service_name }}.{{ severity }}"  # Uses .service_name for routing
data_stream.namespace = "{{ namespace_suffix | default: \"default\" }}"  # ← ELASTIC BEST PRACTICE: Dynamic namespace!
api_version = "v8"

# Authentication
[sinks.elasticsearch.auth]
strategy = "basic"
user = "elastic"
password = "${ELASTICSEARCH_PASSWORD}"  # Set via secret

# TLS configuration
[sinks.elasticsearch.tls]
verify_certificate = false  # For self-signed certs
verify_hostname = false

# Buffer configuration
[sinks.elasticsearch.buffer]
type = "disk"
max_size = 268435488  # 256MB buffer
when_full = "drop_newest"

[sinks.elasticsearch.batch]
max_bytes = 10485760  # 10MB batches
timeout_secs = 10

[sinks.elasticsearch.request]
timeout_secs = 60
retry_attempts = 3

# Console output for debugging (optional)
[sinks.console]
type = "console"
inputs = ["sample_logs"]
encoding.codec = "json"
target = "stdout"
