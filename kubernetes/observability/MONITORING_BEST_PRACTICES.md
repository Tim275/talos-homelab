# ðŸ“Š Monitoring Best Practices - Grafana Blog & Industry Guides

**Curated Collection**: Wie die Enterprise deine Stack-Komponenten monitored

---

## ðŸŽ¯ **Inspiration: GitLab CI/CD Observability**

**Article**: [A serverless approach to CI/CD observability with GitLab and Grafana](https://grafana.com/blog/2025/10/10/a-serverless-approach-to-ci-cd-observability-with-gitlab-and-grafana/)

**Key Patterns**:
```yaml
What they monitor:
  â”œâ”€ Pipeline success/failure rates (real-time)
  â”œâ”€ Deployment frequency & timing
  â”œâ”€ Code changes correlated with system performance
  â””â”€ Build duration variations by project

Approach:
  â”œâ”€ GitLab webhooks â†’ AWS Lambda â†’ Grafana Loki
  â”œâ”€ Treating CI/CD events as structured logs (not isolated metrics)
  â”œâ”€ Deployment markers on application dashboards
  â””â”€ Event correlation: commits + deployments + system behavior

Best Practice:
  "The lines between application monitoring, infrastructure observability,
   and CI/CD tracking continue to blur."
```

**Why this matters**:
- âœ… Unified observability (nicht 5 separate Tools!)
- âœ… Deployment events als Grafana Annotations
- âœ… Correlate deploys with performance changes
- âœ… Single pane of glass fÃ¼r Dev + Ops

---

## ðŸ”¹ **ArgoCD GitOps Monitoring**

### ðŸ“– **Official Grafana Guide**

**Article**: [How to use Argo CD to configure Kubernetes Monitoring in Grafana Cloud](https://grafana.com/blog/2023/05/23/how-to-use-argo-cd-to-configure-kubernetes-monitoring-in-grafana-cloud/) (2023)

**Key Points**:
```yaml
GitOps-Native Monitoring:
  â”œâ”€ Dashboards as Code (Git repository)
  â”œâ”€ ArgoCD synchronizes dashboards to Grafana
  â”œâ”€ Grafana Operator manages Grafana instance
  â””â”€ No manual dashboard imports!

Metrics to Monitor:
  â”œâ”€ Application sync status (OutOfSync, Synced, Healthy)
  â”œâ”€ Sync frequency & duration
  â”œâ”€ Failed syncs & auto-heal events
  â””â”€ Drift detection (cluster vs Git)
```

### ðŸ“ˆ **ArgoCD Dashboards**

**Dashboard**: [ArgoCD Overview V3](https://grafana.com/grafana/dashboards/24192-argocd-overview/) (2024)

**Metrics**:
- `argocd_app_info` - Application metadata
- `argocd_app_sync_total` - Sync operations count
- `argocd_app_k8s_request_total` - Kubernetes API calls

**Alerts**:
```promql
# Application OutOfSync > 15 minutes
argocd_app_info{sync_status="OutOfSync"} == 1

# Application Degraded
argocd_app_info{health_status="Degraded"} == 1
```

**Your Setup**:
```yaml
Opportunity:
  â”œâ”€ 30+ ArgoCD Applications deployed
  â”œâ”€ GitOps fÃ¼r ALLES (infrastructure + apps)
  â””â”€ Missing: Deployment annotations in Grafana!

TODO:
  â”œâ”€ Add ArgoCD Dashboard (24192)
  â”œâ”€ Configure ArgoCD â†’ Grafana annotations
  â”‚  (z.B. "n8n-prod deployed at 14:23")
  â””â”€ Alert on OutOfSync > 15min
```

---

## ðŸ˜ **PostgreSQL (CNPG) Monitoring**

### ðŸ“– **Giant Swarm Production Guide**

**Article**: [Making Grafana remember: our journey to persistence with Grafana and PostgreSQL](https://www.giantswarm.io/blog/making-grafana-remember-our-journey-to-persistence-with-grafana-and-postgresql) (2 weeks ago!)

**Best Practices**:
```yaml
Key Metrics:
  â”œâ”€ Connection Pool Utilization
  â”‚  â””â”€ Alert: > 80% = scale replicas!
  â”œâ”€ Replication Lag (Primary â†’ Standby)
  â”‚  â””â”€ Alert: > 10s = investigate!
  â”œâ”€ WAL Archive Status
  â”‚  â””â”€ Failed backups = disaster risk!
  â””â”€ Query Performance (slow queries)

CNPG-Specific:
  â”œâ”€ Failover events (Primary switch)
  â”œâ”€ Backup success/failure rate
  â”œâ”€ PVC disk usage (Rook-Ceph)
  â””â”€ Fencing state (split-brain prevention)
```

### ðŸ“ˆ **Official CNPG Dashboard**

**Dashboard**: [PostgreSQL Database (CNPG)](https://grafana.com/grafana/dashboards/20417-postgresql-database/)

**Your Setup**:
```yaml
Current:
  â”œâ”€ n8n-prod PostgreSQL (CNPG cluster)
  â”œâ”€ n8n-dev PostgreSQL (single instance)
  â”œâ”€ Keycloak PostgreSQL
  â””â”€ Infisical PostgreSQL

Monitoring Status:
  âœ… Metrics exposed (CNPG Operator)
  âŒ Dashboard nicht deployed!
  âŒ Alerts nicht konfiguriert!

TODO:
  â”œâ”€ Deploy CNPG Dashboard (20417)
  â”œâ”€ Add alerts: Replication lag, connection pool
  â””â”€ Backup monitoring (Velero S3)
```

---

## ðŸ“¨ **Kafka (Strimzi) Monitoring**

### ðŸ“– **Grafana Formula 1 Telemetry Guide**

**Article**: [Real-time monitoring of Formula 1 telemetry data on Kubernetes with Grafana, Apache Kafka, and Strimzi](https://grafana.com/blog/2021/02/02/real-time-monitoring-of-formula-1-telemetry-data-on-kubernetes-with-grafana-apache-kafka-and-strimzi/)

**Key Patterns**:
```yaml
Kafka-Specific Metrics:
  â”œâ”€ Consumer Lag (CRITICAL!)
  â”‚  â””â”€ "How far behind is consumer from producer?"
  â”œâ”€ Broker Health (online/offline)
  â”œâ”€ Topic Throughput (messages/sec)
  â”œâ”€ Replication Factor (under-replicated partitions)
  â””â”€ Disk Usage (log retention)

JMX Exporter:
  â”œâ”€ Strimzi auto-deploys JMX Exporter sidecar
  â”œâ”€ Exposes Kafka metrics to Prometheus
  â””â”€ No manual configuration needed!
```

### ðŸ“ˆ **Strimzi Dashboards**

**Dashboard**: [Strimzi Kafka Exporter](https://grafana.com/grafana/dashboards/11285-strimzi-kafka-exporter/)

**Critical Alerts**:
```promql
# Consumer Lag > 1000 messages
kafka_consumergroup_lag > 1000

# Under-replicated partitions
kafka_topic_partition_under_replicated_partition > 0

# Offline brokers
kafka_brokers_online < 3
```

**Your Setup**:
```yaml
Current:
  â”œâ”€ Strimzi Kafka Cluster (3 brokers)
  â”œâ”€ Kafka Connect (experimental)
  â””â”€ Redpanda Console (UI)

TODO:
  â”œâ”€ Deploy Strimzi Dashboard (11285)
  â”œâ”€ Configure Consumer Lag alerts
  â””â”€ Monitor: n8n Kafka integration (wenn deployed)
```

---

## ðŸ”­ **OpenTelemetry Best Practices**

### ðŸ“– **Official Grafana OTel Guide**

**Article**: [OpenTelemetry best practices: A user's guide to getting started](https://grafana.com/blog/2023/12/18/opentelemetry-best-practices-a-users-guide-to-getting-started-with-opentelemetry/)

**Key Insights**:
```yaml
Best Practices:
  â”œâ”€ Use Grafana Agent (not raw OTel Collector)
  â”‚  â””â”€ "Grafana Agent is built on OTel Collector + optimized"
  â”œâ”€ Generate metrics from spans BEFORE tail sampling
  â”‚  â””â”€ Accurate request counts even with 10% sampling!
  â”œâ”€ Use semantic conventions (service.name, deployment.environment)
  â””â”€ Instrument at SDK level + Sidecar level (both!)

Sampling Strategy:
  â”œâ”€ Head Sampling: Simple but loses rare errors
  â”œâ”€ Tail Sampling: Complex but catches errors
  â””â”€ Recommended: Probabilistic (10%) + Error-based (100%)
```

### ðŸ“– **What's New in 2025**

**Article**: [OpenTelemetry and Grafana Labs: what's new and what's next in 2025](https://grafana.com/blog/2025/01/07/opentelemetry-and-grafana-labs-whats-new-and-whats-next-in-2025/)

**Industry Trends**:
```yaml
Adoption:
  â”œâ”€ 89% using Prometheus
  â”œâ”€ 85% using OpenTelemetry
  â””â”€ 40% using BOTH (overlap!)

New in 2024/2025:
  â”œâ”€ Profiling support (CPU, Memory profiles via OTel)
  â”œâ”€ Spring Boot Starter (stable!)
  â””â”€ 100% increase in Google search volume

Integration:
  â”œâ”€ Loki (logs) + Tempo (traces) + Mimir (metrics) + Pyroscope (profiling)
  â””â”€ "Single pane of glass" fÃ¼r ALLE Signale
```

### ðŸ“– **Kubernetes + OpenTelemetry**

**Article**: [Leveraging OpenTelemetry and Grafana for observing Kubernetes applications](https://grafana.com/blog/2024/11/22/leveraging-opentelemetry-and-grafana-for-observing-visualizing-and-monitoring-kubernetes-applications/)

**Architecture**:
```yaml
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application (N8N, Microservices)                   â”‚
â”‚ â””â”€ OpenTelemetry SDK (OTLP)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ OTLP (traces, metrics, logs)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenTelemetry Collector (DaemonSet)                â”‚
â”‚ â”œâ”€ Batch Processor (efficiency)                    â”‚
â”‚ â”œâ”€ Resource Detection (k8s.pod.name, namespace)    â”‚
â”‚ â””â”€ Tail Sampling (catch errors, 10% normal)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana Stack                                      â”‚
â”‚ â”œâ”€ Tempo (traces) â† Elasticsearch (your setup)    â”‚
â”‚ â”œâ”€ Loki (logs) â† S3                                â”‚
â”‚ â”œâ”€ Mimir (metrics) â† Prometheus                    â”‚
â”‚ â””â”€ Pyroscope (profiling)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Your Setup**:
```yaml
Current:
  âœ… OpenTelemetry Collector (DaemonSet) deployed
  âœ… Jaeger backend (Elasticsearch)
  âœ… OTLP endpoints (4317, 4318)
  âŒ No app instrumentation yet!

Best Practice Alignment:
  â”œâ”€ âœ… Batch processing (1s, 1024 batch size)
  â”œâ”€ âœ… Memory limiter (400MB)
  â”œâ”€ âœ… Resource attributes (cluster label)
  â””â”€ â³ TODO: Tail sampling (catch errors)

When you instrument apps:
  â”œâ”€ Use OpenTelemetry SDK (Python, Node.js)
  â”œâ”€ Set semantic attributes (service.name, deployment.environment)
  â””â”€ Traces will flow automatically to Jaeger!
```

---

## ðŸ—ï¸ **Talos Linux Monitoring**

### ðŸ“– **Community Best Practices**

**Article**: [Talos OS Raspberry PI: Prometheus and Grafana](https://blog.devops.dev/talos-os-raspberry-fc5f327b7026) (2024)

**Talos-Specific Metrics**:
```yaml
Node-Level:
  â”œâ”€ Kubelet metrics (cAdvisor)
  â”œâ”€ System services (etcd, containerd, kubelet)
  â”œâ”€ Disk I/O (Rook-Ceph critical!)
  â””â”€ Network throughput (Cilium)

Control Plane Health:
  â”œâ”€ etcd cluster health (quorum)
  â”œâ”€ API Server latency
  â”œâ”€ Scheduler performance
  â””â”€ Controller Manager lag

Talos API Metrics:
  â”œâ”€ Machine config status
  â”œâ”€ Upgrade status (version drift)
  â””â”€ Certificate expiration
```

### ðŸ“ˆ **Kube-Prometheus-Stack**

**Your Setup**:
```yaml
Deployed:
  âœ… Kube-Prometheus-Stack (Prometheus Operator)
  âœ… 50+ pre-built dashboards
  âœ… Grafana + Alertmanager
  âœ… ServiceMonitors for auto-discovery

Best Dashboards for Talos:
  â”œâ”€ Kubernetes / Compute Resources / Cluster
  â”œâ”€ Kubernetes / Compute Resources / Namespace (Pods)
  â”œâ”€ Kubernetes / USE Method / Cluster (Utilization, Saturation, Errors)
  â””â”€ etcd (Control Plane Health)

TODO:
  â”œâ”€ Review all 50 dashboards (welche sind relevant?)
  â”œâ”€ Add custom Talos dashboard (machine config status)
  â””â”€ Alert: etcd cluster health
```

---

## ðŸ—„ï¸ **Rook-Ceph Storage Monitoring**

### ðŸ“– **Rook Documentation**

**Guide**: [Rook Ceph Monitoring](https://rook.io/docs/rook/latest/Storage-Configuration/Monitoring/ceph-monitoring/)

**Critical Metrics**:
```yaml
Cluster Health:
  â”œâ”€ HEALTH_OK / HEALTH_WARN / HEALTH_ERR
  â”œâ”€ OSDs up/down status (disk failures!)
  â”œâ”€ MON quorum (control plane)
  â””â”€ MGR failover events

Performance:
  â”œâ”€ Read/Write IOPS per pool
  â”œâ”€ Latency (RBD vs CephFS vs RGW)
  â”œâ”€ PG (Placement Group) states
  â””â”€ Scrubbing/deep-scrubbing progress

Capacity:
  â”œâ”€ Used vs Available (per pool)
  â”œâ”€ PVC growth rate (predict exhaustion!)
  â””â”€ Object count (S3 buckets)
```

### ðŸ“ˆ **Ceph Dashboards**

**Dashboard**: [Ceph Cluster](https://grafana.com/grafana/dashboards/2842-ceph-cluster/)

**Alerts**:
```promql
# Ceph cluster not healthy
ceph_health_status != 0

# OSD down
ceph_osd_up == 0

# Disk space < 15%
ceph_cluster_total_bytes - ceph_cluster_total_used_bytes < 0.15 * ceph_cluster_total_bytes
```

**Your Setup**:
```yaml
Current:
  â”œâ”€ Rook-Ceph v1.16.0
  â”œâ”€ 3 OSDs (NVMe disks)
  â”œâ”€ Block (RBD) + Object (RGW) + Filesystem (CephFS)
  â””â”€ 30+ PVCs (PostgreSQL, Redis, etc.)

Monitoring Status:
  âœ… Ceph metrics exposed (Prometheus)
  âŒ Dashboard nicht konfiguriert!
  âŒ Capacity alerts fehlen!

TODO:
  â”œâ”€ Deploy Ceph Dashboard (2842)
  â”œâ”€ Alert: OSD down, cluster degraded
  â””â”€ Capacity planning (PVC growth rate)
```

---

## ðŸŒ **Istio Service Mesh Monitoring**

### ðŸ“– **Istio Official Docs**

**Guide**: [Visualizing Your Mesh](https://istio.io/latest/docs/tasks/observability/metrics/querying-metrics/)

**Golden Signals**:
```yaml
Request Rate:
  â”œâ”€ istio_requests_total (per service)
  â””â”€ Breakdown: source, destination, response_code

Latency:
  â”œâ”€ istio_request_duration_milliseconds (histogram)
  â””â”€ p50, p90, p99 latencies

Error Rate:
  â”œâ”€ 4xx errors (client errors)
  â”œâ”€ 5xx errors (server errors)
  â””â”€ Connection failures

Saturation:
  â”œâ”€ Concurrent connections
  â””â”€ Circuit breaker trips
```

### ðŸ“ˆ **Kiali Dashboard**

**Your Setup**:
```yaml
Deployed:
  âœ… Istio v1.27.1 (3 istiod replicas)
  âœ… Kiali (Service Mesh UI)
  âœ… Sail Operator

Kiali Features:
  â”œâ”€ Service graph (visual topology)
  â”œâ”€ Traffic flow (request paths)
  â”œâ”€ mTLS status (per workload)
  â””â”€ Configuration validation (VirtualService errors)

When you deploy microservices:
  â”œâ”€ Namespace label: istio-injection=enabled
  â”œâ”€ Services auto-get sidecars
  â””â”€ Metrics flow to Prometheus automatically!
```

---

## ðŸ“ **Summary: Your Monitoring Roadmap**

### âœ… **Already Deployed**

```yaml
Foundation:
  âœ… Prometheus (Kube-Prometheus-Stack)
  âœ… Grafana (50+ dashboards)
  âœ… Alertmanager (3 tiers)
  âœ… OpenTelemetry Collector (DaemonSet)
  âœ… Jaeger (Elasticsearch backend)
  âœ… Kiali (Istio mesh visualization)
```

### â³ **Missing / TODO**

```yaml
High Priority:
  â”œâ”€ ArgoCD Dashboard + deployment annotations
  â”œâ”€ PostgreSQL (CNPG) Dashboard + replication alerts
  â”œâ”€ Kafka (Strimzi) Dashboard + consumer lag alerts
  â””â”€ Rook-Ceph Dashboard + capacity alerts

Medium Priority:
  â”œâ”€ Custom Talos dashboard (machine config status)
  â”œâ”€ OpenTelemetry tail sampling (error-based)
  â””â”€ Istio golden signals (when microservices deployed)

Low Priority:
  â”œâ”€ Cleanup unused dashboards (50 ist zu viel!)
  â”œâ”€ Dashboard as Code (ArgoCD GitOps)
  â””â”€ Grafana annotations from ArgoCD webhooks
```

### ðŸŽ¯ **Best Practice Patterns (learned from articles)**

```yaml
1. Unified Observability:
   â””â”€ GitLab Artikel: CI/CD events + app metrics auf EINEM Dashboard!
      Apply to: ArgoCD deploys + N8N performance auf 1 Dashboard

2. Deployment Markers:
   â””â”€ Deployments als Grafana Annotations
      "N8N v2.0 deployed at 14:23" â†’ sehe Performance impact!

3. Event Correlation:
   â””â”€ Correlate: Git commit â†’ ArgoCD sync â†’ Pod restart â†’ Latency spike
      Full story in one view!

4. Structured Logging:
   â””â”€ Treat events as logs (not just metrics)
      ArgoCD sync events â†’ Loki â†’ Grafana queries

5. Proactive Alerts:
   â””â”€ Alert BEFORE failure:
      - Replication lag > 10s (not when broken!)
      - Disk space < 15% (not when full!)
      - Consumer lag growing (not when backlog huge!)
```

---

## ðŸ“š **Full Article List**

### Grafana Official Blog

1. **CI/CD Observability** (2025-10-10)
   https://grafana.com/blog/2025/10/10/a-serverless-approach-to-ci-cd-observability-with-gitlab-and-grafana/

2. **ArgoCD Kubernetes Monitoring** (2023-05-23)
   https://grafana.com/blog/2023/05/23/how-to-use-argo-cd-to-configure-kubernetes-monitoring-in-grafana-cloud/

3. **Grafana Operator & GitOps** (2024-01-25)
   https://grafana.com/blog/2024/01/25/how-to-manage-grafana-instances-within-kubernetes/

4. **OpenTelemetry Best Practices** (2023-12-18)
   https://grafana.com/blog/2023/12/18/opentelemetry-best-practices-a-users-guide-to-getting-started-with-opentelemetry/

5. **OpenTelemetry 2025 Trends** (2025-01-07)
   https://grafana.com/blog/2025/01/07/opentelemetry-and-grafana-labs-whats-new-and-whats-next-in-2025/

6. **Kubernetes + OpenTelemetry** (2024-11-22)
   https://grafana.com/blog/2024/11/22/leveraging-opentelemetry-and-grafana-for-observing-visualizing-and-monitoring-kubernetes-applications/

7. **Kafka + Strimzi (Formula 1)** (2021-02-02)
   https://grafana.com/blog/2021/02/02/real-time-monitoring-of-formula-1-telemetry-data-on-kubernetes-with-grafana-apache-kafka-and-strimzi/

### Community Guides

8. **PostgreSQL CNPG Production** (2 weeks ago)
   https://www.giantswarm.io/blog/making-grafana-remember-our-journey-to-persistence-with-grafana-and-postgresql

9. **Talos OS Monitoring** (2024)
   https://blog.devops.dev/talos-os-raspberry-fc5f327b7026

10. **ArgoCD Grafana Alerts** (2025-09)
    https://medium.com/@nadavshaham/monitoring-argocd-applications-with-grafana-alerts-a-scalable-helm-based-approach-0d6ad814f1d5

---

## ðŸš€ **Next Steps**

1. **Review dashboards**: Check welche der 50+ Dashboards relevant sind
2. **Deploy missing dashboards**: ArgoCD, CNPG, Kafka, Ceph
3. **Configure alerts**: Replication lag, consumer lag, disk space
4. **GitOps dashboards**: Dashboards as Code via ArgoCD
5. **Deployment annotations**: ArgoCD webhooks â†’ Grafana annotations

---

**Last Updated**: 2025-10-23
**Source**: Grafana Labs Official Blog + Community Best Practices
