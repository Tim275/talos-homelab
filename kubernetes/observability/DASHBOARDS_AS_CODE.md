# üìä Dashboards as Code - Implementation Guide

**How we manage 80+ Grafana Dashboards as Infrastructure as Code**

---

## üéØ **Architecture Overview**

```yaml
Stack:
  ‚îú‚îÄ Grafana Operator (v5.19.1)
  ‚îÇ  ‚îî‚îÄ Manages Grafana instance lifecycle
  ‚îú‚îÄ GrafanaDashboard CRD (grafana.integreatly.org/v1beta1)
  ‚îÇ  ‚îî‚îÄ Dashboard definitions as YAML
  ‚îú‚îÄ GrafanaDataSource CRD
  ‚îÇ  ‚îî‚îÄ Datasources (Prometheus, Loki, Alertmanager)
  ‚îî‚îÄ ArgoCD
     ‚îî‚îÄ GitOps sync from Git ‚Üí Kubernetes

Benefits:
  ‚úÖ Version control (Git history)
  ‚úÖ Code review (Pull Requests)
  ‚úÖ Automated deployment (ArgoCD)
  ‚úÖ Declarative (desired state)
  ‚úÖ No manual imports!
```

---

## üìù **How It Works**

### 1. **GrafanaDashboard CRD Pattern**

```yaml
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDashboard
metadata:
  name: argocd-overview-v3
  namespace: grafana
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/component: dashboard
    dashboard-tier: argocd
spec:
  # üéØ Which Grafana instance to deploy to
  instanceSelector:
    matchLabels:
      app: grafana

  # üìÅ Dashboard folder in Grafana UI
  folder: "ArgoCD"

  # üîÑ Allow cross-namespace import (dashboards ‚Üí grafana instance)
  allowCrossNamespaceImport: true

  # üìä Dashboard JSON (embedded)
  json: |
    {
      "dashboard": {
        "title": "ArgoCD Overview",
        "panels": [...]
      }
    }
```

**Key Concepts:**

- **instanceSelector**: Matches Grafana CR with label `app: grafana`
- **folder**: Organizes dashboards in Grafana UI (not filesystem!)
- **json**: Full Grafana dashboard JSON embedded in YAML
- **allowCrossNamespaceImport**: Dashboard in any namespace can target Grafana in `grafana` namespace

### 2. **Datasource Pattern**

```yaml
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDatasource
metadata:
  name: prometheus-operated
  namespace: grafana
spec:
  instanceSelector:
    matchLabels:
      app: grafana
  datasource:
    name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus-operator-kube-p-prometheus.monitoring:9090
    isDefault: true
    jsonData:
      timeInterval: 30s
```

---

## üìÇ **Directory Structure**

```
kubernetes/infrastructure/monitoring/grafana/
‚îú‚îÄ‚îÄ kustomization.yaml                    # Main kustomize file (80+ dashboard references)
‚îú‚îÄ‚îÄ grafana.yaml                          # Grafana CR (instance config)
‚îú‚îÄ‚îÄ http-route.yaml                       # HTTPRoute (Envoy Gateway)
‚îú‚îÄ‚îÄ datasources/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus-operated.yaml          # ‚úÖ Main Prometheus datasource
‚îÇ   ‚îú‚îÄ‚îÄ loki.yaml                         # ‚úÖ Loki logs datasource
‚îÇ   ‚îî‚îÄ‚îÄ alertmanager-operated.yaml        # ‚úÖ Alertmanager datasource
‚îî‚îÄ‚îÄ enterprise-dashboards/
    ‚îú‚îÄ‚îÄ tier0-executive/
    ‚îÇ   ‚îú‚îÄ‚îÄ k8s-global-view.yaml          # Executive overview
    ‚îÇ   ‚îî‚îÄ‚îÄ node-system-overview.yaml     # Node-level metrics
    ‚îú‚îÄ‚îÄ argocd/
    ‚îÇ   ‚îú‚îÄ‚îÄ official-argocd-overview-v3.yaml      # ID: 24192 ‚úÖ
    ‚îÇ   ‚îú‚îÄ‚îÄ official-argocd-operational.yaml      # ID: 19993
    ‚îÇ   ‚îî‚îÄ‚îÄ official-argocd-application.yaml      # ID: 19974
    ‚îú‚îÄ‚îÄ postgresql/
    ‚îÇ   ‚îú‚îÄ‚îÄ postgresql-cnpg.yaml          # Custom CNPG dashboard
    ‚îÇ   ‚îî‚îÄ‚îÄ official-cloudnativepg.yaml   # ID: 20417 ‚úÖ
    ‚îú‚îÄ‚îÄ kafka/
    ‚îÇ   ‚îú‚îÄ‚îÄ kafka-strimzi.yaml            # Custom Strimzi dashboard
    ‚îÇ   ‚îú‚îÄ‚îÄ official-kafka-exporter.yaml  # ID: 7589
    ‚îÇ   ‚îî‚îÄ‚îÄ official-kafka-cluster.yaml   # ID: 14505
    ‚îú‚îÄ‚îÄ ceph/
    ‚îÇ   ‚îú‚îÄ‚îÄ official-ceph-cluster.yaml    # ID: 2842 ‚úÖ
    ‚îÇ   ‚îú‚îÄ‚îÄ official-ceph-pools.yaml      # ID: 5342
    ‚îÇ   ‚îî‚îÄ‚îÄ official-ceph-osd.yaml        # ID: 5336
    ‚îú‚îÄ‚îÄ opentelemetry/
    ‚îÇ   ‚îú‚îÄ‚îÄ official-opentelemetry-collector.yaml # ID: 15983
    ‚îÇ   ‚îî‚îÄ‚îÄ official-opentelemetry-apm.yaml       # ID: 19419
    ‚îî‚îÄ‚îÄ ...80+ more dashboards
```

---

## üîß **ServiceMonitor vs PodMonitor**

### **When to use what?**

```yaml
ServiceMonitor (Most Common):
  ‚îú‚îÄ Scrapes metrics via Kubernetes Service
  ‚îú‚îÄ Use when: Service exposes metrics endpoint
  ‚îú‚îÄ Example: Kafka Exporter, Jaeger, ArgoCD
  ‚îî‚îÄ Pattern: Service ‚Üí ServiceMonitor ‚Üí Prometheus

PodMonitor (Special Cases):
  ‚îú‚îÄ Scrapes metrics directly from Pods (bypasses Service)
  ‚îú‚îÄ Use when: Pods expose metrics but no dedicated Service
  ‚îú‚îÄ Example: CNPG PostgreSQL clusters
  ‚îî‚îÄ Pattern: Pod ‚Üí PodMonitor ‚Üí Prometheus
```

### **ServiceMonitor Example (Kafka)**

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kafka-exporter
  namespace: monitoring
  labels:
    release: kube-prometheus-stack  # ‚úÖ Prometheus Operator selector
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka-exporter
  namespaceSelector:
    matchNames:
      - kafka
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
```

**How it works:**
1. ServiceMonitor matches Service with label `app.kubernetes.io/name: kafka-exporter`
2. Prometheus Operator generates Prometheus scrape config
3. Prometheus scrapes `http://kafka-exporter.kafka:9308/metrics` every 30s

### **PodMonitor Example (CNPG PostgreSQL)**

```yaml
# ‚ö†Ô∏è PodMonitor is auto-created by CNPG Operator!
# Enable via Cluster CR:

apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: n8n-postgres
  namespace: n8n-prod
spec:
  instances: 2

  # üìä THIS CREATES PodMonitor AUTOMATICALLY!
  monitoring:
    enablePodMonitor: true                    # ‚úÖ Enable metrics scraping
    customQueriesConfigMap:
      - name: cnpg-default-monitoring         # ‚úÖ Custom queries (lag, connections, etc.)
        key: queries
    disableDefaultQueries: false              # ‚úÖ Keep default metrics
```

**Result:**
- CNPG Operator creates PodMonitor: `n8n-postgres` in `n8n-prod` namespace
- Prometheus scrapes Pod on port `9187` (CNPG metrics port)
- Metrics: `cnpg_pg_database_size_bytes`, `cnpg_pg_replication_lag_seconds`, etc.

---

## ‚úÖ **What We Just Fixed: PostgreSQL Monitoring**

### **Problem**

```yaml
Before:
  ‚îú‚îÄ CNPG Dashboard deployed ‚úÖ
  ‚îú‚îÄ CNPG Pods running ‚úÖ
  ‚îú‚îÄ Metrics exposed on port 9187 ‚úÖ
  ‚îî‚îÄ BUT: enablePodMonitor: false ‚ùå
     Result: Prometheus NOT scraping ‚Üí Dashboard EMPTY!
```

### **Solution**

**Files Modified:**
1. `/platform/data/n8n-prod-cnpg/cluster.yaml`
2. `/platform/data/n8n-dev-cnpg/postgres-cluster.yaml`
3. `/platform/identity/keycloak/postgres-cluster.yaml`
4. `/platform/identity/infisical/postgres-cluster.yaml`

**Changes:**
```yaml
spec:
  monitoring:
    enablePodMonitor: true                    # ‚úÖ Enable PodMonitor creation
    customQueriesConfigMap:
      - name: cnpg-default-monitoring
        key: queries
    disableDefaultQueries: false
```

**Result:**
```bash
$ kubectl get podmonitor -A | grep postgres
infisical    infisical-postgres   4d19h
n8n-dev      n8n-postgres         5m
n8n-prod     n8n-postgres         5m
keycloak     keycloak-db          13d

$ kubectl exec -n monitoring prometheus-0 -- \
    wget -qO- 'http://localhost:9090/api/v1/query?query=cnpg_pg_database_size_bytes'
# 28 time series found! ‚úÖ
```

---

## üìä **Dashboard Status - What Works Now**

### ‚úÖ **Working Dashboards (Metrics Flowing)**

```yaml
ArgoCD:
  ‚îú‚îÄ ArgoCD Overview V3 (24192)              # ‚úÖ 6 ServiceMonitors
  ‚îú‚îÄ ArgoCD Operational (19993)              # ‚úÖ Application metrics
  ‚îú‚îÄ ArgoCD Application (19974)              # ‚úÖ Sync status
  ‚îî‚îÄ ArgoCD Notifications (19975)            # ‚úÖ Notification metrics

PostgreSQL (CNPG):
  ‚îú‚îÄ CloudNativePG Official (20417)          # ‚úÖ 4 PodMonitors (JUST FIXED!)
  ‚îî‚îÄ Custom CNPG Dashboard                   # ‚úÖ Replication lag, connections

Kafka (Strimzi):
  ‚îú‚îÄ Kafka Exporter (7589)                   # ‚úÖ 3 ServiceMonitors
  ‚îú‚îÄ Kafka Cluster (14505)                   # ‚úÖ Broker metrics
  ‚îî‚îÄ Kafka Topics (14506)                    # ‚úÖ Topic metrics

Ceph Storage:
  ‚îú‚îÄ Ceph Cluster (2842)                     # ‚úÖ Rook-Ceph metrics
  ‚îú‚îÄ Ceph Pools (5342)                       # ‚úÖ Pool capacity
  ‚îî‚îÄ Ceph OSD (5336)                         # ‚úÖ OSD health

OpenTelemetry:
  ‚îú‚îÄ OTel Collector (15983)                  # ‚úÖ Traces, metrics, logs
  ‚îî‚îÄ OTel APM (19419)                        # ‚úÖ Application performance

Kubernetes:
  ‚îú‚îÄ Global View (15757)                     # ‚úÖ Cluster overview
  ‚îú‚îÄ API Server (15761)                      # ‚úÖ API latency
  ‚îú‚îÄ etcd (20330)                            # ‚úÖ Control plane health
  ‚îî‚îÄ 10+ more K8s dashboards                 # ‚úÖ All working

Istio Service Mesh:
  ‚îú‚îÄ Istio Mesh (7639)                       # ‚úÖ Service graph
  ‚îú‚îÄ Istio Service (7636)                    # ‚úÖ Request rate
  ‚îî‚îÄ Istio Control Plane (7645)              # ‚úÖ istiod health

Total: 80+ Dashboards deployed as IaC ‚úÖ
```

---

## üöÄ **How to Add New Dashboard**

### **Method 1: From Grafana.com (Recommended)**

1. **Find dashboard on grafana.com**
   ```bash
   # Example: https://grafana.com/grafana/dashboards/24192
   Dashboard ID: 24192
   Name: ArgoCD Overview V3
   ```

2. **Download JSON**
   ```bash
   curl -o argocd-overview.json \
     https://grafana.com/api/dashboards/24192/revisions/1/download
   ```

3. **Create GrafanaDashboard YAML**
   ```bash
   cat > official-argocd-overview-v3.yaml <<EOF
   apiVersion: grafana.integreatly.org/v1beta1
   kind: GrafanaDashboard
   metadata:
     name: official-argocd-overview-v3
     namespace: grafana
     labels:
       app.kubernetes.io/name: grafana
       app.kubernetes.io/component: dashboard
       dashboard-tier: argocd
   spec:
     allowCrossNamespaceImport: true
     folder: "ArgoCD"
     instanceSelector:
       matchLabels:
         app: grafana
     json: |
       $(cat argocd-overview.json | sed 's/^/      /')
   EOF
   ```

4. **Add to kustomization.yaml**
   ```yaml
   resources:
     - enterprise-dashboards/argocd/official-argocd-overview-v3.yaml
   ```

5. **Commit + Push**
   ```bash
   git add .
   git commit -m "feat: add ArgoCD Overview V3 dashboard (ID: 24192)"
   git push
   ```

6. **ArgoCD auto-syncs** (or manual: `kubectl patch application grafana -n argocd --type merge -p '{"operation":{"sync":{}}}'`)

### **Method 2: Custom Dashboard**

1. **Create in Grafana UI** (http://grafana.timourhomelab.org)
2. **Export JSON** (Share ‚Üí Export ‚Üí Save to file)
3. **Convert to YAML** (same as Method 1, step 3)
4. **Commit to Git**

---

## üîç **Troubleshooting**

### **Dashboard not appearing in Grafana?**

```bash
# 1. Check if GrafanaDashboard CR exists
kubectl get grafanadashboard -n grafana | grep argocd

# 2. Check status
kubectl describe grafanadashboard official-argocd-overview-v3 -n grafana

# 3. Check if instance selector matches
kubectl get grafana grafana -n grafana -o jsonpath='{.metadata.labels}'
# Should have: "app":"grafana"

# 4. Check Grafana Operator logs
kubectl logs -n grafana -l app.kubernetes.io/name=grafana-operator
```

### **Dashboard shows "No Data"?**

```bash
# 1. Check if ServiceMonitor/PodMonitor exists
kubectl get servicemonitor,podmonitor -A | grep <component>

# 2. Check if metrics are in Prometheus
kubectl port-forward -n monitoring prometheus-kube-prometheus-stack-prometheus-0 9090
# Open: http://localhost:9090/targets
# Search for your component

# 3. Check metric names in Prometheus
# Example: cnpg_pg_database_size_bytes

# 4. Check datasource in Grafana
# Settings ‚Üí Data Sources ‚Üí Prometheus
# Test & Save
```

### **PodMonitor not created (CNPG)?**

```bash
# 1. Check if monitoring is enabled in Cluster CR
kubectl get cluster.postgresql.cnpg.io <name> -n <namespace> -o yaml | grep -A 5 monitoring

# Expected:
#   monitoring:
#     enablePodMonitor: true

# 2. If missing, add to Cluster YAML and sync ArgoCD

# 3. Verify PodMonitor created
kubectl get podmonitor -n <namespace>

# 4. Check CNPG Operator logs
kubectl logs -n cnpg-system -l app.kubernetes.io/name=cloudnative-pg
```

---

## üìö **Best Practices**

### ‚úÖ **DO**

- **Organize by component** (`argocd/`, `postgresql/`, `kafka/`)
- **Prefix with `official-`** for Grafana.com dashboards
- **Use descriptive names** (`official-argocd-overview-v3.yaml`)
- **Add dashboard ID in comment** (`# ID: 24192`)
- **Set proper folder** (`folder: "ArgoCD"`)
- **Version control everything** (Git history = audit trail)
- **Test in dev first** (deploy to dev Grafana instance first)

### ‚ùå **DON'T**

- **Manual imports** (use GrafanaDashboard CRD instead!)
- **Duplicate dashboards** (check if already exists)
- **Hardcode datasource UIDs** (use `${DS_PROMETHEUS}` variables)
- **Skip instance selector** (dashboard won't deploy!)
- **Forget allowCrossNamespaceImport** (if dashboard in different namespace)

---

## üéØ **Summary: What We Built**

```yaml
Infrastructure as Code:
  ‚îú‚îÄ 80+ Grafana Dashboards (GrafanaDashboard CRDs)
  ‚îú‚îÄ 3 Datasources (Prometheus, Loki, Alertmanager)
  ‚îú‚îÄ 50+ ServiceMonitors (Kafka, ArgoCD, Jaeger, etc.)
  ‚îú‚îÄ 4 PodMonitors (CNPG PostgreSQL clusters)
  ‚îî‚îÄ GitOps deployment (ArgoCD)

Benefits:
  ‚úÖ Zero manual dashboard imports
  ‚úÖ Version control (every change tracked)
  ‚úÖ Code review (PR approval required)
  ‚úÖ Automated deployment (push = deploy)
  ‚úÖ Disaster recovery (Git = source of truth)
  ‚úÖ Multi-environment (dev/prod from same repo)

Result:
  üéâ Enterprise-grade Dashboard Management!
```

---

## üìñ **Related Documentation**

- [MONITORING_BEST_PRACTICES.md](./MONITORING_BEST_PRACTICES.md) - Industry best practices from Grafana blog
- [TRACING_STORAGE_RESEARCH.md](./TRACING_STORAGE_RESEARCH.md) - Jaeger vs Tempo comparison
- [PRODUCTION_OBSERVABILITY_STACK.md](./PRODUCTION_OBSERVABILITY_STACK.md) - Full stack overview

---

**Last Updated**: 2025-10-23
**Status**: Production-Ready ‚úÖ
