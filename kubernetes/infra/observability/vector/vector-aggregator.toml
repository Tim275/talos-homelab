# Vector Aggregator Configuration - Central log processor (replaces Fluentd)
# Runs as Deployment with 2 replicas

data_dir = "/vector-data-dir"

[api]
enabled = true
address = "0.0.0.0:8686"

# Receive from Vector Agents
[sources.vector_agents]
type = "vector"
address = "0.0.0.0:6000"
version = "2"

# Proxmox syslog input
[sources.proxmox_syslog]
type = "syslog"
address = "0.0.0.0:5140"
mode = "udp"

# Direct Talos etcd logs from control plane
[sources.talos_etcd_logs]
type = "exec"
mode = "scheduled"
command = ["talosctl", "-n", "192.168.68.101", "logs", "etcd", "--tail"]
scheduled.exec_interval_secs = 30

# Direct Talos kubelet logs from workers (sample from worker-1)
[sources.talos_kubelet_logs]
type = "exec"
mode = "scheduled"
command = ["talosctl", "-n", "192.168.68.103", "logs", "kubelet", "--tail"]
scheduled.exec_interval_secs = 30

# Process Proxmox logs and add metadata
[transforms.process_proxmox_logs]
type = "remap"
inputs = ["proxmox_syslog"]
source = '''
.source = "proxmox"
.cluster = "talos-homelab"
.datacenter = "homelab"
.node_type = "hypervisor"
'''

# Process Talos system logs from direct talosctl access
[transforms.process_talos_system_logs]
type = "remap"
inputs = ["talos_etcd_logs", "talos_kubelet_logs"]
source = '''
.source = "talos-direct"
.cluster = "talos-homelab"
.environment = "production"

# Detect log type and assign proper index
if contains(string!(.message), "etcd") || exists(.component) && .component == "etcd" {
  .service = "etcd"
  .node_role = "control-plane"
  .index_name = "etcd-control-plane-direct"
} else if contains(string!(.message), "kubelet") {
  .service = "kubelet"
  .node_role = "worker"
  .index_name = "kubelet-worker-direct"
} else {
  .service = "talos-system"
  .node_role = "unknown"
  .index_name = "talos-system-direct"
}

# Extract node IP from message context
if exists(.node_ip) {
  .node_ip = .node_ip
} else {
  .node_ip = "192.168.68.101"  # Default to control plane
}
'''

# Parse and enrich all logs
[transforms.enrich_logs]
type = "remap"
inputs = ["vector_agents", "process_proxmox_logs", "process_talos_system_logs"]
source = '''
# Add timestamp if missing
if !exists(.timestamp) {
  .timestamp = now()
}

# Standardize log level
if exists(.level) {
  .level = downcase(string!(.level))
} else if contains(string!(.message), "ERROR") || contains(string!(.message), "error") {
  .level = "error"
} else if contains(string!(.message), "WARN") || contains(string!(.message), "warn") {
  .level = "warn"
} else {
  .level = "info"
}

# Add granular index routing field with enhanced patterns
if exists(.kubernetes.namespace) && exists(.kubernetes.pod_name) {
  # Create index per application: app-namespace-YYYY.MM.DD
  if exists(.kubernetes.pod_labels.app) {
    .index_name = string!(.kubernetes.pod_labels.app) + "-" + string!(.kubernetes.namespace)
  } else if exists(.kubernetes.pod_labels."app.kubernetes.io/name") {
    .index_name = string!(.kubernetes.pod_labels."app.kubernetes.io/name") + "-" + string!(.kubernetes.namespace)
  } else if contains(string!(.kubernetes.pod_name), "etcd") {
    .index_name = "etcd-control-plane"
  } else if contains(string!(.kubernetes.pod_name), "kube-apiserver") {
    .index_name = "kube-apiserver-control-plane"
  } else if contains(string!(.kubernetes.pod_name), "kube-scheduler") {
    .index_name = "kube-scheduler-control-plane"
  } else if contains(string!(.kubernetes.pod_name), "kube-controller") {
    .index_name = "kube-controller-control-plane"
  } else if string!(.kubernetes.namespace) == "kube-system" {
    .index_name = "system-" + string!(.kubernetes.container_name) + "-kube-system"
  } else {
    .index_name = "pod-" + string!(.kubernetes.namespace)
  }
} else if .source == "proxmox" && exists(.host) {
  # Create index per Proxmox node: proxmox-hostname-YYYY.MM.DD
  .index_name = "proxmox-" + string!(.host)
} else if exists(.kubernetes.node_name) {
  # Create index per Kubernetes node: node-nodename-YYYY.MM.DD
  .index_name = "node-" + string!(.kubernetes.node_name)
} else {
  .index_name = "general"
}
'''

# Sample logs to reduce volume (optional)
[transforms.sample_logs]
type = "sample"
inputs = ["enrich_logs"]
rate = 100  # Keep all logs (set lower to sample, e.g. 10 = 10%)
key_field = ".kubernetes.pod"  # Sample per pod

# Elasticsearch sink
[sinks.elasticsearch]
type = "elasticsearch"
inputs = ["sample_logs"]
endpoints = ["https://production-cluster-es-http.elastic-system.svc.cluster.local:9200"]
mode = "bulk"
bulk.index = "vector-{{ .index_name }}-%Y.%m.%d"
api_version = "v8"

# Authentication
[sinks.elasticsearch.auth]
strategy = "basic"
user = "elastic"
password = "${ELASTICSEARCH_PASSWORD}"  # Set via secret

# TLS configuration
[sinks.elasticsearch.tls]
verify_certificate = false  # For self-signed certs
verify_hostname = false

# Buffer configuration
[sinks.elasticsearch.buffer]
type = "disk"
max_size = 268435488  # 256MB buffer
when_full = "drop_newest"

[sinks.elasticsearch.batch]
max_bytes = 10485760  # 10MB batches
timeout_secs = 10

[sinks.elasticsearch.request]
timeout_secs = 60
retry_attempts = 3

# Console output for debugging (optional)
[sinks.console]
type = "console"
inputs = ["sample_logs"]
encoding.codec = "json"
target = "stdout"

