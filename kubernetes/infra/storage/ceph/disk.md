# Ceph OSD Disk Configuration Guide

## ğŸ”§ AKTUELLER ZUSTAND (Broken)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   work-01       â”‚   work-02       â”‚   work-03       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚scsi0 (40GB) â”‚ â”‚ â”‚scsi0 (40GB) â”‚ â”‚ â”‚scsi0 (40GB) â”‚ â”‚
â”‚ â”‚Talos OS     â”‚ â”‚ â”‚Talos OS     â”‚ â”‚ â”‚Talos OS     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚                 â”‚                 â”‚
â”‚ âŒ Keine OSD    â”‚ âŒ Keine OSD    â”‚ âŒ Keine OSD    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Resultat:**
```
Ceph Cluster Status: âŒ NO OSDs
â”œâ”€â”€ MON Pods: âœ… Running  
â”œâ”€â”€ MGR Pods: âœ… Running  
â”œâ”€â”€ OSD Pods: âŒ NONE     
â””â”€â”€ Storage Pools: âŒ Progressing/Failed

Kafka PVCs: âŒ Pending (No storage available!)
```

## ğŸš€ NACH DISK HINZUFÃœGUNG (Working)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   work-01       â”‚   work-02       â”‚   work-03       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚scsi0 (40GB) â”‚ â”‚ â”‚scsi0 (40GB) â”‚ â”‚ â”‚scsi0 (40GB) â”‚ â”‚
â”‚ â”‚Talos OS     â”‚ â”‚ â”‚Talos OS     â”‚ â”‚ â”‚Talos OS     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚                 â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚scsi1 (50GB) â”‚ â”‚ â”‚scsi1 (50GB) â”‚ â”‚ â”‚scsi1 (50GB) â”‚ â”‚
â”‚ â”‚Ceph OSD     â”‚ â”‚ â”‚Ceph OSD     â”‚ â”‚ â”‚Ceph OSD     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚                 â”‚                 â”‚
â”‚ âœ… OSD Pod     â”‚ âœ… OSD Pod     â”‚ âœ… OSD Pod     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Resultat:**
```
Ceph Cluster Status: âœ… HEALTHY
â”œâ”€â”€ MON Pods: âœ… Running  
â”œâ”€â”€ MGR Pods: âœ… Running  
â”œâ”€â”€ OSD Pods: âœ… 6x Running (6 workers Ã— 1 OSD each)
â””â”€â”€ Storage Pools: âœ… Ready

Kafka PVCs: âœ… Bound (Storage available!)
```

## ğŸ’¾ WAS IST EIN CEPH OSD?

**OSD = Object Storage Daemon**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CEPH CLUSTER             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ MON â”‚    â”‚ MON â”‚    â”‚ MON â”‚       â”‚ â† Koordinieren
â”‚  â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ MGR â”‚    â”‚ MGR â”‚                  â”‚ â† Management
â”‚  â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ OSD â”‚    â”‚ OSD â”‚    â”‚ OSD â”‚       â”‚ â† Speichern Daten!
â”‚  â”‚Disk1â”‚    â”‚Disk2â”‚    â”‚Disk3â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ DATENFLUSS: Kafka PVC â†’ Ceph

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka     â”‚ â†’  â”‚ PVC Request â”‚ â†’  â”‚ Ceph Pool   â”‚
â”‚   Pod       â”‚    â”‚ (5GB)       â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                    â”‚
                           â–¼                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚Storage Classâ”‚    â”‚ OSD Daemon â”‚
                    â”‚rook-ceph-*  â”‚    â”‚ (auf Disk)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**OHNE OSD:** PVC bleibt `Pending` - keine Disks verfÃ¼gbar!  
**MIT OSD:** PVC wird `Bound` - Daten werden gespeichert!

## âš¡ WARUM SEPARATE DISKS?

**Talos Design:**
- **scsi0**: System (OS, Boot, Config) - **RESERVIERT**
- **scsi1**: User Data (Ceph) - **VERFÃœGBAR**

**Ceph Regel:**
- 1 OSD = 1 komplette Disk
- Kann nicht OS-Disk nutzen
- Braucht "rohe" unformatierte Disks

## ğŸ“Š HARDWARE ANALYSIS

### Current Infrastructure:
- **homelab**: 48GB RAM (Control Planes only)
- **nipogi**: 80GB RAM (6 Worker Nodes)
- **Datastore**: `local-zfs` (ZFS Storage)

### Proposed OSD Disk Configuration:

#### Option A: Conservative (Recommended)
- **Disk Size**: 100GB per OSD
- **Total**: 6 workers Ã— 100GB = 600GB
- **Replication**: 3x (effective 200GB usable)
- **Use Case**: Kafka, Apps, Databases

#### Option B: Aggressive  
- **Disk Size**: 200GB per OSD
- **Total**: 6 workers Ã— 200GB = 1.2TB
- **Replication**: 3x (effective 400GB usable)
- **Use Case**: Heavy data workloads, ML

#### Option C: Minimal (Testing)
- **Disk Size**: 50GB per OSD
- **Total**: 6 workers Ã— 50GB = 300GB
- **Replication**: 3x (effective 100GB usable)
- **Use Case**: Development, Testing

## ğŸ—ï¸ STORAGE LOCATION ANALYSIS

### VM Storage vs Host Storage

**Current Setup: VM Storage (local-zfs)**
- âœ… **Pros**: 
  - Isolated per VM
  - Easy backup/migration
  - ZFS benefits (compression, dedup)
  - Simple Terraform management
- âŒ **Cons**: 
  - Layer of abstraction
  - Potential performance overhead
  - Limited by host disk I/O

**Alternative: Host Pass-through**
- âœ… **Pros**: 
  - Direct hardware access
  - Maximum performance
  - No virtualization overhead
- âŒ **Cons**: 
  - Complex setup
  - Hardware dependency
  - Difficult migration

### Recommendation: VM Storage (Current Approach)

**Why VM Storage is Better:**
1. **Simplicity**: Works with existing Terraform
2. **Flexibility**: Easy to resize/migrate
3. **Reliability**: ZFS provides data integrity
4. **Management**: Consistent with current architecture

## ğŸ“ OPTIMAL DISK SIZE CALCULATION

### Workload Analysis:
```
Expected Usage:
â”œâ”€â”€ Kafka (Event Streaming): 20-50GB
â”œâ”€â”€ MongoDB (Platform Data): 30-100GB  
â”œâ”€â”€ PostgreSQL (App Data): 20-50GB
â”œâ”€â”€ General App Storage: 50-100GB
â”œâ”€â”€ Growth Buffer (1 year): 2x
â””â”€â”€ Ceph Overhead: 20%

Total Raw Need: ~400-800GB
With 3x Replication: ~1200-2400GB total
Per OSD (6 nodes): ~200-400GB each
```

### **Final Recommendation: 150GB per OSD**

**Reasoning:**
- **Total Raw**: 6 Ã— 150GB = 900GB
- **Effective (3x repl)**: 300GB usable
- **Buffer**: 50% growth headroom
- **Performance**: Good balance size/speed
- **Cost**: Reasonable disk usage

## ğŸ”§ TERRAFORM IMPLEMENTATION

### Required Changes in `virtual-machines.tofu`:

```hcl
# Add after existing OS disk
dynamic "disk" {
  for_each = each.value.machine_type == "worker" ? [1] : []
  content {
    datastore_id = each.value.datastore_id
    interface    = "scsi1"
    iothread     = true
    cache        = "writethrough"
    discard      = "on"
    ssd          = true
    file_format  = "raw"
    size         = 150  # 150GB for Ceph OSD
  }
}
```

### Impact:
- **Workers Only**: Control planes don't need OSD disks
- **Automatic Discovery**: Rook will auto-detect new disks
- **Zero Config**: No manual intervention needed

## ğŸš€ DEPLOYMENT PROCESS

### Step 1: Apply Infrastructure
```bash
cd tofu/
tofu apply  # Adds 150GB disk to all 6 workers
```

### Step 2: Verify Disk Detection
```bash
kubectl get pods -n rook-ceph | grep osd
# Should show 6 OSD pods after ~5 minutes
```

### Step 3: Check Storage Pools
```bash
kubectl get cephblockpool -n rook-ceph
# Should show pools as "Ready"
```

### Step 4: Test with Kafka
```bash
kubectl get pvc -n kafka
# Should show "Bound" status
```

## ğŸ“ˆ MONITORING

### Health Checks:
```bash
# Ceph cluster status
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph status

# OSD status  
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd tree

# Storage usage
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph df
```

### Expected Results:
- **Health**: `HEALTH_OK`
- **OSDs**: `6 up, 6 in`
- **PGs**: `active+clean`
- **Usage**: `<5% initially`

## ğŸ¯ CONCLUSION

**Recommended Configuration:**
- âœ… **150GB OSD disks** on all 6 worker nodes
- âœ… **VM storage** (local-zfs) for simplicity  
- âœ… **Automatic discovery** by Rook operator
- âœ… **~300GB usable** for all Kubernetes workloads

This provides a robust, scalable storage foundation for the entire homelab Kubernetes cluster while maintaining operational simplicity.