apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.3
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  upgradeOSDRequiresHealthyPGs: false
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10

  mon:
    count: 3
    allowMultiplePerNode: false

  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true

  dashboard:
    enabled: true
    ssl: true

  monitoring:
    enabled: false
    createPrometheusRules: false

  network:
    connections:
      encryption:
        enabled: false
      compression:
        enabled: false

  crashCollector:
    disable: false

  logCollector:
    enabled: true
    periodicity: daily
    maxLogSize: 500M

  cleanupPolicy:
    confirmation: ""

  annotations:

  labels:

  placement:

  resources:

  removeOSDsIfOutAndSafeToRemove: false

  storage:
    useAllNodes: true
    useAllDevices: false
    devices:
      - name: "sdb"  # Die 15GB Ceph Disk auf allen Nodes
    config:
      osdsPerDevice: "1"

  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s

  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0