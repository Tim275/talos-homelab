apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
  labels:
    app.kubernetes.io/name: rook-ceph
    app.kubernetes.io/component: storage
    app.kubernetes.io/part-of: storage-platform
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.4
    allowUnsupported: false
  
  dataDirHostPath: /var/lib/rook
  
  # Google SRE practices: Conservative upgrade settings
  upgradeOSDRequiresHealthyPGs: true
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10

  # Google pattern: Odd number for consensus, minimal viable cluster
  mon:
    count: 3
    allowMultiplePerNode: false

  # Google pattern: Active-standby for management
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true
      - name: balancer
        enabled: true
      - name: prometheus
        enabled: true

  # Minimal viable dashboard
  dashboard:
    enabled: true
    ssl: false

  # Essential monitoring only
  monitoring:
    enabled: true

  # Google pattern: Simple, reliable networking
  network:
    provider: host

  # Essential services only
  crashCollector:
    disable: false

  # Conservative cleanup
  cleanupPolicy:
    confirmation: ""

  # Homelab: AGGRESSIVE memory optimization for 10GB nodes
  resources:
    mgr:
      limits:
        cpu: "300m"        # Reduced from 500m
        memory: "512Mi"    # Reduced from 1Gi - MAJOR SAVINGS!
      requests:
        cpu: "30m"         # Reduced from 50m
        memory: "128Mi"    # Reduced from 256Mi
    mon:
      limits:
        cpu: "300m"        # Reduced from 500m  
        memory: "512Mi"    # Reduced from 1Gi - MAJOR SAVINGS!
      requests:
        cpu: "30m"         # Reduced from 50m
        memory: "128Mi"    # Reduced from 256Mi
    osd:
      limits:
        cpu: "500m"        # Reduced from 1000m
        memory: "1.2Gi"    # Increased from 768Mi - OSDs need more memory!
      requests:
        cpu: "100m"        # Reduced from 200m
        memory: "512Mi"    # Increased from 256Mi

  removeOSDsIfOutAndSafeToRemove: true

  # Optimized storage: 8 workers for maximum performance and redundancy
  storage:
    useAllNodes: false
    useAllDevices: false  # No dedicated block devices
    nodes:
      - name: "work-01"  # nipogi - 8.5GB RAM
        devices:
          - name: "sdb"  # Second disk for storage
      - name: "work-02"  # nipogi - 8.5GB RAM
        devices:
          - name: "sdb"  # Second disk for storage
      - name: "work-03"  # nipogi - 8.5GB RAM  
        devices:
          - name: "sdb"  # Second disk for storage
      - name: "work-04"  # nipogi - 8.5GB RAM
        devices:
          - name: "sdb"  # Second disk for storage
      - name: "work-05"  # nipogi - 8.5GB RAM
        devices:
          - name: "sdb"  # Second disk for storage
      - name: "work-06"  # nipogi - 8.5GB RAM
        devices:
          - name: "sdb"  # Second disk for storage
      - name: "work-07"  # nipogi - 8.5GB RAM
        devices:
          - name: "sdb"  # Second disk for storage
      - name: "work-08"  # nipogi - 8.5GB RAM
        devices:
          - name: "sdb"  # Second disk for storage

  # Google pattern: Simple health checks
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s

  # Google pattern: Graceful disruption management
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0

  # Google pattern: Simple, effective anti-affinity
  placement:
    all:
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute 
          operator: Exists
    mon:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: rook-ceph-mon
            topologyKey: kubernetes.io/hostname
    mgr:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: rook-ceph-mgr
              topologyKey: kubernetes.io/hostname
    osd:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: Exists
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: rook-ceph-osd
              topologyKey: kubernetes.io/hostname