apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.3
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  upgradeOSDRequiresHealthyPGs: false
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10

  mon:
    count: 3
    allowMultiplePerNode: false

  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true

  dashboard:
    enabled: true
    ssl: true

  monitoring:
    enabled: false

  network:
    connections:
      encryption:
        enabled: false
      compression:
        enabled: false

  crashCollector:
    disable: false

  logCollector:
    enabled: true
    periodicity: daily
    maxLogSize: 500M

  cleanupPolicy:
    confirmation: ""

  annotations:

  labels:

  placement:

  resources:
    mgr:
      limits:
        cpu: "1000m"
        memory: "2Gi"
      requests:
        cpu: "100m"
        memory: "512Mi"
    mon:
      limits:
        cpu: "2000m"
        memory: "2Gi"
      requests:
        cpu: "100m"
        memory: "1Gi"
    osd:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "100m"
        memory: "2Gi"

  removeOSDsIfOutAndSafeToRemove: false

  storage:
    useAllNodes: true
    useAllDevices: false
    devices:
      - name: "sdb"  # Die 15GB Ceph Disk auf allen Nodes
    config:
      osdsPerDevice: "1"

  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s

  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0