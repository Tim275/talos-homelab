apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
  labels:
    app.kubernetes.io/name: rook-ceph
    app.kubernetes.io/component: storage
    app.kubernetes.io/part-of: storage-platform
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.4
    allowUnsupported: false
  
  dataDirHostPath: /var/lib/rook
  
  # Google SRE practices: Conservative upgrade settings
  upgradeOSDRequiresHealthyPGs: true
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10

  # Google pattern: Odd number for consensus, minimal viable cluster
  mon:
    count: 3
    allowMultiplePerNode: false

  # Google pattern: Active-standby for management
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true
      - name: balancer
        enabled: true
      - name: prometheus
        enabled: true

  # Minimal viable dashboard
  dashboard:
    enabled: true
    ssl: false

  # Essential monitoring only
  monitoring:
    enabled: true

  # Google pattern: Simple, reliable networking
  network:
    provider: host

  # Essential services only
  crashCollector:
    disable: false

  # Conservative cleanup
  cleanupPolicy:
    confirmation: ""

  # Homelab: Memory-optimized for limited resources
  resources:
    mgr:
      limits:
        cpu: "500m"
        memory: "1Gi"
      requests:
        cpu: "50m"
        memory: "256Mi"
    mon:
      limits:
        cpu: "500m"
        memory: "1Gi"
      requests:
        cpu: "50m"
        memory: "256Mi"
    osd:
      limits:
        cpu: "1000m"
        memory: "1.5Gi"  # Reduced from 2Gi to save memory
      requests:
        cpu: "200m"
        memory: "768Mi"  # Reduced from 1Gi

  removeOSDsIfOutAndSafeToRemove: true

  # Google pattern: Explicit node selection, only nipogi workers (more RAM)
  storage:
    useAllNodes: false
    useAllDevices: true  # Auto-detect all available devices
    nodes:
      - name: "work-01"  # nipogi - 14GB RAM
      - name: "work-03"  # nipogi - 14GB RAM
      - name: "work-04"  # nipogi - 14GB RAM
      - name: "work-05"  # nipogi - 14GB RAM
      - name: "work-06"  # nipogi - 14GB RAM
      - name: "work-07"  # nipogi - 14GB RAM

  # Google pattern: Simple health checks
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s

  # Google pattern: Graceful disruption management
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0

  # Google pattern: Simple, effective anti-affinity
  placement:
    all:
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute 
          operator: Exists
    mon:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: rook-ceph-mon
            topologyKey: kubernetes.io/hostname
    mgr:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: rook-ceph-mgr
              topologyKey: kubernetes.io/hostname
    osd:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: Exists
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: rook-ceph-osd
              topologyKey: kubernetes.io/hostname