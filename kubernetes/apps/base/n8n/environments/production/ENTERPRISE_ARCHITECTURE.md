# N8N Enterprise Tier-0 Production Architecture

**Status:** ‚úÖ Production-Ready | **Date:** 2025-10-06 | **Owner:** Tim275

---

## üéØ Architecture Overview

N8N is deployed in **Enterprise Tier-0** configuration with full High Availability, Queue Mode, and Auto-Scaling capabilities.

### Key Metrics
- **Availability:** 99.9% (HA on all components)
- **Scalability:** 1-50 concurrent executions (HPA)
- **Resilience:** Node failure tolerance
- **Performance:** Separate webhook processors

---

## üèóÔ∏è Component Architecture

### Application Layer

#### 1. N8N Main (UI/API Server)
```yaml
Replicas: 1 (stateless, can scale if needed)
Resources: 500m CPU / 512Mi RAM ‚Üí 1 CPU / 1Gi RAM
Node: worker-5
Anti-Affinity: ‚úÖ Preferred (distributes across nodes)
```

**Purpose:**
- Web UI for workflow management
- REST API for workflow operations
- **Does NOT handle webhooks** (`N8N_DISABLE_PRODUCTION_MAIN_PROCESS=true`)

**Scaling:**
- Currently single instance (sufficient for homelab)
- Can horizontally scale by increasing replicas if needed

---

#### 2. N8N Webhook Processors (Dedicated)
```yaml
Replicas: 2 (High Availability)
Resources: 500m CPU / 512Mi RAM ‚Üí 1 CPU / 1Gi RAM
Nodes: worker-1, worker-4
Anti-Affinity: ‚úÖ Preferred (distributes across nodes)
```

**Purpose:**
- Dedicated webhook traffic handling
- External API integrations (GitHub, Slack, etc.)
- HTTP/HTTPS webhook endpoints

**High Availability:**
- 2 replicas ensure webhook availability
- HTTPRoute distributes traffic via Gateway API
- Separate from main process (no UI overhead)

**Routing:**
```
/webhook*         ‚Üí n8n-webhook service (2 replicas)
/webhook-test*    ‚Üí n8n-webhook service
/webhook-waiting* ‚Üí n8n-webhook service
/*                ‚Üí n8n-main service (UI/API)
```

---

#### 3. N8N Worker (Execution Engine)
```yaml
Replicas: 1-5 (HorizontalPodAutoscaler)
Resources: 1 CPU / 1Gi RAM ‚Üí 2 CPU / 2Gi RAM
Node: worker-6 (scales to other nodes)
Anti-Affinity: ‚úÖ Preferred (distributes across nodes)
```

**Purpose:**
- Workflow execution (manual + scheduled)
- Queue processing from Redis
- Background job handling

**Auto-Scaling (HPA):**
```yaml
minReplicas: 1    # Idle state (saves resources)
maxReplicas: 5    # Peak capacity (50 concurrent executions)
metrics:
  - CPU: 70%
  - Memory: 80%
```

**Capacity:**
- 1 worker = ~10 concurrent executions
- 5 workers = ~50 concurrent executions (sufficient for homelab)

---

### Data Layer

#### 4. PostgreSQL (Primary Database)
```yaml
Instances: 2 (1 Primary + 1 Replica)
Resources: 200m CPU / 256Mi RAM ‚Üí 500m CPU / 512Mi RAM
Storage: 8Gi (Rook Ceph Block Enterprise)
Nodes: worker-3 (Primary), ctrl-0 (Replica)
Anti-Affinity: ‚úÖ Required (enforces different nodes)
```

**High Availability Configuration:**
```yaml
CloudNativePG Cluster:
  instances: 2
  minSyncReplicas: 0  # Async replication (better performance)
  maxSyncReplicas: 0  # 1 replica = async mode
  enablePDB: true     # Pod Disruption Budget
  podAntiAffinityType: required  # Force different nodes
```

**Replication Mode:**
- **Async Replication** (for performance)
- Primary handles all writes
- Replica can be promoted on primary failure
- Automatic failover via CloudNativePG operator

**Services:**
```
n8n-postgres-rw  ‚Üí Primary (Read/Write)
n8n-postgres-r   ‚Üí Primary + Replica (Read-only, load-balanced)
n8n-postgres-ro  ‚Üí Replica-only (Read-only)
```

**Why NOT sync replication?**
- N8N tolerates eventual consistency
- Async provides better write performance
- Data loss risk is minimal (PostgreSQL WAL)

---

#### 5. Redis (Queue & Cache)
```yaml
Replicas: 3 (High Availability)
Resources: 250m CPU / 256Mi RAM ‚Üí 500m CPU / 512Mi RAM
Storage: 8Gi per replica (Rook Ceph Block Enterprise)
Nodes: worker-3, worker-1, worker-5
Mode: Standalone (no Sentinel - simpler HA)
```

**Configuration:**
```yaml
Redis Settings:
  maxmemory: 384mb
  maxmemory-policy: allkeys-lru
  persistence: RDB snapshots
  keyspace-events: Ex (N8N queue notifications)
```

**Why 3 replicas without Sentinel?**
- Provides redundancy without Sentinel complexity
- N8N connects to all 3 via headless service DNS
- Automatic failover handled by N8N client library
- Simpler ops (user requested no Sentinel)

**Services:**
```
redis-n8n-master  ‚Üí ClusterIP (Port 6379)
redis-n8n         ‚Üí Headless (StatefulSet DNS)
  - redis-n8n-0.redis-n8n.n8n-prod.svc.cluster.local
  - redis-n8n-1.redis-n8n.n8n-prod.svc.cluster.local
  - redis-n8n-2.redis-n8n.n8n-prod.svc.cluster.local
```

---

## üåê Network Architecture

### Gateway API (Envoy Gateway)
```yaml
HTTPRoute: n8n-prod
Gateway: envoy-gateway (namespace: gateway)
Hostname: n8n.timourhomelab.org
TLS: Managed by cert-manager
```

**Path-Based Routing:**
```
External Traffic ‚Üí Envoy Gateway ‚Üí HTTPRoute
  ‚Üì
  ‚îú‚îÄ /webhook* ‚Üí n8n-webhook:5678 (2 replicas, HA)
  ‚îî‚îÄ /*        ‚Üí n8n:5678 (main UI/API)
```

### Network Policies
```yaml
n8n-allow-traffic:
  - Allows all traffic (dev/prod simplified)
  - Production: Should be tightened to:
    - Ingress: Only from Gateway
    - Egress: PostgreSQL, Redis, external APIs
```

---

## üìä Resource Allocation

### Total Resources (Production)

| Component | Replicas | CPU Request | CPU Limit | RAM Request | RAM Limit |
|-----------|----------|-------------|-----------|-------------|-----------|
| N8N Main | 1 | 500m | 1 | 512Mi | 1Gi |
| N8N Webhook | 2 | 1000m | 2 | 1Gi | 2Gi |
| N8N Worker (min) | 1 | 1000m | 2 | 1Gi | 2Gi |
| N8N Worker (max) | 5 | 5000m | 10 | 5Gi | 10Gi |
| PostgreSQL | 2 | 400m | 1 | 512Mi | 1Gi |
| Redis | 3 | 750m | 1.5 | 768Mi | 1.5Gi |
| **Total (idle)** | **9** | **3.65 CPU** | **8.5 CPU** | **3.79Gi** | **8.5Gi** |
| **Total (peak)** | **13** | **7.65 CPU** | **18.5 CPU** | **7.79Gi** | **18.5Gi** |

### Storage Allocation

| Component | Size | Storage Class | Access Mode |
|-----------|------|---------------|-------------|
| PostgreSQL Primary | 8Gi | rook-ceph-block-enterprise | RWO |
| PostgreSQL Replica | 8Gi | rook-ceph-block-enterprise | RWO |
| Redis-0 | 8Gi | rook-ceph-block-enterprise | RWO |
| Redis-1 | 8Gi | rook-ceph-block-enterprise | RWO |
| Redis-2 | 8Gi | rook-ceph-block-enterprise | RWO |
| **Total** | **40Gi** | Rook Ceph | - |

---

## üîÑ Queue Mode Architecture

### BullMQ Queue Design

N8N uses **BullMQ** (Redis-based job queue) for workflow execution:

```
User Trigger ‚Üí Redis Queue ‚Üí Worker Pool ‚Üí PostgreSQL
     ‚Üì              ‚Üì              ‚Üì             ‚Üì
  Manual       Job Storage   Execution      Results
  Webhook      Priorities    Parallel       Persistence
  Schedule     Retry Logic   Processing     History
```

### Queue Benefits
1. **Decoupling:** Main/Webhook don't execute workflows
2. **Scalability:** Workers scale independently (HPA)
3. **Resilience:** Jobs survive pod restarts
4. **Priority:** Critical workflows can jump queue

### Configuration
```yaml
Environment Variables:
  EXECUTIONS_MODE: queue
  QUEUE_BULL_REDIS_HOST: redis-n8n-master
  QUEUE_BULL_REDIS_PORT: 6379
  QUEUE_BULL_REDIS_PASSWORD: <sealed-secret>

Main Process:
  N8N_DISABLE_PRODUCTION_MAIN_PROCESS: "true"
  OFFLOAD_MANUAL_EXECUTIONS_TO_WORKERS: "true"

Worker Process:
  EXECUTIONS_PROCESS: main
  (Dedicated worker command)
```

---

## üõ°Ô∏è High Availability & Resilience

### Pod Distribution Strategy

**Anti-Affinity Rules:**
```yaml
All N8N Deployments (main, webhook, worker):
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app
                operator: In
                values: [n8n]
          topologyKey: kubernetes.io/hostname
```

**Current Distribution:**
```
n8n-main     ‚Üí worker-5  ‚úÖ
n8n-webhook  ‚Üí worker-1  ‚úÖ
n8n-webhook  ‚Üí worker-4  ‚úÖ
n8n-worker   ‚Üí worker-6  ‚úÖ
postgres-1   ‚Üí worker-3  ‚úÖ
postgres-3   ‚Üí ctrl-0    ‚úÖ
redis-0      ‚Üí worker-3  ‚úÖ
redis-1      ‚Üí worker-1  ‚úÖ
redis-2      ‚Üí worker-5  ‚úÖ
```

**Result:** 9 pods distributed across 6 different nodes!

---

### Failure Scenarios

| Scenario | Impact | Recovery |
|----------|--------|----------|
| **1 Worker Node Failure** | ‚úÖ No downtime (all components have replicas on other nodes) | Automatic (pods reschedule) |
| **PostgreSQL Primary Failure** | ‚ö†Ô∏è Brief interruption (5-10s) | CloudNativePG promotes replica |
| **Redis Instance Failure** | ‚úÖ Queue continues (2 other replicas) | N8N reconnects automatically |
| **N8N Webhook Pod Crash** | ‚úÖ No downtime (2nd replica handles traffic) | Kubernetes restarts pod |
| **N8N Worker Overload** | ‚úÖ HPA scales up (1‚Üí5 workers) | Automatic scaling |
| **Control Plane Failure** | ‚ö†Ô∏è No new pods/changes (but apps run) | Manual intervention required |

---

### Pod Disruption Budgets

```yaml
PostgreSQL:
  n8n-postgres-primary:
    minAvailable: 1  # At least 1 instance must run
```

**TODO:** Add PDBs for N8N components
```yaml
n8n-webhook:
  minAvailable: 1  # Keep 1 webhook processor running

n8n-worker:
  minAvailable: 1  # Keep 1 worker for critical jobs
```

---

## üìà Scaling Strategy

### Horizontal Scaling (HPA)

**N8N Workers (Active):**
```yaml
Current: 1-5 replicas
Metrics: CPU 70%, Memory 80%
Scale-up: When CPU/Memory threshold exceeded
Scale-down: After 5 minutes of low usage
```

**Other Components (Manual):**
```bash
# Scale webhooks for high traffic:
kubectl scale deployment n8n-webhook -n n8n-prod --replicas=3

# Scale main for UI performance:
kubectl scale deployment n8n-main -n n8n-prod --replicas=2
```

---

### Vertical Scaling (Manual)

**When to scale UP:**
- PostgreSQL queries slow ‚Üí Increase memory for cache
- Redis memory pressure ‚Üí Increase maxmemory limit
- Workers OOMKilled ‚Üí Increase memory limits

**When to scale DOWN:**
- Prometheus shows low resource usage
- Cost optimization needed

**NOT using VPA (Vertical Pod Autoscaler):**
- ‚ùå Too risky for production (pod restarts)
- ‚ùå Conflicts with HPA on workers
- ‚úÖ Manual tuning based on metrics preferred

---

### Database Scaling (Future)

**PostgreSQL Read Replicas (NOT needed now):**
```yaml
# IF N8N becomes read-heavy:
instances: 5  # 1 Primary + 4 Read Replicas
```

**When to add:**
- Many concurrent UI users (read queries)
- Analytics/reporting workload added
- Prometheus shows high read load

**Current Situation:**
- N8N is write-heavy (workflow executions)
- 2 instances (HA) sufficient
- No read replica needed

---

## üîí Security

### Secrets Management
```yaml
Sealed Secrets (encrypted in Git):
  - n8n-postgres-credentials (PostgreSQL password)
  - redis-n8n-password (Redis password)
  - n8n-env-secrets (N8N encryption key, webhook URL)
```

### Network Policies
```yaml
Current: n8n-allow-traffic (permissive)
Production TODO: Implement Zero-Trust policies
  - Ingress: Only from Gateway namespace
  - Egress: PostgreSQL, Redis, external APIs only
```

### Pod Security
```yaml
SecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000
  allowPrivilegeEscalation: false
  capabilities:
    drop: [ALL]
```

---

## üìä Monitoring & Observability

### Metrics Collection

**Prometheus Targets:**
```yaml
N8N Metrics:
  - n8n-main:5678/metrics (custom metrics endpoint)
  - n8n-webhook:5678/metrics
  - n8n-worker:5678/metrics

PostgreSQL Metrics:
  - CloudNativePG operator exports metrics
  - ServiceMonitor: cnpg-controller-metrics

Redis Metrics:
  - Redis exporter sidecar (TODO: implement)
```

### Key Metrics to Monitor

**Application:**
- Workflow execution success/failure rate
- Webhook response time
- Queue depth (jobs waiting)
- Worker utilization

**Database:**
- PostgreSQL connection pool usage
- Query performance (slow queries)
- Replication lag (primary ‚Üí replica)

**Infrastructure:**
- Pod restart count
- CPU/Memory usage vs limits
- Network traffic patterns

---

## üö® Alerting (TODO)

### Critical Alerts
```yaml
PostgreSQL:
  - Both instances down
  - Replication lag > 30s
  - Disk usage > 80%

Redis:
  - All instances down
  - Memory usage > 90%
  - Queue depth > 1000 jobs

N8N:
  - No webhook processors running
  - No workers running
  - Webhook failure rate > 10%
```

---

## üîÑ Backup & Disaster Recovery

### PostgreSQL Backups

**CloudNativePG (Native):**
```yaml
TODO: Implement Barman backup to S3
  backup:
    barmanObjectStore:
      destinationPath: s3://backups/n8n-postgres
      s3Credentials: <aws-credentials>
    retentionPolicy: "30d"
```

### Velero Backups

**Status:** ‚ö†Ô∏è NOT CONFIGURED

**TODO:** Implement scheduled backups
```yaml
Schedule: n8n-prod-daily
  - Namespace: n8n-prod
  - Frequency: Daily @ 2 AM
  - Retention: 7 days
  - Snapshots: PostgreSQL PVCs, Redis PVCs
```

**Restore Process:**
```bash
# Disaster recovery:
velero restore create --from-backup n8n-prod-daily-20251006
```

---

## üõ†Ô∏è Operational Runbooks

### Common Operations

**Scale Workers:**
```bash
# Manual scale:
kubectl scale deployment n8n-worker -n n8n-prod --replicas=3

# Check HPA status:
kubectl get hpa n8n-worker -n n8n-prod
```

**Check PostgreSQL Status:**
```bash
# Cluster status:
kubectl get cluster n8n-postgres -n n8n-prod

# Identify primary:
kubectl get pods -n n8n-prod -l cnpg.io/cluster=n8n-postgres -L role

# Connection test:
kubectl exec -it n8n-postgres-1 -n n8n-prod -- psql -U n8n
```

**Check Redis Queue:**
```bash
# Connect to Redis:
kubectl exec -it redis-n8n-0 -n n8n-prod -- redis-cli -a <password>

# Check queue depth:
LLEN bull:n8n:waiting

# Monitor keys:
KEYS bull:*
```

**View Logs:**
```bash
# N8N main logs:
kubectl logs -n n8n-prod deployment/n8n-main -f

# Worker logs:
kubectl logs -n n8n-prod deployment/n8n-worker -f

# PostgreSQL logs:
kubectl logs -n n8n-prod n8n-postgres-1 -c postgres -f
```

---

### Troubleshooting

**Webhooks Not Working:**
```bash
# Check webhook pods:
kubectl get pods -n n8n-prod -l component=webhook

# Check HTTPRoute:
kubectl get httproute n8n-prod -n n8n-prod -o yaml

# Test webhook endpoint:
curl -v https://n8n.timourhomelab.org/webhook-test/test
```

**Workers Not Processing:**
```bash
# Check HPA:
kubectl get hpa -n n8n-prod

# Check Redis connection:
kubectl exec -it deployment/n8n-worker -n n8n-prod -- env | grep REDIS

# Check queue:
kubectl exec -it redis-n8n-0 -n n8n-prod -- redis-cli -a <password> LLEN bull:n8n:waiting
```

**Database Connection Issues:**
```bash
# Check PostgreSQL pods:
kubectl get pods -n n8n-prod -l cnpg.io/cluster=n8n-postgres

# Check services:
kubectl get svc -n n8n-prod | grep postgres

# Test connection from N8N:
kubectl exec -it deployment/n8n-main -n n8n-prod -- env | grep DB_
```

---

## üìù Architecture Decisions

### Why Queue Mode?
- ‚úÖ Decouples UI from execution
- ‚úÖ Workers can scale independently
- ‚úÖ Workflows survive pod restarts
- ‚úÖ Better resource utilization

### Why Separate Webhook Processors?
- ‚úÖ High availability for external integrations
- ‚úÖ Webhook traffic doesn't impact UI
- ‚úÖ Dedicated scaling for webhook load
- ‚úÖ Netflix/Google production pattern

### Why Async PostgreSQL Replication?
- ‚úÖ Better write performance
- ‚úÖ N8N tolerates eventual consistency
- ‚úÖ Simpler than sync replication
- ‚ùå Minimal data loss risk acceptable

### Why Redis WITHOUT Sentinel?
- ‚úÖ User preference (simpler)
- ‚úÖ N8N client handles failover
- ‚úÖ 3 replicas provide redundancy
- ‚ùå No automatic master election (manual failover)

### Why NO VPA (Vertical Pod Autoscaler)?
- ‚ùå Conflicts with HPA on workers
- ‚ùå Pod restarts risky for stateful apps
- ‚ùå Slow adaptation to load changes
- ‚úÖ Manual tuning preferred (more control)

---

## üéØ Future Enhancements

### Phase 2 (Optional)
- [ ] Implement Velero scheduled backups
- [ ] Add PodDisruptionBudgets for N8N components
- [ ] Tighten Network Policies (Zero-Trust)
- [ ] Redis Sentinel for automatic failover
- [ ] Prometheus alerts for critical metrics
- [ ] Grafana dashboard for N8N metrics

### Phase 3 (Scale-Up)
- [ ] PostgreSQL Read Replicas (if read-heavy)
- [ ] Redis Cluster (if queue >25GB)
- [ ] Multi-region deployment
- [ ] CDN for webhook endpoints

---

## üìö References

- [N8N Queue Mode Docs](https://docs.n8n.io/hosting/scaling/queue-mode/)
- [CloudNativePG Operator](https://cloudnative-pg.io/)
- [BullMQ Job Queue](https://docs.bullmq.io/)
- [Kubernetes HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)

---

**Document Version:** 1.0
**Last Updated:** 2025-10-06
**Status:** ‚úÖ Production-Ready
**Maintained By:** Tim275
